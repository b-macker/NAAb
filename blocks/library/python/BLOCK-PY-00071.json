{
  "code": "extern \"C\" {\n\nvoid BLOCK-PY-00071_execute() {\n    class SqlGenerator(BaseCodeGenerator):\n    def __init__(self):\n        super().__init__(\"sql\")\n        self.supported_databases = [\n            'postgresql', 'postgres', 'pg',\n            'mysql', 'mariadb',\n            'sqlserver', 'mssql', 'tsql',\n            'oracle', 'plsql',\n            'sqlite', 'sqlite3'\n        ]\n    \n    def _initialize_generators(self) -> Dict[str, callable]:\n        \"\"\"Initialize SQL generator methods\"\"\"\n        return {\n            'table_creation': self._generate_table_creation,\n            'complex_query': self._generate_complex_query,\n            'stored_procedure': self._generate_stored_procedure,\n            'analytical_query': self._generate_analytical_query,\n            'etl_process': self._generate_etl_process,\n            'basic_select': self._generate_basic_select,\n            'basic_insert': self._generate_basic_insert,\n            'basic_update': self._generate_basic_update,\n            'default': self._generate_default_sql\n        }\n    \n    def _initialize_patterns(self) -> Dict[str, List[str]]:\n        \"\"\"Initialize SQL keyword patterns for generator selection\"\"\"\n        return {\n            'table_creation': ['create table', 'table create', 'new table', 'build table'],\n            'complex_query': ['join', 'complex query', 'report', 'analytics'],\n            'stored_procedure': ['procedure', 'function', 'stored proc'],\n            'analytical_query': ['analytics', 'analysis', 'olap', 'reporting'],\n            'etl_process': ['etl', 'extract', 'transform', 'load', 'pipeline'],\n            'basic_select': ['select', 'query', 'find', 'get', 'retrieve'],\n            'basic_insert': ['insert', 'add', 'create data', 'new record'],\n            'basic_update': ['update', 'modify', 'change', 'edit']\n        }\n    \n    def _initialize_templates(self) -> Dict[str, str]:\n        \"\"\"Initialize SQL code templates\"\"\"\n        return {\n            'basic_table': \"\"\"CREATE TABLE {table_name} (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\"\"\",\n            'basic_query': \"SELECT * FROM {table_name} WHERE {condition};\",\n            'basic_insert': \"INSERT INTO {table_name} ({columns}) VALUES ({values});\",\n            'basic_update': \"UPDATE {table_name} SET {assignments} WHERE {condition};\"\n        }\n    \n    def _generate_fallback(self, request) -> CodeGenerationResult:\n        \"\"\"Generate fallback SQL when no specific generator matches\"\"\"\n        return CodeGenerationResult(\n            success=True,\n            code=self._generate_example_queries(self._detect_database(request.message.lower())),\n            language=\"sql\",\n            code_type=CodeType.UTILITY,\n            complexity=\"low\",\n            generator_used=\"fallback\"\n        )\n        \n    def generate_code(self, request) -> CodeGenerationResult:\n        try:\n            message = getattr(request, 'message', '').lower()\n            complexity = getattr(request, 'complexity', 'medium')\n            database = getattr(request, 'framework', '').lower() or self._detect_database(message)\n            \n            patterns = {\n                # Database Schema Operations\n                r'create.*table|table.*create': self._generate_table_creation,\n                r'alter.*table|modify.*table': self._generate_table_alteration,\n                r'drop.*table|delete.*table.*structure': self._generate_table_drop,\n                r'create.*index|index.*create': self._generate_index_creation,\n                r'create.*view|view.*create': self._generate_view_creation,\n                \n                # Data Operations\n                r'insert.*data|add.*record': self._generate_data_insertion,\n                r'update.*data|modify.*record': self._generate_data_update,\n                r'delete.*data|remove.*record': self._generate_data_deletion,\n                r'select.*data|query.*data|fetch.*data': self._generate_data_query,\n                r'upsert|merge.*data': self._generate_upsert_operation,\n                \n                # Advanced Queries\n                r'join.*query|complex.*query': self._generate_complex_query,\n                r'aggregate.*query|sum|count|avg|group': self._generate_aggregate_query,\n                r'window.*function|rank|row_number': self._generate_window_function,\n                r'cte|common.*table.*expression|with.*query': self._generate_cte_query,\n                r'recursive.*query|hierarchical': self._generate_recursive_query,\n                \n                # Database Functions & Procedures\n                r'stored.*procedure|procedure.*create': self._generate_stored_procedure,\n                r'function.*create|user.*function': self._generate_database_function,\n                r'trigger.*create|create.*trigger': self._generate_trigger,\n                r'cursor.*operation|cursor.*query': self._generate_cursor_operation,\n                \n                # Performance & Analytics\n                r'performance.*query|optimize.*query': self._generate_performance_query,\n                r'analytical.*query|olap|cube': self._generate_analytical_query,\n                r'partition.*query|partition.*table': self._generate_partitioned_table,\n                r'materialized.*view|indexed.*view': self._generate_materialized_view,\n                \n                # Security & Administration\n                r'user.*management|create.*user': self._generate_user_management,\n                r'role.*management|grant|revoke': self._generate_role_management,\n                r'backup.*script|backup.*database': self._generate_backup_script,\n                r'migration.*script|schema.*migration': self._generate_migration_script,\n                \n                # Data Warehousing\n                r'etl.*process|data.*pipeline': self._generate_etl_process,\n                r'fact.*table|dimension.*table': self._generate_dimensional_model,\n                r'star.*schema|snowflake.*schema': self._generate_warehouse_schema,\n            }\n            \n            for pattern, generator_func in patterns.items():\n                if re.search(pattern, message):\n                    result = generator_func(request, database, complexity)\n                    result.dependencies = self._extract_dependencies(result.code, database)\n                    return result\n            \n            return self._generate_default_sql(request, database, complexity)\n            \n        except Exception as e:\n            return CodeGenerationResult(\n                success=False,\n                code=\"\",\n                language=\"sql\",\n                error=f\"SQL generation error: {str(e)}\"\n            )\n    \n    def _detect_database(self, message: str) -> str:\n        \"\"\"Detect database type from message\"\"\"\n        db_patterns = {\n            'postgresql': ['postgres', 'pg', 'postgresql'],\n            'mysql': ['mysql', 'mariadb'],\n            'sqlserver': ['sql server', 'mssql', 'tsql', 't-sql'],\n            'oracle': ['oracle', 'plsql', 'pl/sql'],\n            'sqlite': ['sqlite', 'sqlite3']\n        }\n        \n        for db_type, keywords in db_patterns.items():\n            for keyword in keywords:\n                if keyword in message:\n                    return db_type\n        return 'postgresql'  # Default to PostgreSQL\n    \n    def _generate_table_creation(self, request, database: str, complexity: str) -> CodeGenerationResult:\n        \"\"\"Generate comprehensive table creation SQL\"\"\"\n        message = request.message.lower()\n        \n        if 'user' in message:\n            table_name = 'users'\n            columns = self._get_user_table_columns(database)\n        elif 'product' in message:\n            table_name = 'products'\n            columns = self._get_product_table_columns(database)\n        elif 'order' in message:\n            table_name = 'orders'\n            columns = self._get_order_table_columns(database)\n        else:\n            table_name = 'example_table'\n            columns = self._get_generic_table_columns(database)\n        \n        sql = self._build_create_table_sql(table_name, columns, database, complexity)\n        \n        return CodeGenerationResult(\n            success=True,\n            code=sql,\n            language=\"sql\",\n            framework=database,\n            code_type=CodeType.SCHEMA\n        )\n    \n    def _get_user_table_columns(self, database: str) -> List[Dict]:\n        \"\"\"Generate user table columns based on database type\"\"\"\n        base_columns = [\n            {'name': 'id', 'type': self._get_primary_key_type(database), 'constraints': ['PRIMARY KEY']},\n            {'name': 'username', 'type': 'VARCHAR(50)', 'constraints': ['UNIQUE', 'NOT NULL']},\n            {'name': 'email', 'type': 'VARCHAR(255)', 'constraints': ['UNIQUE', 'NOT NULL']},\n            {'name': 'password_hash', 'type': 'VARCHAR(255)', 'constraints': ['NOT NULL']},\n            {'name': 'first_name', 'type': 'VARCHAR(100)', 'constraints': ['NOT NULL']},\n            {'name': 'last_name', 'type': 'VARCHAR(100)', 'constraints': ['NOT NULL']},\n            {'name': 'is_active', 'type': 'BOOLEAN', 'constraints': ['DEFAULT TRUE']},\n            {'name': 'created_at', 'type': self._get_timestamp_type(database), 'constraints': [self._get_default_timestamp(database)]},\n            {'name': 'updated_at', 'type': self._get_timestamp_type(database), 'constraints': [self._get_default_timestamp(database)]}\n        ]\n        \n        if database == 'postgresql':\n            base_columns.append({'name': 'metadata', 'type': 'JSONB', 'constraints': ['DEFAULT \\'{}\\'']})\n        elif database == 'mysql':\n            base_columns.append({'name': 'metadata', 'type': 'JSON', 'constraints': ['DEFAULT (JSON_OBJECT())']})\n        \n        return base_columns\n    \n    def _get_product_table_columns(self, database: str) -> List[Dict]:\n        \"\"\"Generate product table columns\"\"\"\n        return [\n            {'name': 'id', 'type': self._get_primary_key_type(database), 'constraints': ['PRIMARY KEY']},\n            {'name': 'sku', 'type': 'VARCHAR(50)', 'constraints': ['UNIQUE', 'NOT NULL']},\n            {'name': 'name', 'type': 'VARCHAR(255)', 'constraints': ['NOT NULL']},\n            {'name': 'description', 'type': 'TEXT', 'constraints': []},\n            {'name': 'price', 'type': 'DECIMAL(10,2)', 'constraints': ['NOT NULL', 'CHECK (price >= 0)']},\n            {'name': 'cost', 'type': 'DECIMAL(10,2)', 'constraints': ['CHECK (cost >= 0)']},\n            {'name': 'stock_quantity', 'type': 'INTEGER', 'constraints': ['DEFAULT 0', 'CHECK (stock_quantity >= 0)']},\n            {'name': 'category_id', 'type': 'INTEGER', 'constraints': ['REFERENCES categories(id)']},\n            {'name': 'is_active', 'type': 'BOOLEAN', 'constraints': ['DEFAULT TRUE']},\n            {'name': 'created_at', 'type': self._get_timestamp_type(database), 'constraints': [self._get_default_timestamp(database)]},\n            {'name': 'updated_at', 'type': self._get_timestamp_type(database), 'constraints': [self._get_default_timestamp(database)]}\n        ]\n    \n    def _get_order_table_columns(self, database: str) -> List[Dict]:\n        \"\"\"Generate order table columns\"\"\"\n        return [\n            {'name': 'id', 'type': self._get_primary_key_type(database), 'constraints': ['PRIMARY KEY']},\n            {'name': 'order_number', 'type': 'VARCHAR(50)', 'constraints': ['UNIQUE', 'NOT NULL']},\n            {'name': 'user_id', 'type': 'INTEGER', 'constraints': ['NOT NULL', 'REFERENCES users(id)']},\n            {'name': 'status', 'type': f'VARCHAR(20)', 'constraints': [f'DEFAULT \\'pending\\'', 'CHECK (status IN (\\'pending\\', \\'confirmed\\', \\'shipped\\', \\'delivered\\', \\'cancelled\\'))']},\n            {'name': 'subtotal', 'type': 'DECIMAL(10,2)', 'constraints': ['NOT NULL', 'CHECK (subtotal >= 0)']},\n            {'name': 'tax_amount', 'type': 'DECIMAL(10,2)', 'constraints': ['DEFAULT 0', 'CHECK (tax_amount >= 0)']},\n            {'name': 'shipping_cost', 'type': 'DECIMAL(10,2)', 'constraints': ['DEFAULT 0', 'CHECK (shipping_cost >= 0)']},\n            {'name': 'total_amount', 'type': 'DECIMAL(10,2)', 'constraints': ['NOT NULL', 'CHECK (total_amount >= 0)']},\n            {'name': 'shipping_address', 'type': 'TEXT', 'constraints': []},\n            {'name': 'order_date', 'type': self._get_timestamp_type(database), 'constraints': [self._get_default_timestamp(database)]},\n            {'name': 'shipped_date', 'type': self._get_timestamp_type(database), 'constraints': []},\n            {'name': 'delivered_date', 'type': self._get_timestamp_type(database), 'constraints': []}\n        ]\n    \n    def _get_generic_table_columns(self, database: str) -> List[Dict]:\n        \"\"\"Generate generic table columns\"\"\"\n        return [\n            {'name': 'id', 'type': self._get_primary_key_type(database), 'constraints': ['PRIMARY KEY']},\n            {'name': 'name', 'type': 'VARCHAR(255)', 'constraints': ['NOT NULL']},\n            {'name': 'description', 'type': 'TEXT', 'constraints': []},\n            {'name': 'status', 'type': 'VARCHAR(20)', 'constraints': ['DEFAULT \\'active\\'']},\n            {'name': 'created_at', 'type': self._get_timestamp_type(database), 'constraints': [self._get_default_timestamp(database)]},\n            {'name': 'updated_at', 'type': self._get_timestamp_type(database), 'constraints': [self._get_default_timestamp(database)]}\n        ]\n    \n    def _build_create_table_sql(self, table_name: str, columns: List[Dict], database: str, complexity: str) -> str:\n        \"\"\"Build CREATE TABLE SQL statement\"\"\"\n        sql_parts = [f\"-- Create {table_name} table for {database.upper()}\"]\n        sql_parts.append(f\"CREATE TABLE {table_name} (\")\n        \n        column_definitions = []\n        for column in columns:\n            col_def = f\"    {column['name']} {column['type']}\"\n            if column['constraints']:\n                col_def += f\" {' '.join(column['constraints'])}\"\n            column_definitions.append(col_def)\n        \n        sql_parts.append(',\\n'.join(column_definitions))\n        sql_parts.append(\");\")\n        \n        # Add indexes for performance\n        if complexity in ['medium', 'high']:\n            sql_parts.append(\"\")\n            sql_parts.append(f\"-- Performance indexes for {table_name}\")\n            if table_name == 'users':\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_email ON {table_name}(email);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_active ON {table_name}(is_active);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_created ON {table_name}(created_at);\")\n            elif table_name == 'products':\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_sku ON {table_name}(sku);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_category ON {table_name}(category_id);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_price ON {table_name}(price);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_active ON {table_name}(is_active);\")\n            elif table_name == 'orders':\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_user ON {table_name}(user_id);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_status ON {table_name}(status);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_date ON {table_name}(order_date);\")\n                sql_parts.append(f\"CREATE INDEX idx_{table_name}_number ON {table_name}(order_number);\")\n        \n        # Add triggers for updated_at if high complexity\n        if complexity == 'high' and database == 'postgresql':\n            sql_parts.append(\"\")\n            sql_parts.append(f\"-- Auto-update trigger for {table_name}\")\n            sql_parts.append(f\"\"\"CREATE OR REPLACE FUNCTION update_{table_name}_updated_at()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = CURRENT_TIMESTAMP;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER trigger_{table_name}_updated_at\n    BEFORE UPDATE ON {table_name}\n    FOR EACH ROW\n    EXECUTE FUNCTION update_{table_name}_updated_at();\"\"\")\n        \n        return '\\n'.join(sql_parts)\n    \n    def _generate_complex_query(self, request, database: str, complexity: str) -> CodeGenerationResult:\n        \"\"\"Generate complex JOIN queries\"\"\"\n        sql = f\"\"\"-- Complex multi-table query for {database.upper()}\n-- Query: Sales report with customer and product details\nSELECT \n    o.order_number,\n    o.order_date,\n    CONCAT(u.first_name, ' ', u.last_name) as customer_name,\n    u.email as customer_email,\n    p.name as product_name,\n    p.sku as product_sku,\n    oi.quantity,\n    oi.unit_price,\n    (oi.quantity * oi.unit_price) as line_total,\n    o.total_amount as order_total,\n    c.name as category_name,\n    CASE \n        WHEN o.status = 'delivered' THEN 'Completed'\n        WHEN o.status = 'shipped' THEN 'In Transit'\n        WHEN o.status = 'confirmed' THEN 'Processing'\n        ELSE 'Pending'\n    END as order_status_display\nFROM orders o\nINNER JOIN users u ON o.user_id = u.id\nINNER JOIN order_items oi ON o.id = oi.order_id\nINNER JOIN products p ON oi.product_id = p.id\nLEFT JOIN categories c ON p.category_id = c.id\nWHERE o.order_date >= {self._get_date_function(database, -30)}\n    AND u.is_active = true\n    AND p.is_active = true\nORDER BY o.order_date DESC, o.order_number;\n\n-- Query: Customer purchase summary with ranking\nWITH customer_stats AS (\n    SELECT \n        u.id,\n        CONCAT(u.first_name, ' ', u.last_name) as customer_name,\n        u.email,\n        COUNT(DISTINCT o.id) as total_orders,\n        SUM(o.total_amount) as total_spent,\n        AVG(o.total_amount) as avg_order_value,\n        MAX(o.order_date) as last_order_date,\n        MIN(o.order_date) as first_order_date\n    FROM users u\n    INNER JOIN orders o ON u.id = o.user_id\n    WHERE o.status IN ('delivered', 'shipped')\n        AND o.order_date >= {self._get_date_function(database, -365)}\n    GROUP BY u.id, u.first_name, u.last_name, u.email\n)\nSELECT \n    *,\n    RANK() OVER (ORDER BY total_spent DESC) as spending_rank,\n    CASE \n        WHEN total_spent >= 1000 THEN 'VIP'\n        WHEN total_spent >= 500 THEN 'Premium'\n        WHEN total_spent >= 100 THEN 'Regular'\n        ELSE 'New'\n    END as customer_tier\nFROM customer_stats\nORDER BY total_spent DESC;\"\"\"\n\n        return CodeGenerationResult(\n            success=True,\n            code=sql,\n            language=\"sql\",\n            framework=database,\n            code_type=CodeType.QUERY\n        )\n    \n    def _generate_stored_procedure(self, request, database: str, complexity: str) -> CodeGenerationResult:\n        \"\"\"Generate stored procedure based on database type\"\"\"\n        if database == 'postgresql':\n            sql = self._generate_postgresql_procedure()\n        elif database == 'mysql':\n            sql = self._generate_mysql_procedure()\n        elif database == 'sqlserver':\n            sql = self._generate_sqlserver_procedure()\n        elif database == 'oracle':\n            sql = self._generate_oracle_procedure()\n        else:\n            sql = self._generate_postgresql_procedure()  # Default\n        \n        return CodeGenerationResult(\n            success=True,\n            code=sql,\n            language=\"sql\",\n            framework=database,\n            code_type=CodeType.FUNCTION\n        )\n    \n    def _generate_postgresql_procedure(self) -> str:\n        \"\"\"Generate PostgreSQL stored procedure\"\"\"\n        return \"\"\"-- PostgreSQL Stored Procedure: Process Order\nCREATE OR REPLACE FUNCTION process_order(\n    p_user_id INTEGER,\n    p_product_ids INTEGER[],\n    p_quantities INTEGER[],\n    p_shipping_address TEXT\n) RETURNS TABLE(\n    order_id INTEGER,\n    order_number VARCHAR(50),\n    total_amount DECIMAL(10,2),\n    status VARCHAR(20)\n) \nLANGUAGE plpgsql\nAS $$\nDECLARE\n    v_order_id INTEGER;\n    v_order_number VARCHAR(50);\n    v_subtotal DECIMAL(10,2) := 0;\n    v_tax_amount DECIMAL(10,2);\n    v_total_amount DECIMAL(10,2);\n    v_product_id INTEGER;\n    v_quantity INTEGER;\n    v_unit_price DECIMAL(10,2);\n    v_line_total DECIMAL(10,2);\n    i INTEGER;\nBEGIN\n    -- Validate input arrays\n    IF array_length(p_product_ids, 1) != array_length(p_quantities, 1) THEN\n        RAISE EXCEPTION 'Product IDs and quantities arrays must have same length';\n    END IF;\n    \n    -- Generate order number\n    v_order_number := 'ORD-' || to_char(NOW(), 'YYYYMMDD') || '-' || \n                      lpad(nextval('order_number_seq')::TEXT, 6, '0');\n    \n    -- Create order record\n    INSERT INTO orders (order_number, user_id, status, subtotal, tax_amount, \n                       total_amount, shipping_address, order_date)\n    VALUES (v_order_number, p_user_id, 'pending', 0, 0, 0, p_shipping_address, NOW())\n    RETURNING id INTO v_order_id;\n    \n    -- Process each product\n    FOR i IN 1..array_length(p_product_ids, 1) LOOP\n        v_product_id := p_product_ids[i];\n        v_quantity := p_quantities[i];\n        \n        -- Get product price and validate stock\n        SELECT price INTO v_unit_price\n        FROM products \n        WHERE id = v_product_id AND is_active = true AND stock_quantity >= v_quantity;\n        \n        IF NOT FOUND THEN\n            RAISE EXCEPTION 'Product % not available or insufficient stock', v_product_id;\n        END IF;\n        \n        v_line_total := v_quantity * v_unit_price;\n        v_subtotal := v_subtotal + v_line_total;\n        \n        -- Create order item\n        INSERT INTO order_items (order_id, product_id, quantity, unit_price, line_total)\n        VALUES (v_order_id, v_product_id, v_quantity, v_unit_price, v_line_total);\n        \n        -- Update product stock\n        UPDATE products \n        SET stock_quantity = stock_quantity - v_quantity,\n            updated_at = NOW()\n        WHERE id = v_product_id;\n    END LOOP;\n    \n    -- Calculate tax (8.5% rate)\n    v_tax_amount := v_subtotal * 0.085;\n    v_total_amount := v_subtotal + v_tax_amount;\n    \n    -- Update order totals\n    UPDATE orders \n    SET subtotal = v_subtotal,\n        tax_amount = v_tax_amount,\n        total_amount = v_total_amount,\n        status = 'confirmed',\n        updated_at = NOW()\n    WHERE id = v_order_id;\n    \n    -- Return order details\n    RETURN QUERY \n    SELECT v_order_id, v_order_number, v_total_amount, 'confirmed'::VARCHAR(20);\n    \nEXCEPTION\n    WHEN OTHERS THEN\n        -- Rollback handled automatically by PostgreSQL\n        RAISE EXCEPTION 'Order processing failed: %', SQLERRM;\nEND;\n$$;\n\n-- Usage example:\n-- SELECT * FROM process_order(1, ARRAY[1,2,3], ARRAY[2,1,4], '123 Main St, City, State 12345');\"\"\"\n\n    def _generate_analytical_query(self, request, database: str, complexity: str) -> CodeGenerationResult:\n        \"\"\"Generate analytical/OLAP queries\"\"\"\n        sql = f\"\"\"-- Advanced Analytical Queries for {database.upper()}\n\n-- 1. Sales Performance Analysis with Moving Averages\nWITH daily_sales AS (\n    SELECT \n        DATE({self._get_date_extract('order_date', database)}) as sale_date,\n        COUNT(DISTINCT id) as orders_count,\n        SUM(total_amount) as daily_revenue,\n        AVG(total_amount) as avg_order_value\n    FROM orders \n    WHERE status IN ('delivered', 'shipped')\n        AND order_date >= {self._get_date_function(database, -90)}\n    GROUP BY DATE({self._get_date_extract('order_date', database)})\n),\nsales_with_trends AS (\n    SELECT *,\n        AVG(daily_revenue) OVER (\n            ORDER BY sale_date \n            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n        ) as revenue_7day_ma,\n        AVG(daily_revenue) OVER (\n            ORDER BY sale_date \n            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW  \n        ) as revenue_30day_ma,\n        LAG(daily_revenue, 1) OVER (ORDER BY sale_date) as prev_day_revenue,\n        LAG(daily_revenue, 7) OVER (ORDER BY sale_date) as prev_week_revenue\n    FROM daily_sales\n)\nSELECT *,\n    ROUND(((daily_revenue - prev_day_revenue) / NULLIF(prev_day_revenue, 0)) * 100, 2) as day_over_day_pct,\n    ROUND(((daily_revenue - prev_week_revenue) / NULLIF(prev_week_revenue, 0)) * 100, 2) as week_over_week_pct,\n    CASE \n        WHEN daily_revenue > revenue_7day_ma * 1.2 THEN 'High Performance'\n        WHEN daily_revenue > revenue_7day_ma * 0.8 THEN 'Normal Performance'\n        ELSE 'Below Average'\n    END as performance_category\nFROM sales_with_trends\nORDER BY sale_date DESC;\n\n-- 2. Customer Cohort Analysis\nWITH customer_cohorts AS (\n    SELECT \n        u.id as customer_id,\n        DATE_TRUNC('month', MIN(o.order_date)) as cohort_month,\n        DATE_TRUNC('month', o.order_date) as order_month\n    FROM users u\n    INNER JOIN orders o ON u.id = o.user_id\n    WHERE o.status IN ('delivered', 'shipped')\n    GROUP BY u.id, DATE_TRUNC('month', o.order_date)\n),\ncohort_data AS (\n    SELECT \n        cohort_month,\n        order_month,\n        COUNT(DISTINCT customer_id) as customers,\n        {self._get_months_between('order_month', 'cohort_month', database)} as period_number\n    FROM customer_cohorts\n    GROUP BY cohort_month, order_month\n),\ncohort_table AS (\n    SELECT \n        cohort_month,\n        period_number,\n        customers,\n        FIRST_VALUE(customers) OVER (\n            PARTITION BY cohort_month \n            ORDER BY period_number\n        ) as cohort_size\n    FROM cohort_data\n)\nSELECT \n    cohort_month,\n    cohort_size,\n    period_number,\n    customers,\n    ROUND((customers::DECIMAL / cohort_size) * 100, 2) as retention_rate\nFROM cohort_table\nWHERE period_number <= 12\nORDER BY cohort_month, period_number;\n\n-- 3. Product Performance with ABC Analysis\nWITH product_performance AS (\n    SELECT \n        p.id,\n        p.name,\n        p.sku,\n        c.name as category,\n        COUNT(DISTINCT oi.order_id) as orders_count,\n        SUM(oi.quantity) as total_quantity_sold,\n        SUM(oi.line_total) as total_revenue,\n        AVG(oi.unit_price) as avg_selling_price,\n        SUM(oi.quantity * p.cost) as total_cost,\n        (SUM(oi.line_total) - SUM(oi.quantity * p.cost)) as total_profit\n    FROM products p\n    INNER JOIN order_items oi ON p.id = oi.product_id\n    INNER JOIN orders o ON oi.order_id = o.id\n    LEFT JOIN categories c ON p.category_id = c.id\n    WHERE o.status IN ('delivered', 'shipped')\n        AND o.order_date >= {self._get_date_function(database, -365)}\n    GROUP BY p.id, p.name, p.sku, c.name, p.cost\n),\nrevenue_ranking AS (\n    SELECT *,\n        SUM(total_revenue) OVER () as company_total_revenue,\n        SUM(total_revenue) OVER (ORDER BY total_revenue DESC) as cumulative_revenue,\n        ROW_NUMBER() OVER (ORDER BY total_revenue DESC) as revenue_rank\n    FROM product_performance\n)\nSELECT *,\n    ROUND((total_revenue / company_total_revenue) * 100, 2) as revenue_percentage,\n    ROUND((cumulative_revenue / company_total_revenue) * 100, 2) as cumulative_percentage,\n    CASE \n        WHEN cumulative_revenue <= company_total_revenue * 0.8 THEN 'A - Top Performers'\n        WHEN cumulative_revenue <= company_total_revenue * 0.95 THEN 'B - Good Performers' \n        ELSE 'C - Low Performers'\n    END as abc_category,\n    ROUND((total_profit / NULLIF(total_revenue, 0)) * 100, 2) as profit_margin_pct\nFROM revenue_ranking\nORDER BY total_revenue DESC;\"\"\"\n\n        return CodeGenerationResult(\n            success=True,\n            code=sql,\n            language=\"sql\",\n            framework=database,\n            code_type=CodeType.ANALYTICS\n        )\n    \n    def _generate_etl_process(self, request, database: str, complexity: str) -> CodeGenerationResult:\n        \"\"\"Generate ETL process scripts\"\"\"\n        sql = f\"\"\"-- ETL Process Script for {database.upper()}\n-- Extract, Transform, Load pipeline for data warehouse\n\n-- 1. Extract: Create staging tables for raw data\nCREATE TABLE staging_sales_raw (\n    transaction_id VARCHAR(50),\n    customer_email VARCHAR(255),\n    product_sku VARCHAR(50),\n    quantity INTEGER,\n    unit_price DECIMAL(10,2),\n    transaction_date VARCHAR(50),\n    store_location VARCHAR(100),\n    sales_rep VARCHAR(100),\n    source_system VARCHAR(20),\n    extracted_at {self._get_timestamp_type(database)} DEFAULT {self._get_current_timestamp(database)}\n);\n\n-- 2. Transform: Data cleaning and validation procedure\n{self._generate_etl_transform_procedure(database)}\n\n-- 3. Load: Insert into data warehouse fact table\nCREATE TABLE fact_sales (\n    sale_id {self._get_primary_key_type(database)} PRIMARY KEY,\n    date_key INTEGER REFERENCES dim_date(date_key),\n    customer_key INTEGER REFERENCES dim_customer(customer_key),\n    product_key INTEGER REFERENCES dim_product(product_key),\n    store_key INTEGER REFERENCES dim_store(store_key),\n    sales_rep_key INTEGER REFERENCES dim_sales_rep(sales_rep_key),\n    quantity INTEGER NOT NULL,\n    unit_price DECIMAL(10,2) NOT NULL,\n    line_total DECIMAL(10,2) NOT NULL,\n    discount_amount DECIMAL(10,2) DEFAULT 0,\n    tax_amount DECIMAL(10,2) DEFAULT 0,\n    created_at {self._get_timestamp_type(database)} DEFAULT {self._get_current_timestamp(database)},\n    updated_at {self._get_timestamp_type(database)} DEFAULT {self._get_current_timestamp(database)}\n);\n\n-- 4. ETL Error Logging Table\nCREATE TABLE etl_log (\n    log_id {self._get_primary_key_type(database)} PRIMARY KEY,\n    process_name VARCHAR(100) NOT NULL,\n    start_time {self._get_timestamp_type(database)},\n    end_time {self._get_timestamp_type(database)},\n    status VARCHAR(20) CHECK (status IN ('running', 'completed', 'failed')),\n    records_processed INTEGER DEFAULT 0,\n    records_inserted INTEGER DEFAULT 0,\n    records_updated INTEGER DEFAULT 0,\n    records_rejected INTEGER DEFAULT 0,\n    error_message TEXT,\n    created_at {self._get_timestamp_type(database)} DEFAULT {self._get_current_timestamp(database)}\n);\n\n-- 5. Main ETL Process\n{self._generate_main_etl_procedure(database)}\n\n-- 6. Data Quality Checks\nCREATE VIEW data_quality_report AS\nSELECT \n    'fact_sales' as table_name,\n    COUNT(*) as total_records,\n    COUNT(CASE WHEN quantity <= 0 THEN 1 END) as invalid_quantity,\n    COUNT(CASE WHEN unit_price <= 0 THEN 1 END) as invalid_price,\n    COUNT(CASE WHEN line_total != (quantity * unit_price) THEN 1 END) as calculation_errors,\n    MIN(created_at) as earliest_record,\n    MAX(created_at) as latest_record\nFROM fact_sales\nUNION ALL\nSELECT \n    'staging_sales_raw' as table_name,\n    COUNT(*) as total_records,\n    COUNT(CASE WHEN quantity IS NULL OR quantity <= 0 THEN 1 END) as invalid_quantity,\n    COUNT(CASE WHEN unit_price IS NULL OR unit_price <= 0 THEN 1 END) as invalid_price,\n    COUNT(CASE WHEN customer_email NOT LIKE '%@%' THEN 1 END) as invalid_emails,\n    MIN(extracted_at) as earliest_record,\n    MAX(extracted_at) as latest_record\nFROM staging_sales_raw;\n\n-- 7. Performance monitoring query\nSELECT \n    process_name,\n    AVG(EXTRACT(EPOCH FROM (end_time - start_time))) / 60 as avg_duration_minutes,\n    AVG(records_processed) as avg_records_processed,\n    SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failure_count,\n    COUNT(*) as total_runs\nFROM etl_log\nWHERE start_time >= {self._get_date_function(database, -30)}\nGROUP BY process_name\nORDER BY avg_duration_minutes DESC;\"\"\"\n\n        return CodeGenerationResult(\n            success=True,\n            code=sql,\n            language=\"sql\",\n            framework=database,\n            code_type=CodeType.ETL\n        )\n    \n    def _generate_default_sql(self, request, database: str, complexity: str) -> CodeGenerationResult:\n        \"\"\"Generate default SQL based on request\"\"\"\n        message = request.message.lower()\n        \n        if any(word in message for word in ['select', 'query', 'find', 'get']):\n            sql = self._generate_basic_select(database)\n        elif any(word in message for word in ['insert', 'add', 'create data']):\n            sql = self._generate_basic_insert(database)\n        elif any(word in message for word in ['update', 'modify', 'change']):\n            sql = self._generate_basic_update(database)\n        else:\n            sql = self._generate_example_queries(database)\n        \n        return CodeGenerationResult(\n            success=True,\n            code=sql,\n            language=\"sql\",\n            framework=database,\n            code_type=CodeType.QUERY\n        )\n    \n    # Database-specific helper methods\n    def _get_primary_key_type(self, database: str) -> str:\n        mapping = {\n            'postgresql': 'SERIAL',\n            'mysql': 'AUTO_INCREMENT INTEGER',\n            'sqlserver': 'INTEGER IDENTITY(1,1)',\n            'oracle': 'NUMBER GENERATED ALWAYS AS IDENTITY',\n            'sqlite': 'INTEGER PRIMARY KEY AUTOINCREMENT'\n        }\n        return mapping.get(database, 'SERIAL')\n    \n    def _get_timestamp_type(self, database: str) -> str:\n        mapping = {\n            'postgresql': 'TIMESTAMP WITH TIME ZONE',\n            'mysql': 'TIMESTAMP',\n            'sqlserver': 'DATETIME2',\n            'oracle': 'TIMESTAMP WITH TIME ZONE',\n            'sqlite': 'DATETIME'\n        }\n        return mapping.get(database, 'TIMESTAMP')\n    \n    def _get_default_timestamp(self, database: str) -> str:\n        mapping = {\n            'postgresql': 'DEFAULT CURRENT_TIMESTAMP',\n            'mysql': 'DEFAULT CURRENT_TIMESTAMP',\n            'sqlserver': 'DEFAULT GETUTCDATE()',\n            'oracle': 'DEFAULT CURRENT_TIMESTAMP',\n            'sqlite': 'DEFAULT CURRENT_TIMESTAMP'\n        }\n        return mapping.get(database, 'DEFAULT CURRENT_TIMESTAMP')\n    \n    def _get_current_timestamp(self, database: str) -> str:\n        mapping = {\n            'postgresql': 'CURRENT_TIMESTAMP',\n            'mysql': 'CURRENT_TIMESTAMP',\n            'sqlserver': 'GETUTCDATE()',\n            'oracle': 'CURRENT_TIMESTAMP',\n            'sqlite': 'CURRENT_TIMESTAMP'\n        }\n        return mapping.get(database, 'CURRENT_TIMESTAMP')\n    \n    def _get_date_function(self, database: str, days_offset: int) -> str:\n        if database == 'postgresql':\n            return f\"CURRENT_DATE + INTERVAL '{days_offset} days'\"\n        elif database == 'mysql':\n            return f\"DATE_ADD(CURRENT_DATE, INTERVAL {days_offset} DAY)\"\n        elif database == 'sqlserver':\n            return f\"DATEADD(day, {days_offset}, GETDATE())\"\n        elif database == 'oracle':\n            return f\"SYSDATE + {days_offset}\"\n        elif database == 'sqlite':\n            return f\"date('now', '{days_offset} days')\"\n        return f\"CURRENT_DATE + INTERVAL '{days_offset} days'\"\n    \n    def _extract_dependencies(self, code: str, database: str) -> List[str]:\n        \"\"\"Extract SQL dependencies from generated code\"\"\"\n        dependencies = [f\"{database}-client\"]\n        \n        # Add specific dependencies based on code content\n        if 'JSONB' in code or 'JSON' in code:\n            dependencies.append(\"json-support\")\n        if 'TRIGGER' in code.upper():\n            dependencies.append(\"trigger-support\") \n        if 'PROCEDURE' in code.upper() or 'FUNCTION' in code.upper():\n            dependencies.append(\"stored-procedures\")\n        if 'WINDOW' in code.upper() or 'OVER(' in code:\n            dependencies.append(\"window-functions\")\n        if 'WITH' in code.upper() and 'AS (' in code:\n            dependencies.append(\"cte-support\")\n            \n        return list(set(dependencies))\n    \n    # Additional helper methods for complex SQL generation\n    def _get_date_extract(self, column: str, database: str) -> str:\n        if database in ['postgresql', 'sqlite']:\n            return column\n        elif database == 'mysql':\n            return f\"DATE({column})\"\n        elif database == 'sqlserver':\n            return f\"CAST({column} AS DATE)\"\n        elif database == 'oracle':\n            return f\"TRUNC({column})\"\n        return column\n    \n    def _get_months_between(self, date1: str, date2: str, database: str) -> str:\n        if database == 'postgresql':\n            return f\"EXTRACT(MONTH FROM AGE({date1}, {date2}))\"\n        elif database == 'mysql':\n            return f\"PERIOD_DIFF(DATE_FORMAT({date1}, '%Y%m'), DATE_FORMAT({date2}, '%Y%m'))\"\n        elif database == 'sqlserver':\n            return f\"DATEDIFF(MONTH, {date2}, {date1})\"\n        elif database == 'oracle':\n            return f\"MONTHS_BETWEEN({date1}, {date2})\"\n        return f\"EXTRACT(MONTH FROM AGE({date1}, {date2}))\"\n    \n    def _generate_etl_transform_procedure(self, database: str) -> str:\n        if database == 'postgresql':\n            return \"\"\"CREATE OR REPLACE FUNCTION transform_sales_data()\nRETURNS INTEGER\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    processed_count INTEGER := 0;\n    error_count INTEGER := 0;\nBEGIN\n    -- Clean and validate data\n    UPDATE staging_sales_raw \n    SET \n        customer_email = LOWER(TRIM(customer_email)),\n        product_sku = UPPER(TRIM(product_sku)),\n        transaction_date = CASE \n            WHEN transaction_date ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}' THEN transaction_date::TIMESTAMP\n            ELSE NULL\n        END\n    WHERE customer_email ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'\n        AND quantity > 0 \n        AND unit_price > 0;\n    \n    GET DIAGNOSTICS processed_count = ROW_COUNT;\n    \n    -- Mark invalid records\n    UPDATE staging_sales_raw \n    SET source_system = 'INVALID_' || source_system\n    WHERE customer_email !~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'\n        OR quantity <= 0 \n        OR unit_price <= 0\n        OR transaction_date IS NULL;\n    \n    RETURN processed_count;\nEND;\n$$;\"\"\"\n        return \"-- Transform procedure placeholder for \" + database\n    \n    def _generate_main_etl_procedure(self, database: str) -> str:\n        if database == 'postgresql':\n            return \"\"\"CREATE OR REPLACE FUNCTION run_sales_etl()\nRETURNS TABLE(\n    status VARCHAR(20),\n    records_processed INTEGER,\n    records_inserted INTEGER,\n    duration_seconds INTEGER\n)\nLANGUAGE plpgsql\nAS $$\nDECLARE\n    start_time TIMESTAMP;\n    end_time TIMESTAMP;\n    log_id INTEGER;\n    processed_count INTEGER;\n    inserted_count INTEGER;\nBEGIN\n    start_time := CURRENT_TIMESTAMP;\n    \n    -- Log ETL start\n    INSERT INTO etl_log (process_name, start_time, status)\n    VALUES ('sales_etl', start_time, 'running')\n    RETURNING etl_log.log_id INTO log_id;\n    \n    BEGIN\n        -- Transform data\n        SELECT transform_sales_data() INTO processed_count;\n        \n        -- Load into fact table\n        INSERT INTO fact_sales (date_key, customer_key, product_key, store_key, \n                               sales_rep_key, quantity, unit_price, line_total)\n        SELECT \n            dd.date_key,\n            dc.customer_key,\n            dp.product_key,\n            ds.store_key,\n            dr.sales_rep_key,\n            sr.quantity,\n            sr.unit_price,\n            (sr.quantity * sr.unit_price) as line_total\n        FROM staging_sales_raw sr\n        INNER JOIN dim_customer dc ON sr.customer_email = dc.email\n        INNER JOIN dim_product dp ON sr.product_sku = dp.sku\n        INNER JOIN dim_store ds ON sr.store_location = ds.location_name\n        INNER JOIN dim_sales_rep dr ON sr.sales_rep = dr.rep_name\n        INNER JOIN dim_date dd ON DATE(sr.transaction_date::TIMESTAMP) = dd.full_date\n        WHERE sr.source_system NOT LIKE 'INVALID_%';\n        \n        GET DIAGNOSTICS inserted_count = ROW_COUNT;\n        end_time := CURRENT_TIMESTAMP;\n        \n        -- Update log with success\n        UPDATE etl_log \n        SET end_time = end_time,\n            status = 'completed',\n            records_processed = processed_count,\n            records_inserted = inserted_count\n        WHERE etl_log.log_id = log_id;\n        \n        -- Clean up staging table\n        DELETE FROM staging_sales_raw WHERE extracted_at < CURRENT_DATE - INTERVAL '7 days';\n        \n        RETURN QUERY SELECT 'completed'::VARCHAR(20), processed_count, inserted_count, \n                           EXTRACT(EPOCH FROM (end_time - start_time))::INTEGER;\n        \n    EXCEPTION\n        WHEN OTHERS THEN\n            end_time := CURRENT_TIMESTAMP;\n            UPDATE etl_log \n            SET end_time = end_time,\n                status = 'failed',\n                error_message = SQLERRM\n            WHERE etl_log.log_id = log_id;\n            \n            RETURN QUERY SELECT 'failed'::VARCHAR(20), 0, 0, \n                               EXTRACT(EPOCH FROM (end_time - start_time))::INTEGER;\n    END;\nEND;\n$$;\"\"\"\n        return \"-- Main ETL procedure placeholder for \" + database\n    \n    def _generate_basic_select(self, database: str) -> str:\n        return f\"\"\"-- Basic SELECT queries for {database.upper()}\n\n-- 1. Simple data retrieval\nSELECT id, name, email, created_at\nFROM users \nWHERE is_active = true\nORDER BY created_at DESC\nLIMIT 10;\n\n-- 2. Aggregated data\nSELECT \n    status,\n    COUNT(*) as order_count,\n    SUM(total_amount) as total_revenue,\n    AVG(total_amount) as avg_order_value\nFROM orders \nWHERE order_date >= {self._get_date_function(database, -30)}\nGROUP BY status\nORDER BY total_revenue DESC;\n\n-- 3. JOIN query\nSELECT \n    u.first_name,\n    u.last_name,\n    COUNT(o.id) as total_orders,\n    SUM(o.total_amount) as total_spent\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.is_active = true\nGROUP BY u.id, u.first_name, u.last_name\nHAVING COUNT(o.id) > 0\nORDER BY total_spent DESC;\"\"\"\n    \n    def _generate_basic_insert(self, database: str) -> str:\n        return f\"\"\"-- INSERT statements for {database.upper()}\n\n-- 1. Single record insert\nINSERT INTO users (username, email, password_hash, first_name, last_name)\nVALUES ('john_doe', 'john@example.com', 'hashed_password_123', 'John', 'Doe');\n\n-- 2. Multiple records insert  \nINSERT INTO products (sku, name, price, cost, stock_quantity, category_id)\nVALUES \n    ('PRD001', 'Premium Widget', 29.99, 15.00, 100, 1),\n    ('PRD002', 'Standard Widget', 19.99, 10.00, 200, 1),\n    ('PRD003', 'Basic Widget', 9.99, 5.00, 300, 1);\n\n-- 3. Insert with subquery\nINSERT INTO order_items (order_id, product_id, quantity, unit_price, line_total)\nSELECT \n    o.id as order_id,\n    p.id as product_id,\n    2 as quantity,\n    p.price as unit_price,\n    (2 * p.price) as line_total\nFROM orders o\nCROSS JOIN products p\nWHERE o.order_number = 'ORD-20240101-000001'\n    AND p.sku IN ('PRD001', 'PRD002');\"\"\"\n    \n    def _generate_basic_update(self, database: str) -> str:\n        return f\"\"\"-- UPDATE statements for {database.upper()}\n\n-- 1. Simple update\nUPDATE products \nSET price = 24.99,\n    updated_at = {self._get_current_timestamp(database)}\nWHERE sku = 'PRD001';\n\n-- 2. Conditional update\nUPDATE orders \nSET status = 'shipped',\n    shipped_date = {self._get_current_timestamp(database)},\n    updated_at = {self._get_current_timestamp(database)}\nWHERE status = 'confirmed' \n    AND order_date >= {self._get_date_function(database, -7)};\n\n-- 3. Update with JOIN\nUPDATE products p\nSET stock_quantity = p.stock_quantity - oi.total_sold\nFROM (\n    SELECT \n        product_id,\n        SUM(quantity) as total_sold\n    FROM order_items oi\n    INNER JOIN orders o ON oi.order_id = o.id\n    WHERE o.status = 'delivered'\n        AND o.delivered_date >= {self._get_date_function(database, -1)}\n    GROUP BY product_id\n) oi\nWHERE p.id = oi.product_id;\"\"\"\n    \n    def _generate_example_queries(self, database: str) -> str:\n        return f\"\"\"-- Example SQL queries for {database.upper()}\n\n-- 1. Basic data retrieval\nSELECT * FROM users WHERE is_active = true LIMIT 5;\n\n-- 2. Aggregation example\nSELECT \n    DATE({self._get_date_extract('created_at', database)}) as signup_date,\n    COUNT(*) as new_users\nFROM users \nWHERE created_at >= {self._get_date_function(database, -30)}\nGROUP BY DATE({self._get_date_extract('created_at', database)})\nORDER BY signup_date;\n\n-- 3. Performance query\nSELECT \n    table_name,\n    pg_size_pretty(pg_total_relation_size(table_name::regclass)) as size\nFROM (VALUES ('users'), ('products'), ('orders'), ('order_items')) as t(table_name);\"\"\"\n}\n\n} // extern \"C\"\n",
  "id": "BLOCK-PY-00071",
  "language": "python",
  "source_file": "/storage/emulated/0/Download/integrate ideas/Generators/sql_generator.py",
  "source_line": 17,
  "validation_status": "validated"
}