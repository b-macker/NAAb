{
  "code": "#include \"absl/base/internal/spinlock.h\"\n#include <algorithm>\n#include <atomic>\n#include <limits>\n#include \"absl/base/attributes.h\"\n#include \"absl/base/config.h\"\n#include \"absl/base/internal/atomic_hook.h\"\n#include \"absl/base/internal/cycleclock.h\"\n#include \"absl/base/internal/spinlock_wait.h\"\n#include \"absl/base/internal/sysinfo.h\" /* For NumCPUs() */\n#include \"absl/base/call_once.h\"\n\nusing namespace absl;\n\nextern \"C\" {\n\nvoid BLOCK-CPP-02333_execute() {\n    {\nABSL_NAMESPACE_BEGIN\nnamespace base_internal {\n\nABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES static base_internal::AtomicHook<void (*)(\n    const void *lock, int64_t wait_cycles)>\n    submit_profile_data;\n\nvoid RegisterSpinLockProfiler(void (*fn)(const void *contendedlock,\n                                         int64_t wait_cycles)) {\n  submit_profile_data.Store(fn);\n}\n\n#ifdef ABSL_INTERNAL_NEED_REDUNDANT_CONSTEXPR_DECL\n// Static member variable definitions.\nconstexpr uint32_t SpinLock::kSpinLockHeld;\nconstexpr uint32_t SpinLock::kSpinLockCooperative;\nconstexpr uint32_t SpinLock::kSpinLockDisabledScheduling;\nconstexpr uint32_t SpinLock::kSpinLockSleeper;\nconstexpr uint32_t SpinLock::kWaitTimeMask;\n#endif\n\n// Uncommon constructors.\nSpinLock::SpinLock(base_internal::SchedulingMode mode)\n    : lockword_(IsCooperative(mode) ? kSpinLockCooperative : 0) {\n  ABSL_TSAN_MUTEX_CREATE(this, __tsan_mutex_not_static);\n}\n\n// Monitor the lock to see if its value changes within some time period\n// (adaptive_spin_count loop iterations). The last value read from the lock\n// is returned from the method.\nuint32_t SpinLock::SpinLoop() {\n  // We are already in the slow path of SpinLock, initialize the\n  // adaptive_spin_count here.\n  ABSL_CONST_INIT static absl::once_flag init_adaptive_spin_count;\n  ABSL_CONST_INIT static int adaptive_spin_count = 0;\n  base_internal::LowLevelCallOnce(&init_adaptive_spin_count, []() {\n    adaptive_spin_count = base_internal::NumCPUs() > 1 ? 1000 : 1;\n  });\n\n  int c = adaptive_spin_count;\n  uint32_t lock_value;\n  do {\n    lock_value = lockword_.load(std::memory_order_relaxed);\n  } while ((lock_value & kSpinLockHeld) != 0 && --c > 0);\n  return lock_value;\n}\n\nvoid SpinLock::SlowLock() {\n  uint32_t lock_value = SpinLoop();\n  lock_value = TryLockInternal(lock_value, 0);\n  if ((lock_value & kSpinLockHeld) == 0) {\n    return;\n  }\n\n  base_internal::SchedulingMode scheduling_mode;\n  if ((lock_value & kSpinLockCooperative) != 0) {\n    scheduling_mode = base_internal::SCHEDULE_COOPERATIVE_AND_KERNEL;\n  } else {\n    scheduling_mode = base_internal::SCHEDULE_KERNEL_ONLY;\n  }\n\n  // The lock was not obtained initially, so this thread needs to wait for\n  // it.  Record the current timestamp in the local variable wait_start_time\n  // so the total wait time can be stored in the lockword once this thread\n  // obtains the lock.\n  int64_t wait_start_time = CycleClock::Now();\n  uint32_t wait_cycles = 0;\n  int lock_wait_call_count = 0;\n  while ((lock_value & kSpinLockHeld) != 0) {\n    // If the lock is currently held, but not marked as having a sleeper, mark\n    // it as having a sleeper.\n    if ((lock_value & kWaitTimeMask) == 0) {\n      // Here, just \"mark\" that the thread is going to sleep.  Don't store the\n      // lock wait time in the lock -- the lock word stores the amount of time\n      // that the current holder waited before acquiring the lock, not the wait\n      // time of any thread currently waiting to acquire it.\n      if (lockword_.compare_exchange_strong(\n              lock_value, lock_value | kSpinLockSleeper,\n              std::memory_order_relaxed, std::memory_order_relaxed)) {\n        // Successfully transitioned to kSpinLockSleeper.  Pass\n        // kSpinLockSleeper to the SpinLockWait routine to properly indicate\n        // the last lock_value observed.\n        lock_value |= kSpinLockSleeper;\n      } else if ((lock_value & kSpinLockHeld) == 0) {\n        // Lock is free again, so try and acquire it before sleeping.  The\n        // new lock state will be the number of cycles this thread waited if\n        // this thread obtains the lock.\n        lock_value = TryLockInternal(lock_value, wait_cycles);\n        continue;   // Skip the delay at the end of the loop.\n      } else if ((lock_value & kWaitTimeMask) == 0) {\n        // The lock is still held, without a waiter being marked, but something\n        // else about the lock word changed, causing our CAS to fail. For\n        // example, a new lock holder may have acquired the lock with\n        // kSpinLockDisabledScheduling set, whereas the previous holder had not\n        // set that flag. In this case, attempt again to mark ourselves as a\n        // waiter.\n        continue;\n      }\n    }\n\n    // SpinLockDelay() calls into fiber scheduler, we need to see\n    // synchronization there to avoid false positives.\n    ABSL_TSAN_MUTEX_PRE_DIVERT(this, 0);\n    // Wait for an OS specific delay.\n    base_internal::SpinLockDelay(&lockword_, lock_value, ++lock_wait_call_count,\n                                 scheduling_mode);\n    ABSL_TSAN_MUTEX_POST_DIVERT(this, 0);\n    // Spin again after returning from the wait routine to give this thread\n    // some chance of obtaining the lock.\n    lock_value = SpinLoop();\n    wait_cycles = EncodeWaitCycles(wait_start_time, CycleClock::Now());\n    lock_value = TryLockInternal(lock_value, wait_cycles);\n  }\n}\n\nvoid SpinLock::SlowUnlock(uint32_t lock_value) {\n  base_internal::SpinLockWake(&lockword_,\n                              false);  // wake waiter if necessary\n\n  // If our acquisition was contended, collect contentionz profile info.  We\n  // reserve a unitary wait time to represent that a waiter exists without our\n  // own acquisition having been contended.\n  if ((lock_value & kWaitTimeMask) != kSpinLockSleeper) {\n    const int64_t wait_cycles = DecodeWaitCycles(lock_value);\n    ABSL_TSAN_MUTEX_PRE_DIVERT(this, 0);\n    submit_profile_data(this, wait_cycles);\n    ABSL_TSAN_MUTEX_POST_DIVERT(this, 0);\n  }\n}\n\n// We use the upper 29 bits of the lock word to store the time spent waiting to\n// acquire this lock.  This is reported by contentionz profiling.  Since the\n// lower bits of the cycle counter wrap very quickly on high-frequency\n// processors we divide to reduce the granularity to 2^kProfileTimestampShift\n// sized units.  On a 4Ghz machine this will lose track of wait times greater\n// than (2^29/4 Ghz)*128 =~ 17.2 seconds.  Such waits should be extremely rare.\nstatic constexpr int kProfileTimestampShift = 7;\n\n// We currently reserve the lower 3 bits.\nstatic constexpr int kLockwordReservedShift = 3;\n\nuint32_t SpinLock::EncodeWaitCycles(int64_t wait_start_time,\n                                    int64_t wait_end_time) {\n  static const int64_t kMaxWaitTime =\n      std::numeric_limits<uint32_t>::max() >> kLockwordReservedShift;\n  int64_t scaled_wait_time =\n      (wait_end_time - wait_start_time) >> kProfileTimestampShift;\n\n  // Return a representation of the time spent waiting that can be stored in\n  // the lock word's upper bits.\n  uint32_t clamped = static_cast<uint32_t>(\n      std::min(scaled_wait_time, kMaxWaitTime) << kLockwordReservedShift);\n\n  if (clamped == 0) {\n    return kSpinLockSleeper;  // Just wake waiters, but don't record contention.\n  }\n  // Bump up value if necessary to avoid returning kSpinLockSleeper.\n  const uint32_t kMinWaitTime =\n      kSpinLockSleeper + (1 << kLockwordReservedShift);\n  if (clamped == kSpinLockSleeper) {\n    return kMinWaitTime;\n  }\n  return clamped;\n}\n\nint64_t SpinLock::DecodeWaitCycles(uint32_t lock_value) {\n  // Cast to uint32_t first to ensure bits [63:32] are cleared.\n  const int64_t scaled_wait_time =\n      static_cast<uint32_t>(lock_value & kWaitTimeMask);\n  return scaled_wait_time << (kProfileTimestampShift - kLockwordReservedShift);\n}\n\n}  // namespace base_internal\nABSL_NAMESPACE_END\n}\n}\n\n} // extern \"C\"\n",
  "id": "BLOCK-CPP-02333",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/base/internal/spinlock.cc",
  "source_line": 57,
  "validation_status": "validated"
}