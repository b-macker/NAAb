{
  "code": "{\nABSL_NAMESPACE_BEGIN\nnamespace random_internal {\nnamespace {\n\n// RandenPoolEntry is a thread-safe pseudorandom bit generator, implementing a\n// single generator within a RandenPool<T>. It is an internal implementation\n// detail, and does not aim to conform to [rand.req.urng].\n//\n// NOTE: There are alignment issues when used on ARM, for instance.\n// See the allocation code in PoolAlignedAlloc().\nclass RandenPoolEntry {\n public:\n  static constexpr size_t kState = RandenTraits::kStateBytes / sizeof(uint32_t);\n  static constexpr size_t kCapacity =\n      RandenTraits::kCapacityBytes / sizeof(uint32_t);\n\n  void Init(absl::Span<const uint32_t> data) {\n    SpinLockHolder l(&mu_);  // Always uncontested.\n    std::copy(data.begin(), data.end(), std::begin(state_));\n    next_ = kState;\n  }\n\n  // Copy bytes into out.\n  void Fill(uint8_t* out, size_t bytes) ABSL_LOCKS_EXCLUDED(mu_);\n\n  // Returns random bits from the buffer in units of T.\n  template <typename T>\n  inline T Generate() ABSL_LOCKS_EXCLUDED(mu_);\n\n  inline void MaybeRefill() ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n    if (next_ >= kState) {\n      next_ = kCapacity;\n      impl_.Generate(state_);\n    }\n  }\n\n private:\n  // Randen URBG state.\n  uint32_t state_[kState] ABSL_GUARDED_BY(mu_);  // First to satisfy alignment.\n  SpinLock mu_;\n  const Randen impl_;\n  size_t next_ ABSL_GUARDED_BY(mu_);\n};\n\ntemplate <>\ninline uint8_t RandenPoolEntry::Generate<uint8_t>() {\n  SpinLockHolder l(&mu_);\n  MaybeRefill();\n  return static_cast<uint8_t>(state_[next_++]);\n}\n\ntemplate <>\ninline uint16_t RandenPoolEntry::Generate<uint16_t>() {\n  SpinLockHolder l(&mu_);\n  MaybeRefill();\n  return static_cast<uint16_t>(state_[next_++]);\n}\n\ntemplate <>\ninline uint32_t RandenPoolEntry::Generate<uint32_t>() {\n  SpinLockHolder l(&mu_);\n  MaybeRefill();\n  return state_[next_++];\n}\n\ntemplate <>\ninline uint64_t RandenPoolEntry::Generate<uint64_t>() {\n  SpinLockHolder l(&mu_);\n  if (next_ >= kState - 1) {\n    next_ = kCapacity;\n    impl_.Generate(state_);\n  }\n  auto p = state_ + next_;\n  next_ += 2;\n\n  uint64_t result;\n  std::memcpy(&result, p, sizeof(result));\n  return result;\n}\n\nvoid RandenPoolEntry::Fill(uint8_t* out, size_t bytes) {\n  SpinLockHolder l(&mu_);\n  while (bytes > 0) {\n    MaybeRefill();\n    size_t remaining = (kState - next_) * sizeof(state_[0]);\n    size_t to_copy = std::min(bytes, remaining);\n    std::memcpy(out, &state_[next_], to_copy);\n    out += to_copy;\n    bytes -= to_copy;\n    next_ += (to_copy + sizeof(state_[0]) - 1) / sizeof(state_[0]);\n  }\n}\n\n// Number of pooled urbg entries.\nstatic constexpr size_t kPoolSize = 8;\n\n// Shared pool entries.\nstatic absl::once_flag pool_once;\nABSL_CACHELINE_ALIGNED static RandenPoolEntry* shared_pools[kPoolSize];\n\n// Returns an id in the range [0 ... kPoolSize), which indexes into the\n// pool of random engines.\n//\n// Each thread to access the pool is assigned a sequential ID (without reuse)\n// from the pool-id space; the id is cached in a thread_local variable.\n// This id is assigned based on the arrival-order of the thread to the\n// GetPoolID call; this has no binary, CL, or runtime stability because\n// on subsequent runs the order within the same program may be significantly\n// different. However, as other thread IDs are not assigned sequentially,\n// this is not expected to matter.\nsize_t GetPoolID() {\n  static_assert(kPoolSize >= 1,\n                \"At least one urbg instance is required for PoolURBG\");\n\n  ABSL_CONST_INIT static std::atomic<uint64_t> sequence{0};\n\n#ifdef ABSL_HAVE_THREAD_LOCAL\n  static thread_local size_t my_pool_id = kPoolSize;\n  if (ABSL_PREDICT_FALSE(my_pool_id == kPoolSize)) {\n    my_pool_id = (sequence++ % kPoolSize);\n  }\n  return my_pool_id;\n#else\n  static pthread_key_t tid_key = [] {\n    pthread_key_t tmp_key;\n    int err = pthread_key_create(&tmp_key, nullptr);\n    if (err) {\n      ABSL_RAW_LOG(FATAL, \"pthread_key_create failed with %d\", err);\n    }\n    return tmp_key;\n  }();\n\n  // Store the value in the pthread_{get/set}specific. However an uninitialized\n  // value is 0, so add +1 to distinguish from the null value.\n  uintptr_t my_pool_id =\n      reinterpret_cast<uintptr_t>(pthread_getspecific(tid_key));\n  if (ABSL_PREDICT_FALSE(my_pool_id == 0)) {\n    // No allocated ID, allocate the next value, cache it, and return.\n    my_pool_id = (sequence++ % kPoolSize) + 1;\n    int err = pthread_setspecific(tid_key, reinterpret_cast<void*>(my_pool_id));\n    if (err) {\n      ABSL_RAW_LOG(FATAL, \"pthread_setspecific failed with %d\", err);\n    }\n  }\n  return my_pool_id - 1;\n#endif\n}\n\n// Allocate a RandenPoolEntry with at least 32-byte alignment, which is required\n// by ARM platform code.\nRandenPoolEntry* PoolAlignedAlloc() {\n  constexpr size_t kAlignment =\n      ABSL_CACHELINE_SIZE > 32 ? ABSL_CACHELINE_SIZE : 32;\n\n  // Not all the platforms that we build for have std::aligned_alloc, however\n  // since we never free these objects, we can over allocate and munge the\n  // pointers to the correct alignment.\n  uintptr_t x = reinterpret_cast<uintptr_t>(\n      new char[sizeof(RandenPoolEntry) + kAlignment]);\n  auto y = x % kAlignment;\n  void* aligned = reinterpret_cast<void*>(y == 0 ? x : (x + kAlignment - y));\n  return new (aligned) RandenPoolEntry();\n}\n\n// Allocate and initialize kPoolSize objects of type RandenPoolEntry.\n//\n// The initialization strategy is to initialize one object directly from\n// OS entropy, then to use that object to seed all of the individual\n// pool instances.\nvoid InitPoolURBG() {\n  static constexpr size_t kSeedSize =\n      RandenTraits::kStateBytes / sizeof(uint32_t);\n  // Read the seed data from OS entropy once.\n  uint32_t seed_material[kPoolSize * kSeedSize];\n  if (!random_internal::ReadSeedMaterialFromOSEntropy(\n          absl::MakeSpan(seed_material))) {\n    random_internal::ThrowSeedGenException();\n  }\n  for (size_t i = 0; i < kPoolSize; i++) {\n    shared_pools[i] = PoolAlignedAlloc();\n    shared_pools[i]->Init(\n        absl::MakeSpan(&seed_material[i * kSeedSize], kSeedSize));\n  }\n}\n\n// Returns the pool entry for the current thread.\nRandenPoolEntry* GetPoolForCurrentThread() {\n  absl::call_once(pool_once, InitPoolURBG);\n  return shared_pools[GetPoolID()];\n}\n\n}  // namespace\n\ntemplate <typename T>\ntypename RandenPool<T>::result_type RandenPool<T>::Generate() {\n  auto* pool = GetPoolForCurrentThread();\n  return pool->Generate<T>();\n}\n\ntemplate <typename T>\nvoid RandenPool<T>::Fill(absl::Span<result_type> data) {\n  auto* pool = GetPoolForCurrentThread();\n  pool->Fill(reinterpret_cast<uint8_t*>(data.data()),\n             data.size() * sizeof(result_type));\n}\n\ntemplate class RandenPool<uint8_t>;\ntemplate class RandenPool<uint16_t>;\ntemplate class RandenPool<uint32_t>;\ntemplate class RandenPool<uint64_t>;\n\n}  // namespace random_internal\nABSL_NAMESPACE_END\n}",
  "id": "BLOCK-CPP-02786",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/random/internal/pool_urbg.cc",
  "source_line": 39,
  "validation_status": "validated"
}