{
  "code": "#include <intrin.h>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <pmmintrin.h>\n#include <immintrin.h>\n#include \"absl/crc/internal/non_temporal_arm_intrinsics.h\"\n#include <algorithm>\n#include <cassert>\n#include <cstdint>\n#include <cstring>\n#include \"absl/base/config.h\"\n#include \"absl/base/optimization.h\"\n\nusing namespace absl;\nusing namespace crc_internal;\n\nextern \"C\" {\n\nvoid BLOCK-CPP-05447_execute() {\n    {\n\n// This non-temporal memcpy does regular load and non-temporal store memory\n// copy. It is compatible to both 16-byte aligned and unaligned addresses. If\n// data at the destination is not immediately accessed, using non-temporal\n// memcpy can save 1 DRAM load of the destination cacheline.\nconstexpr size_t kCacheLineSize = ABSL_CACHELINE_SIZE;\n\n// If the objects overlap, the behavior is undefined.\ninline void *non_temporal_store_memcpy(void *__restrict dst,\n                                       const void *__restrict src, size_t len) {\n#if defined(__SSE3__) || defined(__aarch64__) || \\\n    (defined(_MSC_VER) && defined(__AVX__))\n  // This implementation requires SSE3.\n  // MSVC cannot target SSE3 directly, but when MSVC targets AVX,\n  // SSE3 support is implied.\n  uint8_t *d = reinterpret_cast<uint8_t *>(dst);\n  const uint8_t *s = reinterpret_cast<const uint8_t *>(src);\n\n  // memcpy() the misaligned header. At the end of this if block, <d> is\n  // aligned to a 64-byte cacheline boundary or <len> == 0.\n  if (reinterpret_cast<uintptr_t>(d) & (kCacheLineSize - 1)) {\n    uintptr_t bytes_before_alignment_boundary =\n        kCacheLineSize -\n        (reinterpret_cast<uintptr_t>(d) & (kCacheLineSize - 1));\n    size_t header_len = (std::min)(bytes_before_alignment_boundary, len);\n    assert(bytes_before_alignment_boundary < kCacheLineSize);\n    memcpy(d, s, header_len);\n    d += header_len;\n    s += header_len;\n    len -= header_len;\n  }\n\n  if (len >= kCacheLineSize) {\n    _mm_sfence();\n    __m128i *dst_cacheline = reinterpret_cast<__m128i *>(d);\n    const __m128i *src_cacheline = reinterpret_cast<const __m128i *>(s);\n    constexpr int kOpsPerCacheLine = kCacheLineSize / sizeof(__m128i);\n    size_t loops = len / kCacheLineSize;\n\n    while (len >= kCacheLineSize) {\n      __m128i temp1, temp2, temp3, temp4;\n      temp1 = _mm_lddqu_si128(src_cacheline + 0);\n      temp2 = _mm_lddqu_si128(src_cacheline + 1);\n      temp3 = _mm_lddqu_si128(src_cacheline + 2);\n      temp4 = _mm_lddqu_si128(src_cacheline + 3);\n      _mm_stream_si128(dst_cacheline + 0, temp1);\n      _mm_stream_si128(dst_cacheline + 1, temp2);\n      _mm_stream_si128(dst_cacheline + 2, temp3);\n      _mm_stream_si128(dst_cacheline + 3, temp4);\n      src_cacheline += kOpsPerCacheLine;\n      dst_cacheline += kOpsPerCacheLine;\n      len -= kCacheLineSize;\n    }\n    d += loops * kCacheLineSize;\n    s += loops * kCacheLineSize;\n    _mm_sfence();\n  }\n\n  // memcpy the tail.\n  if (len) {\n    memcpy(d, s, len);\n  }\n  return dst;\n#else\n  // Fallback to regular memcpy.\n  return memcpy(dst, src, len);\n#endif  // __SSE3__ || __aarch64__ || (_MSC_VER && __AVX__)\n}\n\ninline void *non_temporal_store_memcpy_avx(void *__restrict dst,\n                                           const void *__restrict src,\n                                           size_t len) {\n#ifdef __AVX__\n  uint8_t *d = reinterpret_cast<uint8_t *>(dst);\n  const uint8_t *s = reinterpret_cast<const uint8_t *>(src);\n\n  // memcpy() the misaligned header. At the end of this if block, <d> is\n  // aligned to a 64-byte cacheline boundary or <len> == 0.\n  if (reinterpret_cast<uintptr_t>(d) & (kCacheLineSize - 1)) {\n    uintptr_t bytes_before_alignment_boundary =\n        kCacheLineSize -\n        (reinterpret_cast<uintptr_t>(d) & (kCacheLineSize - 1));\n    size_t header_len = (std::min)(bytes_before_alignment_boundary, len);\n    assert(bytes_before_alignment_boundary < kCacheLineSize);\n    memcpy(d, s, header_len);\n    d += header_len;\n    s += header_len;\n    len -= header_len;\n  }\n\n  if (len >= kCacheLineSize) {\n    _mm_sfence();\n    __m256i *dst_cacheline = reinterpret_cast<__m256i *>(d);\n    const __m256i *src_cacheline = reinterpret_cast<const __m256i *>(s);\n    constexpr int kOpsPerCacheLine = kCacheLineSize / sizeof(__m256i);\n    size_t loops = len / kCacheLineSize;\n\n    while (len >= kCacheLineSize) {\n      __m256i temp1, temp2;\n      temp1 = _mm256_lddqu_si256(src_cacheline + 0);\n      temp2 = _mm256_lddqu_si256(src_cacheline + 1);\n      _mm256_stream_si256(dst_cacheline + 0, temp1);\n      _mm256_stream_si256(dst_cacheline + 1, temp2);\n      src_cacheline += kOpsPerCacheLine;\n      dst_cacheline += kOpsPerCacheLine;\n      len -= kCacheLineSize;\n    }\n    d += loops * kCacheLineSize;\n    s += loops * kCacheLineSize;\n    _mm_sfence();\n  }\n\n  // memcpy the tail.\n  if (len) {\n    memcpy(d, s, len);\n  }\n  return dst;\n#else\n  // Fallback to regular memcpy when AVX is not available.\n  return memcpy(dst, src, len);\n#endif  // __AVX__\n}\n\n}\n}\n\n} // extern \"C\"\n",
  "id": "BLOCK-CPP-05447",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/crc/internal/non_temporal_memcpy.h",
  "source_line": 52,
  "validation_status": "validated"
}