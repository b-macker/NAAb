{
  "code": "{\nABSL_NAMESPACE_BEGIN\nnamespace container_internal {\n\n// We have space for `growth_left` before a single block of control bytes. A\n// single block of empty control bytes for tables without any slots allocated.\n// This enables removing a branch in the hot path of find(). In order to ensure\n// that the control bytes are aligned to 16, we have 16 bytes before the control\n// bytes even though growth_left only needs 8.\nconstexpr ctrl_t ZeroCtrlT() { return static_cast<ctrl_t>(0); }\nalignas(16) ABSL_CONST_INIT ABSL_DLL const ctrl_t kEmptyGroup[32] = {\n    ZeroCtrlT(),       ZeroCtrlT(),    ZeroCtrlT(),    ZeroCtrlT(),\n    ZeroCtrlT(),       ZeroCtrlT(),    ZeroCtrlT(),    ZeroCtrlT(),\n    ZeroCtrlT(),       ZeroCtrlT(),    ZeroCtrlT(),    ZeroCtrlT(),\n    ZeroCtrlT(),       ZeroCtrlT(),    ZeroCtrlT(),    ZeroCtrlT(),\n    ctrl_t::kSentinel, ctrl_t::kEmpty, ctrl_t::kEmpty, ctrl_t::kEmpty,\n    ctrl_t::kEmpty,    ctrl_t::kEmpty, ctrl_t::kEmpty, ctrl_t::kEmpty,\n    ctrl_t::kEmpty,    ctrl_t::kEmpty, ctrl_t::kEmpty, ctrl_t::kEmpty,\n    ctrl_t::kEmpty,    ctrl_t::kEmpty, ctrl_t::kEmpty, ctrl_t::kEmpty};\n\n#ifdef ABSL_INTERNAL_NEED_REDUNDANT_CONSTEXPR_DECL\nconstexpr size_t Group::kWidth;\n#endif\n\nnamespace {\n\n// Returns \"random\" seed.\ninline size_t RandomSeed() {\n#ifdef ABSL_HAVE_THREAD_LOCAL\n  static thread_local size_t counter = 0;\n  // On Linux kernels >= 5.4 the MSAN runtime has a false-positive when\n  // accessing thread local storage data from loaded libraries\n  // (https://github.com/google/sanitizers/issues/1265), for this reason counter\n  // needs to be annotated as initialized.\n  ABSL_ANNOTATE_MEMORY_IS_INITIALIZED(&counter, sizeof(size_t));\n  size_t value = ++counter;\n#else   // ABSL_HAVE_THREAD_LOCAL\n  static std::atomic<size_t> counter(0);\n  size_t value = counter.fetch_add(1, std::memory_order_relaxed);\n#endif  // ABSL_HAVE_THREAD_LOCAL\n  return value ^ static_cast<size_t>(reinterpret_cast<uintptr_t>(&counter));\n}\n\nbool ShouldRehashForBugDetection(const ctrl_t* ctrl, size_t capacity) {\n  // Note: we can't use the abseil-random library because abseil-random\n  // depends on swisstable. We want to return true with probability\n  // `min(1, RehashProbabilityConstant() / capacity())`. In order to do this,\n  // we probe based on a random hash and see if the offset is less than\n  // RehashProbabilityConstant().\n  return probe(ctrl, capacity, absl::HashOf(RandomSeed())).offset() <\n         RehashProbabilityConstant();\n}\n\n}  // namespace\n\nGenerationType* EmptyGeneration() {\n  if (SwisstableGenerationsEnabled()) {\n    constexpr size_t kNumEmptyGenerations = 1024;\n    static constexpr GenerationType kEmptyGenerations[kNumEmptyGenerations]{};\n    return const_cast<GenerationType*>(\n        &kEmptyGenerations[RandomSeed() % kNumEmptyGenerations]);\n  }\n  return nullptr;\n}\n\nbool CommonFieldsGenerationInfoEnabled::\n    should_rehash_for_bug_detection_on_insert(const ctrl_t* ctrl,\n                                              size_t capacity) const {\n  if (reserved_growth_ == kReservedGrowthJustRanOut) return true;\n  if (reserved_growth_ > 0) return false;\n  return ShouldRehashForBugDetection(ctrl, capacity);\n}\n\nbool CommonFieldsGenerationInfoEnabled::should_rehash_for_bug_detection_on_move(\n    const ctrl_t* ctrl, size_t capacity) const {\n  return ShouldRehashForBugDetection(ctrl, capacity);\n}\n\nbool ShouldInsertBackwards(size_t hash, const ctrl_t* ctrl) {\n  // To avoid problems with weak hashes and single bit tests, we use % 13.\n  // TODO(kfm,sbenza): revisit after we do unconditional mixing\n  return (H1(hash, ctrl) ^ RandomSeed()) % 13 > 6;\n}\n\nvoid ConvertDeletedToEmptyAndFullToDeleted(ctrl_t* ctrl, size_t capacity) {\n  assert(ctrl[capacity] == ctrl_t::kSentinel);\n  assert(IsValidCapacity(capacity));\n  for (ctrl_t* pos = ctrl; pos < ctrl + capacity; pos += Group::kWidth) {\n    Group{pos}.ConvertSpecialToEmptyAndFullToDeleted(pos);\n  }\n  // Copy the cloned ctrl bytes.\n  std::memcpy(ctrl + capacity + 1, ctrl, NumClonedBytes());\n  ctrl[capacity] = ctrl_t::kSentinel;\n}\n// Extern template instantiation for inline function.\ntemplate FindInfo find_first_non_full(const CommonFields&, size_t);\n\nFindInfo find_first_non_full_outofline(const CommonFields& common,\n                                       size_t hash) {\n  return find_first_non_full(common, hash);\n}\n\n// Returns the address of the slot just after slot assuming each slot has the\n// specified size.\nstatic inline void* NextSlot(void* slot, size_t slot_size) {\n  return reinterpret_cast<void*>(reinterpret_cast<uintptr_t>(slot) + slot_size);\n}\n\n// Returns the address of the slot just before slot assuming each slot has the\n// specified size.\nstatic inline void* PrevSlot(void* slot, size_t slot_size) {\n  return reinterpret_cast<void*>(reinterpret_cast<uintptr_t>(slot) - slot_size);\n}\n\nvoid DropDeletesWithoutResize(CommonFields& common,\n                              const PolicyFunctions& policy, void* tmp_space) {\n  void* set = &common;\n  void* slot_array = common.slot_array();\n  const size_t capacity = common.capacity();\n  assert(IsValidCapacity(capacity));\n  assert(!is_small(capacity));\n  // Algorithm:\n  // - mark all DELETED slots as EMPTY\n  // - mark all FULL slots as DELETED\n  // - for each slot marked as DELETED\n  //     hash = Hash(element)\n  //     target = find_first_non_full(hash)\n  //     if target is in the same group\n  //       mark slot as FULL\n  //     else if target is EMPTY\n  //       transfer element to target\n  //       mark slot as EMPTY\n  //       mark target as FULL\n  //     else if target is DELETED\n  //       swap current element with target element\n  //       mark target as FULL\n  //       repeat procedure for current slot with moved from element (target)\n  ctrl_t* ctrl = common.control();\n  ConvertDeletedToEmptyAndFullToDeleted(ctrl, capacity);\n  auto hasher = policy.hash_slot;\n  auto transfer = policy.transfer;\n  const size_t slot_size = policy.slot_size;\n\n  size_t total_probe_length = 0;\n  void* slot_ptr = SlotAddress(slot_array, 0, slot_size);\n  for (size_t i = 0; i != capacity;\n       ++i, slot_ptr = NextSlot(slot_ptr, slot_size)) {\n    assert(slot_ptr == SlotAddress(slot_array, i, slot_size));\n    if (!IsDeleted(ctrl[i])) continue;\n    const size_t hash = (*hasher)(set, slot_ptr);\n    const FindInfo target = find_first_non_full(common, hash);\n    const size_t new_i = target.offset;\n    total_probe_length += target.probe_length;\n\n    // Verify if the old and new i fall within the same group wrt the hash.\n    // If they do, we don't need to move the object as it falls already in the\n    // best probe we can.\n    const size_t probe_offset = probe(common, hash).offset();\n    const auto probe_index = [probe_offset, capacity](size_t pos) {\n      return ((pos - probe_offset) & capacity) / Group::kWidth;\n    };\n\n    // Element doesn't move.\n    if (ABSL_PREDICT_TRUE(probe_index(new_i) == probe_index(i))) {\n      SetCtrl(common, i, H2(hash), slot_size);\n      continue;\n    }\n\n    void* new_slot_ptr = SlotAddress(slot_array, new_i, slot_size);\n    if (IsEmpty(ctrl[new_i])) {\n      // Transfer element to the empty spot.\n      // SetCtrl poisons/unpoisons the slots so we have to call it at the\n      // right time.\n      SetCtrl(common, new_i, H2(hash), slot_size);\n      (*transfer)(set, new_slot_ptr, slot_ptr);\n      SetCtrl(common, i, ctrl_t::kEmpty, slot_size);\n    } else {\n      assert(IsDeleted(ctrl[new_i]));\n      SetCtrl(common, new_i, H2(hash), slot_size);\n      // Until we are done rehashing, DELETED marks previously FULL slots.\n\n      // Swap i and new_i elements.\n      (*transfer)(set, tmp_space, new_slot_ptr);\n      (*transfer)(set, new_slot_ptr, slot_ptr);\n      (*transfer)(set, slot_ptr, tmp_space);\n\n      // repeat the processing of the ith slot\n      --i;\n      slot_ptr = PrevSlot(slot_ptr, slot_size);\n    }\n  }\n  ResetGrowthLeft(common);\n  common.infoz().RecordRehash(total_probe_length);\n}\n\nstatic bool WasNeverFull(CommonFields& c, size_t index) {\n  if (is_single_group(c.capacity())) {\n    return true;\n  }\n  const size_t index_before = (index - Group::kWidth) & c.capacity();\n  const auto empty_after = Group(c.control() + index).MaskEmpty();\n  const auto empty_before = Group(c.control() + index_before).MaskEmpty();\n\n  // We count how many consecutive non empties we have to the right and to the\n  // left of `it`. If the sum is >= kWidth then there is at least one probe\n  // window that might have seen a full group.\n  return empty_before && empty_after &&\n         static_cast<size_t>(empty_after.TrailingZeros()) +\n                 empty_before.LeadingZeros() <\n             Group::kWidth;\n}\n\nvoid EraseMetaOnly(CommonFields& c, size_t index, size_t slot_size) {\n  assert(IsFull(c.control()[index]) && \"erasing a dangling iterator\");\n  c.decrement_size();\n  c.infoz().RecordErase();\n\n  if (WasNeverFull(c, index)) {\n    SetCtrl(c, index, ctrl_t::kEmpty, slot_size);\n    c.set_growth_left(c.growth_left() + 1);\n    return;\n  }\n\n  SetCtrl(c, index, ctrl_t::kDeleted, slot_size);\n}\n\nvoid ClearBackingArray(CommonFields& c, const PolicyFunctions& policy,\n                       bool reuse) {\n  c.set_size(0);\n  if (reuse) {\n    ResetCtrl(c, policy.slot_size);\n    ResetGrowthLeft(c);\n    c.infoz().RecordStorageChanged(0, c.capacity());\n  } else {\n    // We need to record infoz before calling dealloc, which will unregister\n    // infoz.\n    c.infoz().RecordClearedReservation();\n    c.infoz().RecordStorageChanged(0, 0);\n    (*policy.dealloc)(c, policy);\n    c.set_control(EmptyGroup());\n    c.set_generation_ptr(EmptyGeneration());\n    c.set_slots(nullptr);\n    c.set_capacity(0);\n  }\n}\n\nvoid HashSetResizeHelper::GrowIntoSingleGroupShuffleControlBytes(\n    ctrl_t* new_ctrl, size_t new_capacity) const {\n  assert(is_single_group(new_capacity));\n  constexpr size_t kHalfWidth = Group::kWidth / 2;\n  assert(old_capacity_ < kHalfWidth);\n\n  const size_t half_old_capacity = old_capacity_ / 2;\n\n  // NOTE: operations are done with compile time known size = kHalfWidth.\n  // Compiler optimizes that into single ASM operation.\n\n  // Copy second half of bytes to the beginning.\n  // We potentially copy more bytes in order to have compile time known size.\n  // Mirrored bytes from the old_ctrl_ will also be copied.\n  // In case of old_capacity_ == 3, we will copy 1st element twice.\n  // Examples:\n  // old_ctrl = 0S0EEEEEEE...\n  // new_ctrl = S0EEEEEEEE...\n  //\n  // old_ctrl = 01S01EEEEE...\n  // new_ctrl = 1S01EEEEEE...\n  //\n  // old_ctrl = 0123456S0123456EE...\n  // new_ctrl = 456S0123?????????...\n  std::memcpy(new_ctrl, old_ctrl_ + half_old_capacity + 1, kHalfWidth);\n  // Clean up copied kSentinel from old_ctrl.\n  new_ctrl[half_old_capacity] = ctrl_t::kEmpty;\n\n  // Clean up damaged or uninitialized bytes.\n\n  // Clean bytes after the intended size of the copy.\n  // Example:\n  // new_ctrl = 1E01EEEEEEE????\n  // *new_ctrl= 1E0EEEEEEEE????\n  // position      /\n  std::memset(new_ctrl + old_capacity_ + 1, static_cast<int8_t>(ctrl_t::kEmpty),\n              kHalfWidth);\n  // Clean non-mirrored bytes that are not initialized.\n  // For small old_capacity that may be inside of mirrored bytes zone.\n  // Examples:\n  // new_ctrl = 1E0EEEEEEEE??????????....\n  // *new_ctrl= 1E0EEEEEEEEEEEEE?????....\n  // position           /\n  //\n  // new_ctrl = 456E0123???????????...\n  // *new_ctrl= 456E0123EEEEEEEE???...\n  // position           /\n  std::memset(new_ctrl + kHalfWidth, static_cast<int8_t>(ctrl_t::kEmpty),\n              kHalfWidth);\n  // Clean last mirrored bytes that are not initialized\n  // and will not be overwritten by mirroring.\n  // Examples:\n  // new_ctrl = 1E0EEEEEEEEEEEEE????????\n  // *new_ctrl= 1E0EEEEEEEEEEEEEEEEEEEEE\n  // position           S       /\n  //\n  // new_ctrl = 456E0123EEEEEEEE???????????????\n  // *new_ctrl= 456E0123EEEEEEEE???????EEEEEEEE\n  // position                  S       /\n  std::memset(new_ctrl + new_capacity + kHalfWidth,\n              static_cast<int8_t>(ctrl_t::kEmpty), kHalfWidth);\n\n  // Create mirrored bytes. old_capacity_ < kHalfWidth\n  // Example:\n  // new_ctrl = 456E0123EEEEEEEE???????EEEEEEEE\n  // *new_ctrl= 456E0123EEEEEEEE456E0123EEEEEEE\n  // position                  S/\n  ctrl_t g[kHalfWidth];\n  std::memcpy(g, new_ctrl, kHalfWidth);\n  std::memcpy(new_ctrl + new_capacity + 1, g, kHalfWidth);\n\n  // Finally set sentinel to its place.\n  new_ctrl[new_capacity] = ctrl_t::kSentinel;\n}\n\nvoid HashSetResizeHelper::GrowIntoSingleGroupShuffleTransferableSlots(\n    void* old_slots, void* new_slots, size_t slot_size) const {\n  assert(old_capacity_ > 0);\n  const size_t half_old_capacity = old_capacity_ / 2;\n\n  SanitizerUnpoisonMemoryRegion(old_slots, slot_size * old_capacity_);\n  std::memcpy(new_slots,\n              SlotAddress(old_slots, half_old_capacity + 1, slot_size),\n              slot_size * half_old_capacity);\n  std::memcpy(SlotAddress(new_slots, half_old_capacity + 1, slot_size),\n              old_slots, slot_size * (half_old_capacity + 1));\n}\n\nvoid HashSetResizeHelper::GrowSizeIntoSingleGroupTransferable(\n    CommonFields& c, void* old_slots, size_t slot_size) {\n  assert(old_capacity_ < Group::kWidth / 2);\n  assert(is_single_group(c.capacity()));\n  assert(IsGrowingIntoSingleGroupApplicable(old_capacity_, c.capacity()));\n\n  GrowIntoSingleGroupShuffleControlBytes(c.control(), c.capacity());\n  GrowIntoSingleGroupShuffleTransferableSlots(old_slots, c.slot_array(),\n                                              slot_size);\n\n  // We poison since GrowIntoSingleGroupShuffleTransferableSlots\n  // may leave empty slots unpoisoned.\n  PoisonSingleGroupEmptySlots(c, slot_size);\n}\n\n}  // namespace container_internal\nABSL_NAMESPACE_END\n}",
  "id": "BLOCK-CPP-02436",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/container/internal/raw_hash_set.cc",
  "source_line": 29,
  "validation_status": "validated"
}