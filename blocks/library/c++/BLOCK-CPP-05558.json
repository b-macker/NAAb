{
  "code": "{\nABSL_NAMESPACE_BEGIN\nnamespace flags_internal {\n\n// Align 'x' up to the nearest 'align' bytes.\ninline constexpr size_t AlignUp(size_t x, size_t align) {\n  return align * ((x + align - 1) / align);\n}\n\n// A SequenceLock implements lock-free reads. A sequence counter is incremented\n// before and after each write, and readers access the counter before and after\n// accessing the protected data. If the counter is verified to not change during\n// the access, and the sequence counter value was even, then the reader knows\n// that the read was race-free and valid. Otherwise, the reader must fall back\n// to a Mutex-based code path.\n//\n// This particular SequenceLock starts in an \"uninitialized\" state in which\n// TryRead() returns false. It must be enabled by calling MarkInitialized().\n// This serves as a marker that the associated flag value has not yet been\n// initialized and a slow path needs to be taken.\n//\n// The memory reads and writes protected by this lock must use the provided\n// `TryRead()` and `Write()` functions. These functions behave similarly to\n// `memcpy()`, with one oddity: the protected data must be an array of\n// `std::atomic<uint64>`. This is to comply with the C++ standard, which\n// considers data races on non-atomic objects to be undefined behavior. See \"Can\n// Seqlocks Get Along With Programming Language Memory Models?\"[1] by Hans J.\n// Boehm for more details.\n//\n// [1] https://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf\nclass SequenceLock {\n public:\n  constexpr SequenceLock() : lock_(kUninitialized) {}\n\n  // Mark that this lock is ready for use.\n  void MarkInitialized() {\n    assert(lock_.load(std::memory_order_relaxed) == kUninitialized);\n    lock_.store(0, std::memory_order_release);\n  }\n\n  // Copy \"size\" bytes of data from \"src\" to \"dst\", protected as a read-side\n  // critical section of the sequence lock.\n  //\n  // Unlike traditional sequence lock implementations which loop until getting a\n  // clean read, this implementation returns false in the case of concurrent\n  // calls to `Write`. In such a case, the caller should fall back to a\n  // locking-based slow path.\n  //\n  // Returns false if the sequence lock was not yet marked as initialized.\n  //\n  // NOTE: If this returns false, \"dst\" may be overwritten with undefined\n  // (potentially uninitialized) data.\n  bool TryRead(void* dst, const std::atomic<uint64_t>* src, size_t size) const {\n    // Acquire barrier ensures that no loads done by f() are reordered\n    // above the first load of the sequence counter.\n    int64_t seq_before = lock_.load(std::memory_order_acquire);\n    if (ABSL_PREDICT_FALSE(seq_before & 1) == 1) return false;\n    RelaxedCopyFromAtomic(dst, src, size);\n    // Another acquire fence ensures that the load of 'lock_' below is\n    // strictly ordered after the RelaxedCopyToAtomic call above.\n    std::atomic_thread_fence(std::memory_order_acquire);\n    int64_t seq_after = lock_.load(std::memory_order_relaxed);\n    return ABSL_PREDICT_TRUE(seq_before == seq_after);\n  }\n\n  // Copy \"size\" bytes from \"src\" to \"dst\" as a write-side critical section\n  // of the sequence lock. Any concurrent readers will be forced to retry\n  // until they get a read that does not conflict with this write.\n  //\n  // This call must be externally synchronized against other calls to Write,\n  // but may proceed concurrently with reads.\n  void Write(std::atomic<uint64_t>* dst, const void* src, size_t size) {\n    // We can use relaxed instructions to increment the counter since we\n    // are extenally synchronized. The std::atomic_thread_fence below\n    // ensures that the counter updates don't get interleaved with the\n    // copy to the data.\n    int64_t orig_seq = lock_.load(std::memory_order_relaxed);\n    assert((orig_seq & 1) == 0);  // Must be initially unlocked.\n    lock_.store(orig_seq + 1, std::memory_order_relaxed);\n\n    // We put a release fence between update to lock_ and writes to shared data.\n    // Thus all stores to shared data are effectively release operations and\n    // update to lock_ above cannot be re-ordered past any of them. Note that\n    // this barrier is not for the fetch_add above.  A release barrier for the\n    // fetch_add would be before it, not after.\n    std::atomic_thread_fence(std::memory_order_release);\n    RelaxedCopyToAtomic(dst, src, size);\n    // \"Release\" semantics ensure that none of the writes done by\n    // RelaxedCopyToAtomic() can be reordered after the following modification.\n    lock_.store(orig_seq + 2, std::memory_order_release);\n  }\n\n  // Return the number of times that Write() has been called.\n  //\n  // REQUIRES: This must be externally synchronized against concurrent calls to\n  // `Write()` or `IncrementModificationCount()`.\n  // REQUIRES: `MarkInitialized()` must have been previously called.\n  int64_t ModificationCount() const {\n    int64_t val = lock_.load(std::memory_order_relaxed);\n    assert(val != kUninitialized && (val & 1) == 0);\n    return val / 2;\n  }\n\n  // REQUIRES: This must be externally synchronized against concurrent calls to\n  // `Write()` or `ModificationCount()`.\n  // REQUIRES: `MarkInitialized()` must have been previously called.\n  void IncrementModificationCount() {\n    int64_t val = lock_.load(std::memory_order_relaxed);\n    assert(val != kUninitialized);\n    lock_.store(val + 2, std::memory_order_relaxed);\n  }\n\n private:\n  // Perform the equivalent of \"memcpy(dst, src, size)\", but using relaxed\n  // atomics.\n  static void RelaxedCopyFromAtomic(void* dst, const std::atomic<uint64_t>* src,\n                                    size_t size) {\n    char* dst_byte = static_cast<char*>(dst);\n    while (size >= sizeof(uint64_t)) {\n      uint64_t word = src->load(std::memory_order_relaxed);\n      std::memcpy(dst_byte, &word, sizeof(word));\n      dst_byte += sizeof(word);\n      src++;\n      size -= sizeof(word);\n    }\n    if (size > 0) {\n      uint64_t word = src->load(std::memory_order_relaxed);\n      std::memcpy(dst_byte, &word, size);\n    }\n  }\n\n  // Perform the equivalent of \"memcpy(dst, src, size)\", but using relaxed\n  // atomics.\n  static void RelaxedCopyToAtomic(std::atomic<uint64_t>* dst, const void* src,\n                                  size_t size) {\n    const char* src_byte = static_cast<const char*>(src);\n    while (size >= sizeof(uint64_t)) {\n      uint64_t word;\n      std::memcpy(&word, src_byte, sizeof(word));\n      dst->store(word, std::memory_order_relaxed);\n      src_byte += sizeof(word);\n      dst++;\n      size -= sizeof(word);\n    }\n    if (size > 0) {\n      uint64_t word = 0;\n      std::memcpy(&word, src_byte, size);\n      dst->store(word, std::memory_order_relaxed);\n    }\n  }\n\n  static constexpr int64_t kUninitialized = -1;\n  std::atomic<int64_t> lock_;\n};\n\n}  // namespace flags_internal\nABSL_NAMESPACE_END\n}",
  "id": "BLOCK-CPP-05558",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/flags/internal/sequence_lock.h",
  "source_line": 28,
  "validation_status": "validated"
}