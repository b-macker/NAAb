{
  "code": "{\nABSL_NAMESPACE_BEGIN\nnamespace crc_internal {\n\nnamespace {\n\ninline crc32c_t ShortCrcCopy(char* dst, const char* src, std::size_t length,\n                             crc32c_t crc) {\n  // Small copy: just go 1 byte at a time: being nice to the branch predictor\n  // is more important here than anything else\n  uint32_t crc_uint32 = static_cast<uint32_t>(crc);\n  for (std::size_t i = 0; i < length; i++) {\n    uint8_t data = *reinterpret_cast<const uint8_t*>(src);\n    crc_uint32 = CRC32_u8(crc_uint32, data);\n    *reinterpret_cast<uint8_t*>(dst) = data;\n    ++src;\n    ++dst;\n  }\n  return crc32c_t{crc_uint32};\n}\n\nconstexpr size_t kIntLoadsPerVec = sizeof(V128) / sizeof(uint64_t);\n\n// Common function for copying the tails of multiple large regions.\ntemplate <size_t vec_regions, size_t int_regions>\ninline void LargeTailCopy(crc32c_t* crcs, char** dst, const char** src,\n                          size_t region_size, size_t copy_rounds) {\n  std::array<V128, vec_regions> data;\n  std::array<uint64_t, kIntLoadsPerVec * int_regions> int_data;\n\n  while (copy_rounds > 0) {\n    for (size_t i = 0; i < vec_regions; i++) {\n      size_t region = i;\n\n      auto* vsrc = reinterpret_cast<const V128*>(*src + region_size * region);\n      auto* vdst = reinterpret_cast<V128*>(*dst + region_size * region);\n\n      // Load the blocks, unaligned\n      data[i] = V128_LoadU(vsrc);\n\n      // Store the blocks, aligned\n      V128_Store(vdst, data[i]);\n\n      // Compute the running CRC\n      crcs[region] = crc32c_t{static_cast<uint32_t>(\n          CRC32_u64(static_cast<uint32_t>(crcs[region]),\n                    static_cast<uint64_t>(V128_Extract64<0>(data[i]))))};\n      crcs[region] = crc32c_t{static_cast<uint32_t>(\n          CRC32_u64(static_cast<uint32_t>(crcs[region]),\n                    static_cast<uint64_t>(V128_Extract64<1>(data[i]))))};\n    }\n\n    for (size_t i = 0; i < int_regions; i++) {\n      size_t region = vec_regions + i;\n\n      auto* usrc =\n          reinterpret_cast<const uint64_t*>(*src + region_size * region);\n      auto* udst = reinterpret_cast<uint64_t*>(*dst + region_size * region);\n\n      for (size_t j = 0; j < kIntLoadsPerVec; j++) {\n        size_t data_index = i * kIntLoadsPerVec + j;\n\n        int_data[data_index] = *(usrc + j);\n        crcs[region] = crc32c_t{static_cast<uint32_t>(CRC32_u64(\n            static_cast<uint32_t>(crcs[region]), int_data[data_index]))};\n\n        *(udst + j) = int_data[data_index];\n      }\n    }\n\n    // Increment pointers\n    *src += sizeof(V128);\n    *dst += sizeof(V128);\n    --copy_rounds;\n  }\n}\n\n}  // namespace\n\ntemplate <size_t vec_regions, size_t int_regions>\nclass AcceleratedCrcMemcpyEngine : public CrcMemcpyEngine {\n public:\n  AcceleratedCrcMemcpyEngine() = default;\n  AcceleratedCrcMemcpyEngine(const AcceleratedCrcMemcpyEngine&) = delete;\n  AcceleratedCrcMemcpyEngine operator=(const AcceleratedCrcMemcpyEngine&) =\n      delete;\n\n  crc32c_t Compute(void* __restrict dst, const void* __restrict src,\n                   std::size_t length, crc32c_t initial_crc) const override;\n};\n\ntemplate <size_t vec_regions, size_t int_regions>\ncrc32c_t AcceleratedCrcMemcpyEngine<vec_regions, int_regions>::Compute(\n    void* __restrict dst, const void* __restrict src, std::size_t length,\n    crc32c_t initial_crc) const {\n  constexpr std::size_t kRegions = vec_regions + int_regions;\n  static_assert(kRegions > 0, \"Must specify at least one region.\");\n  constexpr uint32_t kCrcDataXor = uint32_t{0xffffffff};\n  constexpr std::size_t kBlockSize = sizeof(V128);\n  constexpr std::size_t kCopyRoundSize = kRegions * kBlockSize;\n\n  // Number of blocks per cacheline.\n  constexpr std::size_t kBlocksPerCacheLine = ABSL_CACHELINE_SIZE / kBlockSize;\n\n  char* dst_bytes = static_cast<char*>(dst);\n  const char* src_bytes = static_cast<const char*>(src);\n\n  // Make sure that one prefetch per big block is enough to cover the whole\n  // dataset, and we don't prefetch too much.\n  static_assert(ABSL_CACHELINE_SIZE % kBlockSize == 0,\n                \"Cache lines are not divided evenly into blocks, may have \"\n                \"unintended behavior!\");\n\n  // Experimentally-determined boundary between a small and large copy.\n  // Below this number, spin-up and concatenation of CRCs takes enough time that\n  // it kills the throughput gains of using 3 regions and wide vectors.\n  constexpr size_t kCrcSmallSize = 256;\n\n  // Experimentally-determined prefetch distance.  Main loop copies will\n  // prefeth data 2 cache lines ahead.\n  constexpr std::size_t kPrefetchAhead = 2 * ABSL_CACHELINE_SIZE;\n\n  // Small-size CRC-memcpy : just do CRC + memcpy\n  if (length < kCrcSmallSize) {\n    crc32c_t crc =\n        ExtendCrc32c(initial_crc, absl::string_view(src_bytes, length));\n    memcpy(dst, src, length);\n    return crc;\n  }\n\n  // Start work on the CRC: undo the XOR from the previous calculation or set up\n  // the initial value of the CRC.\n  // initial_crc ^= kCrcDataXor;\n  initial_crc = crc32c_t{static_cast<uint32_t>(initial_crc) ^ kCrcDataXor};\n\n  // Do an initial alignment copy, so we can use aligned store instructions to\n  // the destination pointer.  We align the destination pointer because the\n  // penalty for an unaligned load is small compared to the penalty of an\n  // unaligned store on modern CPUs.\n  std::size_t bytes_from_last_aligned =\n      reinterpret_cast<uintptr_t>(dst) & (kBlockSize - 1);\n  if (bytes_from_last_aligned != 0) {\n    std::size_t bytes_for_alignment = kBlockSize - bytes_from_last_aligned;\n\n    // Do the short-sized copy and CRC.\n    initial_crc =\n        ShortCrcCopy(dst_bytes, src_bytes, bytes_for_alignment, initial_crc);\n    src_bytes += bytes_for_alignment;\n    dst_bytes += bytes_for_alignment;\n    length -= bytes_for_alignment;\n  }\n\n  // We are going to do the copy and CRC in kRegions regions to make sure that\n  // we can saturate the CRC unit.  The CRCs will be combined at the end of the\n  // run.  Copying will use the SSE registers, and we will extract words from\n  // the SSE registers to add to the CRC.  Initially, we run the loop one full\n  // cache line per region at a time, in order to insert prefetches.\n\n  // Initialize CRCs for kRegions regions.\n  crc32c_t crcs[kRegions];\n  crcs[0] = initial_crc;\n  for (size_t i = 1; i < kRegions; i++) {\n    crcs[i] = crc32c_t{kCrcDataXor};\n  }\n\n  // Find the number of rounds to copy and the region size.  Also compute the\n  // tail size here.\n  size_t copy_rounds = length / kCopyRoundSize;\n\n  // Find the size of each region and the size of the tail.\n  const std::size_t region_size = copy_rounds * kBlockSize;\n  const std::size_t tail_size = length - (kRegions * region_size);\n\n  // Holding registers for data in each region.\n  std::array<V128, vec_regions> vec_data;\n  std::array<uint64_t, int_regions * kIntLoadsPerVec> int_data;\n\n  // Main loop.\n  while (copy_rounds > kBlocksPerCacheLine) {\n    // Prefetch kPrefetchAhead bytes ahead of each pointer.\n    for (size_t i = 0; i < kRegions; i++) {\n      absl::PrefetchToLocalCache(src_bytes + kPrefetchAhead + region_size * i);\n#ifdef ABSL_INTERNAL_HAVE_X86_64_ACCELERATED_CRC_MEMCPY_ENGINE\n      // TODO(b/297082454): investigate dropping prefetch on x86.\n      absl::PrefetchToLocalCache(dst_bytes + kPrefetchAhead + region_size * i);\n#endif\n    }\n\n    // Load and store data, computing CRC on the way.\n    for (size_t i = 0; i < kBlocksPerCacheLine; i++) {\n      // Copy and CRC the data for the CRC regions.\n      for (size_t j = 0; j < vec_regions; j++) {\n        // Cycle which regions get vector load/store and integer load/store, to\n        // engage prefetching logic around vector load/stores and save issue\n        // slots by using the integer registers.\n        size_t region = (j + i) % kRegions;\n\n        auto* vsrc =\n            reinterpret_cast<const V128*>(src_bytes + region_size * region);\n        auto* vdst = reinterpret_cast<V128*>(dst_bytes + region_size * region);\n\n        // Load and CRC data.\n        vec_data[j] = V128_LoadU(vsrc + i);\n        crcs[region] = crc32c_t{static_cast<uint32_t>(\n            CRC32_u64(static_cast<uint32_t>(crcs[region]),\n                      static_cast<uint64_t>(V128_Extract64<0>(vec_data[j]))))};\n        crcs[region] = crc32c_t{static_cast<uint32_t>(\n            CRC32_u64(static_cast<uint32_t>(crcs[region]),\n                      static_cast<uint64_t>(V128_Extract64<1>(vec_data[j]))))};\n\n        // Store the data.\n        V128_Store(vdst + i, vec_data[j]);\n      }\n\n      // Preload the partial CRCs for the CLMUL subregions.\n      for (size_t j = 0; j < int_regions; j++) {\n        // Cycle which regions get vector load/store and integer load/store, to\n        // engage prefetching logic around vector load/stores and save issue\n        // slots by using the integer registers.\n        size_t region = (j + vec_regions + i) % kRegions;\n\n        auto* usrc =\n            reinterpret_cast<const uint64_t*>(src_bytes + region_size * region);\n        auto* udst =\n            reinterpret_cast<uint64_t*>(dst_bytes + region_size * region);\n\n        for (size_t k = 0; k < kIntLoadsPerVec; k++) {\n          size_t data_index = j * kIntLoadsPerVec + k;\n\n          // Load and CRC the data.\n          int_data[data_index] = *(usrc + i * kIntLoadsPerVec + k);\n          crcs[region] = crc32c_t{static_cast<uint32_t>(CRC32_u64(\n              static_cast<uint32_t>(crcs[region]), int_data[data_index]))};\n\n          // Store the data.\n          *(udst + i * kIntLoadsPerVec + k) = int_data[data_index];\n        }\n      }\n    }\n\n    // Increment pointers\n    src_bytes += kBlockSize * kBlocksPerCacheLine;\n    dst_bytes += kBlockSize * kBlocksPerCacheLine;\n    copy_rounds -= kBlocksPerCacheLine;\n  }\n\n  // Copy and CRC the tails of each region.\n  LargeTailCopy<vec_regions, int_regions>(crcs, &dst_bytes, &src_bytes,\n                                          region_size, copy_rounds);\n\n  // Move the source and destination pointers to the end of the region\n  src_bytes += region_size * (kRegions - 1);\n  dst_bytes += region_size * (kRegions - 1);\n\n  // Copy and CRC the tail through the XMM registers.\n  std::size_t tail_blocks = tail_size / kBlockSize;\n  LargeTailCopy<0, 1>(&crcs[kRegions - 1], &dst_bytes, &src_bytes, 0,\n                      tail_blocks);\n\n  // Final tail copy for under 16 bytes.\n  crcs[kRegions - 1] =\n      ShortCrcCopy(dst_bytes, src_bytes, tail_size - tail_blocks * kBlockSize,\n                   crcs[kRegions - 1]);\n\n  if (kRegions == 1) {\n    // If there is only one region, finalize and return its CRC.\n    return crc32c_t{static_cast<uint32_t>(crcs[0]) ^ kCrcDataXor};\n  }\n\n  // Finalize the first CRCs: XOR the internal CRCs by the XOR mask to undo the\n  // XOR done before doing block copy + CRCs.\n  for (size_t i = 0; i + 1 < kRegions; i++) {\n    crcs[i] = crc32c_t{static_cast<uint32_t>(crcs[i]) ^ kCrcDataXor};\n  }\n\n  // Build a CRC of the first kRegions - 1 regions.\n  crc32c_t full_crc = crcs[0];\n  for (size_t i = 1; i + 1 < kRegions; i++) {\n    full_crc = ConcatCrc32c(full_crc, crcs[i], region_size);\n  }\n\n  // Finalize and concatenate the final CRC, then return.\n  crcs[kRegions - 1] =\n      crc32c_t{static_cast<uint32_t>(crcs[kRegions - 1]) ^ kCrcDataXor};\n  return ConcatCrc32c(full_crc, crcs[kRegions - 1], region_size + tail_size);\n}\n\nCrcMemcpy::ArchSpecificEngines CrcMemcpy::GetArchSpecificEngines() {\n#ifdef UNDEFINED_BEHAVIOR_SANITIZER\n  // UBSAN does not play nicely with unaligned loads (which we use a lot).\n  // Get the underlying architecture.\n  CpuType cpu_type = GetCpuType();\n  switch (cpu_type) {\n    case CpuType::kAmdRome:\n    case CpuType::kAmdNaples:\n    case CpuType::kAmdMilan:\n    case CpuType::kAmdGenoa:\n    case CpuType::kAmdRyzenV3000:\n    case CpuType::kIntelCascadelakeXeon:\n    case CpuType::kIntelSkylakeXeon:\n    case CpuType::kIntelSkylake:\n    case CpuType::kIntelBroadwell:\n    case CpuType::kIntelHaswell:\n    case CpuType::kIntelIvybridge:\n      return {\n          /*.temporal=*/new FallbackCrcMemcpyEngine(),\n          /*.non_temporal=*/new CrcNonTemporalMemcpyAVXEngine(),\n      };\n    // INTEL_SANDYBRIDGE performs better with SSE than AVX.\n    case CpuType::kIntelSandybridge:\n      return {\n          /*.temporal=*/new FallbackCrcMemcpyEngine(),\n          /*.non_temporal=*/new CrcNonTemporalMemcpyEngine(),\n      };\n    default:\n      return {/*.temporal=*/new FallbackCrcMemcpyEngine(),\n              /*.non_temporal=*/new FallbackCrcMemcpyEngine()};\n  }\n#else\n  // Get the underlying architecture.\n  CpuType cpu_type = GetCpuType();\n  switch (cpu_type) {\n    // On Zen 2, PEXTRQ uses 2 micro-ops, including one on the vector store port\n    // which data movement from the vector registers to the integer registers\n    // (where CRC32C happens) to crowd the same units as vector stores.  As a\n    // result, using that path exclusively causes bottlenecking on this port.\n    // We can avoid this bottleneck by using the integer side of the CPU for\n    // most operations rather than the vector side.  We keep a vector region to\n    // engage some of the prefetching logic in the cache hierarchy which seems\n    // to give vector instructions special treatment.  These prefetch units see\n    // strided access to each region, and do the right thing.\n    case CpuType::kAmdRome:\n    case CpuType::kAmdNaples:\n    case CpuType::kAmdMilan:\n    case CpuType::kAmdGenoa:\n    case CpuType::kAmdRyzenV3000:\n      return {\n          /*.temporal=*/new AcceleratedCrcMemcpyEngine<1, 2>(),\n          /*.non_temporal=*/new CrcNonTemporalMemcpyAVXEngine(),\n      };\n    // PCLMULQDQ is slow and we don't have wide enough issue width to take\n    // advantage of it.  For an unknown architecture, don't risk using CLMULs.\n    case CpuType::kIntelCascadelakeXeon:\n    case CpuType::kIntelSkylakeXeon:\n    case CpuType::kIntelSkylake:\n    case CpuType::kIntelBroadwell:\n    case CpuType::kIntelHaswell:\n    case CpuType::kIntelIvybridge:\n      return {\n          /*.temporal=*/new AcceleratedCrcMemcpyEngine<3, 0>(),\n          /*.non_temporal=*/new CrcNonTemporalMemcpyAVXEngine(),\n      };\n    // INTEL_SANDYBRIDGE performs better with SSE than AVX.\n    case CpuType::kIntelSandybridge:\n      return {\n          /*.temporal=*/new AcceleratedCrcMemcpyEngine<3, 0>(),\n          /*.non_temporal=*/new CrcNonTemporalMemcpyEngine(),\n      };\n    default:\n      return {/*.temporal=*/new FallbackCrcMemcpyEngine(),\n              /*.non_temporal=*/new FallbackCrcMemcpyEngine()};\n  }\n#endif  // UNDEFINED_BEHAVIOR_SANITIZER\n}\n\n// For testing, allow the user to specify which engine they want.\nstd::unique_ptr<CrcMemcpyEngine> CrcMemcpy::GetTestEngine(int vector,\n                                                          int integer) {\n  if (vector == 3 && integer == 0) {\n    return std::make_unique<AcceleratedCrcMemcpyEngine<3, 0>>();\n  } else if (vector == 1 && integer == 2) {\n    return std::make_unique<AcceleratedCrcMemcpyEngine<1, 2>>();\n  } else if (vector == 1 && integer == 0) {\n    return std::make_unique<AcceleratedCrcMemcpyEngine<1, 0>>();\n  }\n  return nullptr;\n}\n\n}  // namespace crc_internal\nABSL_NAMESPACE_END\n}",
  "id": "BLOCK-CPP-02481",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/crc/internal/crc_memcpy_x86_arm_combined.cc",
  "source_line": 67,
  "validation_status": "validated"
}