{
  "code": "{\n  static constexpr size_t kWidth = 8;\n\n  explicit GroupAArch64Impl(const ctrl_t* pos) {\n    ctrl = vld1_u8(reinterpret_cast<const uint8_t*>(pos));\n  }\n\n  BitMask<uint64_t, kWidth, 3> Match(h2_t hash) const {\n    uint8x8_t dup = vdup_n_u8(hash);\n    auto mask = vceq_u8(ctrl, dup);\n    return BitMask<uint64_t, kWidth, 3>(\n        vget_lane_u64(vreinterpret_u64_u8(mask), 0));\n  }\n\n  NonIterableBitMask<uint64_t, kWidth, 3> MaskEmpty() const {\n    uint64_t mask =\n        vget_lane_u64(vreinterpret_u64_u8(vceq_s8(\n                          vdup_n_s8(static_cast<int8_t>(ctrl_t::kEmpty)),\n                          vreinterpret_s8_u8(ctrl))),\n                      0);\n    return NonIterableBitMask<uint64_t, kWidth, 3>(mask);\n  }\n\n  // Returns a bitmask representing the positions of full slots.\n  // Note: for `is_small()` tables group may contain the \"same\" slot twice:\n  // original and mirrored.\n  BitMask<uint64_t, kWidth, 3> MaskFull() const {\n    uint64_t mask = vget_lane_u64(\n        vreinterpret_u64_u8(vcge_s8(vreinterpret_s8_u8(ctrl),\n                                    vdup_n_s8(static_cast<int8_t>(0)))),\n        0);\n    return BitMask<uint64_t, kWidth, 3>(mask);\n  }\n\n  NonIterableBitMask<uint64_t, kWidth, 3> MaskEmptyOrDeleted() const {\n    uint64_t mask =\n        vget_lane_u64(vreinterpret_u64_u8(vcgt_s8(\n                          vdup_n_s8(static_cast<int8_t>(ctrl_t::kSentinel)),\n                          vreinterpret_s8_u8(ctrl))),\n                      0);\n    return NonIterableBitMask<uint64_t, kWidth, 3>(mask);\n  }\n\n  uint32_t CountLeadingEmptyOrDeleted() const {\n    uint64_t mask =\n        vget_lane_u64(vreinterpret_u64_u8(vcle_s8(\n                          vdup_n_s8(static_cast<int8_t>(ctrl_t::kSentinel)),\n                          vreinterpret_s8_u8(ctrl))),\n                      0);\n    // Similar to MaskEmptyorDeleted() but we invert the logic to invert the\n    // produced bitfield. We then count number of trailing zeros.\n    // Clang and GCC optimize countr_zero to rbit+clz without any check for 0,\n    // so we should be fine.\n    return static_cast<uint32_t>(countr_zero(mask)) >> 3;\n  }\n\n  void ConvertSpecialToEmptyAndFullToDeleted(ctrl_t* dst) const {\n    uint64_t mask = vget_lane_u64(vreinterpret_u64_u8(ctrl), 0);\n    constexpr uint64_t msbs = 0x8080808080808080ULL;\n    constexpr uint64_t slsbs = 0x0202020202020202ULL;\n    constexpr uint64_t midbs = 0x7e7e7e7e7e7e7e7eULL;\n    auto x = slsbs & (mask >> 6);\n    auto res = (x + midbs) | msbs;\n    little_endian::Store64(dst, res);\n  }\n\n  uint8x8_t ctrl;\n}",
  "id": "BLOCK-CPP-05348",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/container/internal/raw_hash_set.h",
  "source_line": 681,
  "validation_status": "validated"
}