{
  "code": "{\nABSL_NAMESPACE_BEGIN\nnamespace cord_internal {\n\n#ifdef ABSL_INTERNAL_NEED_REDUNDANT_CONSTEXPR_DECL\nconstexpr size_t CordzInfo::kMaxStackDepth;\n#endif\n\nABSL_CONST_INIT CordzInfo::List CordzInfo::global_list_{absl::kConstInit};\n\nnamespace {\n\n// CordRepAnalyzer performs the analysis of a cord.\n//\n// It computes absolute node counts and total memory usage, and an 'estimated\n// fair share memory usage` statistic.\n// Conceptually, it divides the 'memory usage' at each location in the 'cord\n// graph' by the cumulative reference count of that location. The cumulative\n// reference count is the factored total of all edges leading into that node.\n//\n// The top level node is treated specially: we assume the current thread\n// (typically called from the CordzHandler) to hold a reference purely to\n// perform a safe analysis, and not being part of the application. So we\n// subtract 1 from the reference count of the top node to compute the\n// 'application fair share' excluding the reference of the current thread.\n//\n// An example of fair sharing, and why we multiply reference counts:\n// Assume we have 2 CordReps, both being a Substring referencing a Flat:\n//   CordSubstring A (refcount = 5) --> child Flat C (refcount = 2)\n//   CordSubstring B (refcount = 9) --> child Flat C (refcount = 2)\n//\n// Flat C has 2 incoming edges from the 2 substrings (refcount = 2) and is not\n// referenced directly anywhere else. Translated into a 'fair share', we then\n// attribute 50% of the memory (memory / refcount = 2) to each incoming edge.\n// Rep A has a refcount of 5, so we attribute each incoming edge 1 / 5th of the\n// memory cost below it, i.e.: the fair share of Rep A of the memory used by C\n// is then 'memory C / (refcount C * refcount A) + (memory A / refcount A)'.\n// It is also easy to see how all incoming edges add up to 100%.\nclass CordRepAnalyzer {\n public:\n  // Creates an analyzer instance binding to `statistics`.\n  explicit CordRepAnalyzer(CordzStatistics& statistics)\n      : statistics_(statistics) {}\n\n  // Analyzes the memory statistics and node counts for the provided `rep`, and\n  // adds the results to `statistics`. Note that node counts and memory sizes\n  // are not initialized, computed values are added to any existing values.\n  void AnalyzeCordRep(const CordRep* rep) {\n    ABSL_ASSERT(rep != nullptr);\n\n    // Process all linear nodes.\n    // As per the class comments, use refcout - 1 on the top level node, as the\n    // top level node is assumed to be referenced only for analysis purposes.\n    size_t refcount = rep->refcount.Get();\n    RepRef repref{rep, (refcount > 1) ? refcount - 1 : 1};\n\n    // Process the top level CRC node, if present.\n    if (repref.tag() == CRC) {\n      statistics_.node_count++;\n      statistics_.node_counts.crc++;\n      memory_usage_.Add(sizeof(CordRepCrc), repref.refcount);\n      repref = repref.Child(repref.rep->crc()->child);\n    }\n\n    // Process all top level linear nodes (substrings and flats).\n    repref = CountLinearReps(repref, memory_usage_);\n\n    switch (repref.tag()) {\n      case CordRepKind::BTREE:\n        AnalyzeBtree(repref);\n        break;\n      default:\n        // We should have a btree node if not null.\n        ABSL_ASSERT(repref.tag() == CordRepKind::UNUSED_0);\n        break;\n    }\n\n    // Adds values to output\n    statistics_.estimated_memory_usage += memory_usage_.total;\n    statistics_.estimated_fair_share_memory_usage +=\n        static_cast<size_t>(memory_usage_.fair_share);\n  }\n\n private:\n  // RepRef identifies a CordRep* inside the Cord tree with its cumulative\n  // refcount including itself. For example, a tree consisting of a substring\n  // with a refcount of 3 and a child flat with a refcount of 4 will have RepRef\n  // refcounts of 3 and 12 respectively.\n  struct RepRef {\n    const CordRep* rep;\n    size_t refcount;\n\n    // Returns a 'child' RepRef which contains the cumulative reference count\n    // of this instance multiplied by the child's reference count. Returns a\n    // nullptr RepRef value with a refcount of 0 if `child` is nullptr.\n    RepRef Child(const CordRep* child) const {\n      if (child == nullptr) return RepRef{nullptr, 0};\n      return RepRef{child, refcount * child->refcount.Get()};\n    }\n\n    // Returns the tag of this rep, or UNUSED_0 if this instance is null\n    constexpr CordRepKind tag() const {\n      ABSL_ASSERT(rep == nullptr || rep->tag != CordRepKind::UNUSED_0);\n      return rep ? static_cast<CordRepKind>(rep->tag) : CordRepKind::UNUSED_0;\n    }\n  };\n\n  // Memory usage values\n  struct MemoryUsage {\n    size_t total = 0;\n    double fair_share = 0.0;\n\n    // Adds 'size` memory usage to this class, with a cumulative (recursive)\n    // reference count of `refcount`\n    void Add(size_t size, size_t refcount) {\n      total += size;\n      fair_share += static_cast<double>(size) / refcount;\n    }\n  };\n\n  // Counts a flat of the provide allocated size\n  void CountFlat(size_t size) {\n    statistics_.node_count++;\n    statistics_.node_counts.flat++;\n    if (size <= 64) {\n      statistics_.node_counts.flat_64++;\n    } else if (size <= 128) {\n      statistics_.node_counts.flat_128++;\n    } else if (size <= 256) {\n      statistics_.node_counts.flat_256++;\n    } else if (size <= 512) {\n      statistics_.node_counts.flat_512++;\n    } else if (size <= 1024) {\n      statistics_.node_counts.flat_1k++;\n    }\n  }\n\n  // Processes 'linear' reps (substring, flat, external) not requiring iteration\n  // or recursion. Returns RefRep{null} if all reps were processed, else returns\n  // the top-most non-linear concat or ring cordrep.\n  // Node counts are updated into `statistics_`, memory usage is update into\n  // `memory_usage`, which typically references `memory_usage_` except for ring\n  // buffers where we count children unrounded.\n  RepRef CountLinearReps(RepRef rep, MemoryUsage& memory_usage) {\n    // Consume all substrings\n    while (rep.tag() == SUBSTRING) {\n      statistics_.node_count++;\n      statistics_.node_counts.substring++;\n      memory_usage.Add(sizeof(CordRepSubstring), rep.refcount);\n      rep = rep.Child(rep.rep->substring()->child);\n    }\n\n    // Consume possible FLAT\n    if (rep.tag() >= FLAT) {\n      size_t size = rep.rep->flat()->AllocatedSize();\n      CountFlat(size);\n      memory_usage.Add(size, rep.refcount);\n      return RepRef{nullptr, 0};\n    }\n\n    // Consume possible external\n    if (rep.tag() == EXTERNAL) {\n      statistics_.node_count++;\n      statistics_.node_counts.external++;\n      size_t size = rep.rep->length + sizeof(CordRepExternalImpl<intptr_t>);\n      memory_usage.Add(size, rep.refcount);\n      return RepRef{nullptr, 0};\n    }\n\n    return rep;\n  }\n\n  // Analyzes the provided btree.\n  void AnalyzeBtree(RepRef rep) {\n    statistics_.node_count++;\n    statistics_.node_counts.btree++;\n    memory_usage_.Add(sizeof(CordRepBtree), rep.refcount);\n    const CordRepBtree* tree = rep.rep->btree();\n    if (tree->height() > 0) {\n      for (CordRep* edge : tree->Edges()) {\n        AnalyzeBtree(rep.Child(edge));\n      }\n    } else {\n      for (CordRep* edge : tree->Edges()) {\n        CountLinearReps(rep.Child(edge), memory_usage_);\n      }\n    }\n  }\n\n  CordzStatistics& statistics_;\n  MemoryUsage memory_usage_;\n};\n\n}  // namespace\n\nCordzInfo* CordzInfo::Head(const CordzSnapshot& snapshot) {\n  ABSL_ASSERT(snapshot.is_snapshot());\n\n  // We can do an 'unsafe' load of 'head', as we are guaranteed that the\n  // instance it points to is kept alive by the provided CordzSnapshot, so we\n  // can simply return the current value using an acquire load.\n  // We do enforce in DEBUG builds that the 'head' value is present in the\n  // delete queue: ODR violations may lead to 'snapshot' and 'global_list_'\n  // being in different libraries / modules.\n  CordzInfo* head = global_list_.head.load(std::memory_order_acquire);\n  ABSL_ASSERT(snapshot.DiagnosticsHandleIsSafeToInspect(head));\n  return head;\n}\n\nCordzInfo* CordzInfo::Next(const CordzSnapshot& snapshot) const {\n  ABSL_ASSERT(snapshot.is_snapshot());\n\n  // Similar to the 'Head()' function, we do not need a mutex here.\n  CordzInfo* next = ci_next_.load(std::memory_order_acquire);\n  ABSL_ASSERT(snapshot.DiagnosticsHandleIsSafeToInspect(this));\n  ABSL_ASSERT(snapshot.DiagnosticsHandleIsSafeToInspect(next));\n  return next;\n}\n\nvoid CordzInfo::TrackCord(InlineData& cord, MethodIdentifier method) {\n  assert(cord.is_tree());\n  assert(!cord.is_profiled());\n  CordzInfo* cordz_info = new CordzInfo(cord.as_tree(), nullptr, method);\n  cord.set_cordz_info(cordz_info);\n  cordz_info->Track();\n}\n\nvoid CordzInfo::TrackCord(InlineData& cord, const InlineData& src,\n                          MethodIdentifier method) {\n  assert(cord.is_tree());\n  assert(src.is_tree());\n\n  // Unsample current as we the current cord is being replaced with 'src',\n  // so any method history is no longer relevant.\n  CordzInfo* cordz_info = cord.cordz_info();\n  if (cordz_info != nullptr) cordz_info->Untrack();\n\n  // Start new cord sample\n  cordz_info = new CordzInfo(cord.as_tree(), src.cordz_info(), method);\n  cord.set_cordz_info(cordz_info);\n  cordz_info->Track();\n}\n\nvoid CordzInfo::MaybeTrackCordImpl(InlineData& cord, const InlineData& src,\n                                   MethodIdentifier method) {\n  if (src.is_profiled()) {\n    TrackCord(cord, src, method);\n  } else if (cord.is_profiled()) {\n    cord.cordz_info()->Untrack();\n    cord.clear_cordz_info();\n  }\n}\n\nCordzInfo::MethodIdentifier CordzInfo::GetParentMethod(const CordzInfo* src) {\n  if (src == nullptr) return MethodIdentifier::kUnknown;\n  return src->parent_method_ != MethodIdentifier::kUnknown ? src->parent_method_\n                                                           : src->method_;\n}\n\nsize_t CordzInfo::FillParentStack(const CordzInfo* src, void** stack) {\n  assert(stack);\n  if (src == nullptr) return 0;\n  if (src->parent_stack_depth_) {\n    memcpy(stack, src->parent_stack_, src->parent_stack_depth_ * sizeof(void*));\n    return src->parent_stack_depth_;\n  }\n  memcpy(stack, src->stack_, src->stack_depth_ * sizeof(void*));\n  return src->stack_depth_;\n}\n\nCordzInfo::CordzInfo(CordRep* rep,\n                     const CordzInfo* src,\n                     MethodIdentifier method)\n    : rep_(rep),\n      stack_depth_(\n          static_cast<size_t>(absl::GetStackTrace(stack_,\n                                                  /*max_depth=*/kMaxStackDepth,\n                                                  /*skip_count=*/1))),\n      parent_stack_depth_(FillParentStack(src, parent_stack_)),\n      method_(method),\n      parent_method_(GetParentMethod(src)),\n      create_time_(absl::Now()) {\n  update_tracker_.LossyAdd(method);\n  if (src) {\n    // Copy parent counters.\n    update_tracker_.LossyAdd(src->update_tracker_);\n  }\n}\n\nCordzInfo::~CordzInfo() {\n  // `rep_` is potentially kept alive if CordzInfo is included\n  // in a collection snapshot (which should be rare).\n  if (ABSL_PREDICT_FALSE(rep_)) {\n    CordRep::Unref(rep_);\n  }\n}\n\nvoid CordzInfo::Track() {\n  SpinLockHolder l(&list_->mutex);\n\n  CordzInfo* const head = list_->head.load(std::memory_order_acquire);\n  if (head != nullptr) {\n    head->ci_prev_.store(this, std::memory_order_release);\n  }\n  ci_next_.store(head, std::memory_order_release);\n  list_->head.store(this, std::memory_order_release);\n}\n\nvoid CordzInfo::Untrack() {\n  ODRCheck();\n  {\n    SpinLockHolder l(&list_->mutex);\n\n    CordzInfo* const head = list_->head.load(std::memory_order_acquire);\n    CordzInfo* const next = ci_next_.load(std::memory_order_acquire);\n    CordzInfo* const prev = ci_prev_.load(std::memory_order_acquire);\n\n    if (next) {\n      ABSL_ASSERT(next->ci_prev_.load(std::memory_order_acquire) == this);\n      next->ci_prev_.store(prev, std::memory_order_release);\n    }\n    if (prev) {\n      ABSL_ASSERT(head != this);\n      ABSL_ASSERT(prev->ci_next_.load(std::memory_order_acquire) == this);\n      prev->ci_next_.store(next, std::memory_order_release);\n    } else {\n      ABSL_ASSERT(head == this);\n      list_->head.store(next, std::memory_order_release);\n    }\n  }\n\n  // We can no longer be discovered: perform a fast path check if we are not\n  // listed on any delete queue, so we can directly delete this instance.\n  if (SafeToDelete()) {\n    UnsafeSetCordRep(nullptr);\n    delete this;\n    return;\n  }\n\n  // We are likely part of a snapshot, extend the life of the CordRep\n  {\n    absl::MutexLock lock(&mutex_);\n    if (rep_) CordRep::Ref(rep_);\n  }\n  CordzHandle::Delete(this);\n}\n\nvoid CordzInfo::Lock(MethodIdentifier method)\n    ABSL_EXCLUSIVE_LOCK_FUNCTION(mutex_) {\n  mutex_.Lock();\n  update_tracker_.LossyAdd(method);\n  assert(rep_);\n}\n\nvoid CordzInfo::Unlock() ABSL_UNLOCK_FUNCTION(mutex_) {\n  bool tracked = rep_ != nullptr;\n  mutex_.Unlock();\n  if (!tracked) {\n    Untrack();\n  }\n}\n\nabsl::Span<void* const> CordzInfo::GetStack() const {\n  return absl::MakeConstSpan(stack_, stack_depth_);\n}\n\nabsl::Span<void* const> CordzInfo::GetParentStack() const {\n  return absl::MakeConstSpan(parent_stack_, parent_stack_depth_);\n}\n\nCordzStatistics CordzInfo::GetCordzStatistics() const {\n  CordzStatistics stats;\n  stats.method = method_;\n  stats.parent_method = parent_method_;\n  stats.update_tracker = update_tracker_;\n  if (CordRep* rep = RefCordRep()) {\n    stats.size = rep->length;\n    CordRepAnalyzer analyzer(stats);\n    analyzer.AnalyzeCordRep(rep);\n    CordRep::Unref(rep);\n  }\n  return stats;\n}\n\n}  // namespace cord_internal\nABSL_NAMESPACE_END\n}",
  "id": "BLOCK-CPP-02930",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/strings/internal/cordz_info.cc",
  "source_line": 31,
  "validation_status": "validated"
}