{
  "code": "{\n  ABSL_ATTRIBUTE_HOT\n  void Extend(uint32_t* crc, const void* bytes, size_t length) const override {\n    static_assert(num_crc_streams >= 1 && num_crc_streams <= kMaxStreams,\n                  \"Invalid number of crc streams\");\n    static_assert(num_pclmul_streams >= 0 && num_pclmul_streams <= kMaxStreams,\n                  \"Invalid number of pclmul streams\");\n    const uint8_t* p = static_cast<const uint8_t*>(bytes);\n    const uint8_t* e = p + length;\n    uint32_t l = *crc;\n    uint64_t l64;\n\n    // We have dedicated instruction for 1,2,4 and 8 bytes.\n    if (length & 8) {\n      ABSL_INTERNAL_STEP8(l, p);\n      length &= ~size_t{8};\n    }\n    if (length & 4) {\n      ABSL_INTERNAL_STEP4(l);\n      length &= ~size_t{4};\n    }\n    if (length & 2) {\n      ABSL_INTERNAL_STEP2(l);\n      length &= ~size_t{2};\n    }\n    if (length & 1) {\n      ABSL_INTERNAL_STEP1(l);\n      length &= ~size_t{1};\n    }\n    if (length == 0) {\n      *crc = l;\n      return;\n    }\n    // length is now multiple of 16.\n\n    // For small blocks just run simple loop, because cost of combining multiple\n    // streams is significant.\n    if (strategy != CutoffStrategy::Unroll64CRC) {\n      if (length < kSmallCutoff) {\n        while (length >= 16) {\n          ABSL_INTERNAL_STEP8(l, p);\n          ABSL_INTERNAL_STEP8(l, p);\n          length -= 16;\n        }\n        *crc = l;\n        return;\n      }\n    }\n\n    // For medium blocks we run 3 crc streams and combine them as described in\n    // Intel paper above. Running 4th stream doesn't help, because crc\n    // instruction has latency 3 and throughput 1.\n    if (length < kMediumCutoff) {\n      l64 = l;\n      if (strategy == CutoffStrategy::Fold3) {\n        uint64_t l641 = 0;\n        uint64_t l642 = 0;\n        const size_t blockSize = 32;\n        size_t bs = static_cast<size_t>(e - p) / kGroupsSmall / blockSize;\n        const uint8_t* p1 = p + bs * blockSize;\n        const uint8_t* p2 = p1 + bs * blockSize;\n\n        for (size_t i = 0; i + 1 < bs; ++i) {\n          ABSL_INTERNAL_STEP8BY3(l64, l641, l642, p, p1, p2);\n          ABSL_INTERNAL_STEP8BY3(l64, l641, l642, p, p1, p2);\n          ABSL_INTERNAL_STEP8BY3(l64, l641, l642, p, p1, p2);\n          ABSL_INTERNAL_STEP8BY3(l64, l641, l642, p, p1, p2);\n          PrefetchToLocalCache(\n              reinterpret_cast<const char*>(p + kPrefetchHorizonMedium));\n          PrefetchToLocalCache(\n              reinterpret_cast<const char*>(p1 + kPrefetchHorizonMedium));\n          PrefetchToLocalCache(\n              reinterpret_cast<const char*>(p2 + kPrefetchHorizonMedium));\n        }\n        // Don't run crc on last 8 bytes.\n        ABSL_INTERNAL_STEP8BY3(l64, l641, l642, p, p1, p2);\n        ABSL_INTERNAL_STEP8BY3(l64, l641, l642, p, p1, p2);\n        ABSL_INTERNAL_STEP8BY3(l64, l641, l642, p, p1, p2);\n        ABSL_INTERNAL_STEP8BY2(l64, l641, p, p1);\n\n        V128 magic = *(reinterpret_cast<const V128*>(kClmulConstants) + bs - 1);\n\n        V128 tmp = V128_From2x64(0, l64);\n\n        V128 res1 = V128_PMulLow(tmp, magic);\n\n        tmp = V128_From2x64(0, l641);\n\n        V128 res2 = V128_PMul10(tmp, magic);\n        V128 x = V128_Xor(res1, res2);\n        l64 = static_cast<uint64_t>(V128_Low64(x)) ^\n              absl::little_endian::Load64(p2);\n        l64 = CRC32_u64(static_cast<uint32_t>(l642), l64);\n\n        p = p2 + 8;\n      } else if (strategy == CutoffStrategy::Unroll64CRC) {\n        while ((e - p) >= 64) {\n          l64 = Process64BytesCRC(p, l64);\n          p += 64;\n        }\n      }\n    } else {\n      // There is a lot of data, we can ignore combine costs and run all\n      // requested streams (num_crc_streams + num_pclmul_streams),\n      // using prefetch. CRC and PCLMULQDQ use different cpu execution units,\n      // so on some cpus it makes sense to execute both of them for different\n      // streams.\n\n      // Point x at first 8-byte aligned byte in string.\n      const uint8_t* x = RoundUp<8>(p);\n      // Process bytes until p is 8-byte aligned, if that isn't past the end.\n      while (p != x) {\n        ABSL_INTERNAL_STEP1(l);\n      }\n\n      size_t bs = static_cast<size_t>(e - p) /\n                  (num_crc_streams + num_pclmul_streams) / 64;\n      const uint8_t* crc_streams[kMaxStreams];\n      const uint8_t* pclmul_streams[kMaxStreams];\n      // We are guaranteed to have at least one crc stream.\n      crc_streams[0] = p;\n      for (size_t i = 1; i < num_crc_streams; i++) {\n        crc_streams[i] = crc_streams[i - 1] + bs * 64;\n      }\n      pclmul_streams[0] = crc_streams[num_crc_streams - 1] + bs * 64;\n      for (size_t i = 1; i < num_pclmul_streams; i++) {\n        pclmul_streams[i] = pclmul_streams[i - 1] + bs * 64;\n      }\n\n      // Per stream crc sums.\n      uint64_t l64_crc[kMaxStreams] = {l};\n      uint64_t l64_pclmul[kMaxStreams] = {0};\n\n      // Peel first iteration, because PCLMULQDQ stream, needs setup.\n      for (size_t i = 0; i < num_crc_streams; i++) {\n        l64_crc[i] = Process64BytesCRC(crc_streams[i], l64_crc[i]);\n        crc_streams[i] += 16 * 4;\n      }\n\n      V128 partialCRC[kMaxStreams][4];\n      for (size_t i = 0; i < num_pclmul_streams; i++) {\n        partialCRC[i][0] = V128_LoadU(\n            reinterpret_cast<const V128*>(pclmul_streams[i] + 16 * 0));\n        partialCRC[i][1] = V128_LoadU(\n            reinterpret_cast<const V128*>(pclmul_streams[i] + 16 * 1));\n        partialCRC[i][2] = V128_LoadU(\n            reinterpret_cast<const V128*>(pclmul_streams[i] + 16 * 2));\n        partialCRC[i][3] = V128_LoadU(\n            reinterpret_cast<const V128*>(pclmul_streams[i] + 16 * 3));\n        pclmul_streams[i] += 16 * 4;\n      }\n\n      for (size_t i = 1; i < bs; i++) {\n        // Prefetch data for next iterations.\n        for (size_t j = 0; j < num_crc_streams; j++) {\n          PrefetchToLocalCache(\n              reinterpret_cast<const char*>(crc_streams[j] + kPrefetchHorizon));\n        }\n        for (size_t j = 0; j < num_pclmul_streams; j++) {\n          PrefetchToLocalCache(reinterpret_cast<const char*>(pclmul_streams[j] +\n                                                             kPrefetchHorizon));\n        }\n\n        // We process each stream in 64 byte blocks. This can be written as\n        // for (int i = 0; i < num_pclmul_streams; i++) {\n        //   Process64BytesPclmul(pclmul_streams[i], partialCRC[i]);\n        //   pclmul_streams[i] += 16 * 4;\n        // }\n        // for (int i = 0; i < num_crc_streams; i++) {\n        //   l64_crc[i] = Process64BytesCRC(crc_streams[i], l64_crc[i]);\n        //   crc_streams[i] += 16*4;\n        // }\n        // But unrolling and interleaving PCLMULQDQ and CRC blocks manually\n        // gives ~2% performance boost.\n        l64_crc[0] = Process64BytesCRC(crc_streams[0], l64_crc[0]);\n        crc_streams[0] += 16 * 4;\n        if (num_pclmul_streams > 0) {\n          Process64BytesPclmul(pclmul_streams[0], partialCRC[0]);\n          pclmul_streams[0] += 16 * 4;\n        }\n        if (num_crc_streams > 1) {\n          l64_crc[1] = Process64BytesCRC(crc_streams[1], l64_crc[1]);\n          crc_streams[1] += 16 * 4;\n        }\n        if (num_pclmul_streams > 1) {\n          Process64BytesPclmul(pclmul_streams[1], partialCRC[1]);\n          pclmul_streams[1] += 16 * 4;\n        }\n        if (num_crc_streams > 2) {\n          l64_crc[2] = Process64BytesCRC(crc_streams[2], l64_crc[2]);\n          crc_streams[2] += 16 * 4;\n        }\n        if (num_pclmul_streams > 2) {\n          Process64BytesPclmul(pclmul_streams[2], partialCRC[2]);\n          pclmul_streams[2] += 16 * 4;\n        }\n      }\n\n      // PCLMULQDQ based streams require special final step;\n      // CRC based don't.\n      for (size_t i = 0; i < num_pclmul_streams; i++) {\n        l64_pclmul[i] = FinalizePclmulStream(partialCRC[i]);\n      }\n\n      // Combine all streams into single result.\n      uint32_t magic = ComputeZeroConstant(bs * 64);\n      l64 = l64_crc[0];\n      for (size_t i = 1; i < num_crc_streams; i++) {\n        l64 = multiply(static_cast<uint32_t>(l64), magic);\n        l64 ^= l64_crc[i];\n      }\n      for (size_t i = 0; i < num_pclmul_streams; i++) {\n        l64 = multiply(static_cast<uint32_t>(l64), magic);\n        l64 ^= l64_pclmul[i];\n      }\n\n      // Update p.\n      if (num_pclmul_streams > 0) {\n        p = pclmul_streams[num_pclmul_streams - 1];\n      } else {\n        p = crc_streams[num_crc_streams - 1];\n      }\n    }\n    l = static_cast<uint32_t>(l64);\n\n    while ((e - p) >= 16) {\n      ABSL_INTERNAL_STEP8(l, p);\n      ABSL_INTERNAL_STEP8(l, p);\n    }\n    // Process the last few bytes\n    while (p != e) {\n      ABSL_INTERNAL_STEP1(l);\n    }\n\n#undef ABSL_INTERNAL_STEP8BY3\n#undef ABSL_INTERNAL_STEP8BY2\n#undef ABSL_INTERNAL_STEP8\n#undef ABSL_INTERNAL_STEP4\n#undef ABSL_INTERNAL_STEP2\n#undef ABSL_INTERNAL_STEP1\n\n    *crc = l;\n  }\n}",
  "id": "BLOCK-CPP-02495",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/crc/internal/crc_x86_arm_combined.cc",
  "source_line": 362,
  "validation_status": "validated"
}