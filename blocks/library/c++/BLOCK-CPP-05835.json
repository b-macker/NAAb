{
  "code": "#include <atomic>\n#include <cstddef>\n#include <functional>\n#include \"absl/base/config.h\"\n#include \"absl/base/thread_annotations.h\"\n#include \"absl/synchronization/mutex.h\"\n#include \"absl/time/time.h\"\n\nusing namespace absl;\nusing namespace profiling_internal;\n\nextern \"C\" {\n\nvoid BLOCK-CPP-05835_execute() {\n    {\n\n// Sample<T> that has members required for linking samples in the linked list of\n// samples maintained by the SampleRecorder.  Type T defines the sampled data.\ntemplate <typename T>\nstruct Sample {\n  // Guards the ability to restore the sample to a pristine state.  This\n  // prevents races with sampling and resurrecting an object.\n  absl::Mutex init_mu;\n  T* next = nullptr;\n  T* dead ABSL_GUARDED_BY(init_mu) = nullptr;\n  int64_t weight;  // How many sampling events were required to sample this one.\n};\n\n// Holds samples and their associated stack traces with a soft limit of\n// `SetHashtablezMaxSamples()`.\n//\n// Thread safe.\ntemplate <typename T>\nclass SampleRecorder {\n public:\n  SampleRecorder();\n  ~SampleRecorder();\n\n  // Registers for sampling.  Returns an opaque registration info.\n  template <typename... Targs>\n  T* Register(Targs&&... args);\n\n  // Unregisters the sample.\n  void Unregister(T* sample);\n\n  // The dispose callback will be called on all samples the moment they are\n  // being unregistered. Only affects samples that are unregistered after the\n  // callback has been set.\n  // Returns the previous callback.\n  using DisposeCallback = void (*)(const T&);\n  DisposeCallback SetDisposeCallback(DisposeCallback f);\n\n  // Iterates over all the registered `StackInfo`s.  Returning the number of\n  // samples that have been dropped.\n  int64_t Iterate(const std::function<void(const T& stack)>& f);\n\n  size_t GetMaxSamples() const;\n  void SetMaxSamples(size_t max);\n\n private:\n  void PushNew(T* sample);\n  void PushDead(T* sample);\n  template <typename... Targs>\n  T* PopDead(Targs... args);\n\n  std::atomic<size_t> dropped_samples_;\n  std::atomic<size_t> size_estimate_;\n  std::atomic<size_t> max_samples_{1 << 20};\n\n  // Intrusive lock free linked lists for tracking samples.\n  //\n  // `all_` records all samples (they are never removed from this list) and is\n  // terminated with a `nullptr`.\n  //\n  // `graveyard_.dead` is a circular linked list.  When it is empty,\n  // `graveyard_.dead == &graveyard`.  The list is circular so that\n  // every item on it (even the last) has a non-null dead pointer.  This allows\n  // `Iterate` to determine if a given sample is live or dead using only\n  // information on the sample itself.\n  //\n  // For example, nodes [A, B, C, D, E] with [A, C, E] alive and [B, D] dead\n  // looks like this (G is the Graveyard):\n  //\n  //           +---+    +---+    +---+    +---+    +---+\n  //    all -->| A |--->| B |--->| C |--->| D |--->| E |\n  //           |   |    |   |    |   |    |   |    |   |\n  //   +---+   |   | +->|   |-+  |   | +->|   |-+  |   |\n  //   | G |   +---+ |  +---+ |  +---+ |  +---+ |  +---+\n  //   |   |         |        |        |        |\n  //   |   | --------+        +--------+        |\n  //   +---+                                    |\n  //     ^                                      |\n  //     +--------------------------------------+\n  //\n  std::atomic<T*> all_;\n  T graveyard_;\n\n  std::atomic<DisposeCallback> dispose_;\n};\n\ntemplate <typename T>\ntypename SampleRecorder<T>::DisposeCallback\nSampleRecorder<T>::SetDisposeCallback(DisposeCallback f) {\n  return dispose_.exchange(f, std::memory_order_relaxed);\n}\n\ntemplate <typename T>\nSampleRecorder<T>::SampleRecorder()\n    : dropped_samples_(0), size_estimate_(0), all_(nullptr), dispose_(nullptr) {\n  absl::MutexLock l(&graveyard_.init_mu);\n  graveyard_.dead = &graveyard_;\n}\n\ntemplate <typename T>\nSampleRecorder<T>::~SampleRecorder() {\n  T* s = all_.load(std::memory_order_acquire);\n  while (s != nullptr) {\n    T* next = s->next;\n    delete s;\n    s = next;\n  }\n}\n\ntemplate <typename T>\nvoid SampleRecorder<T>::PushNew(T* sample) {\n  sample->next = all_.load(std::memory_order_relaxed);\n  while (!all_.compare_exchange_weak(sample->next, sample,\n                                     std::memory_order_release,\n                                     std::memory_order_relaxed)) {\n  }\n}\n\ntemplate <typename T>\nvoid SampleRecorder<T>::PushDead(T* sample) {\n  if (auto* dispose = dispose_.load(std::memory_order_relaxed)) {\n    dispose(*sample);\n  }\n\n  absl::MutexLock graveyard_lock(&graveyard_.init_mu);\n  absl::MutexLock sample_lock(&sample->init_mu);\n  sample->dead = graveyard_.dead;\n  graveyard_.dead = sample;\n}\n\ntemplate <typename T>\ntemplate <typename... Targs>\nT* SampleRecorder<T>::PopDead(Targs... args) {\n  absl::MutexLock graveyard_lock(&graveyard_.init_mu);\n\n  // The list is circular, so eventually it collapses down to\n  //   graveyard_.dead == &graveyard_\n  // when it is empty.\n  T* sample = graveyard_.dead;\n  if (sample == &graveyard_) return nullptr;\n\n  absl::MutexLock sample_lock(&sample->init_mu);\n  graveyard_.dead = sample->dead;\n  sample->dead = nullptr;\n  sample->PrepareForSampling(std::forward<Targs>(args)...);\n  return sample;\n}\n\ntemplate <typename T>\ntemplate <typename... Targs>\nT* SampleRecorder<T>::Register(Targs&&... args) {\n  size_t size = size_estimate_.fetch_add(1, std::memory_order_relaxed);\n  if (size > max_samples_.load(std::memory_order_relaxed)) {\n    size_estimate_.fetch_sub(1, std::memory_order_relaxed);\n    dropped_samples_.fetch_add(1, std::memory_order_relaxed);\n    return nullptr;\n  }\n\n  T* sample = PopDead(args...);\n  if (sample == nullptr) {\n    // Resurrection failed.  Hire a new warlock.\n    sample = new T();\n    {\n      absl::MutexLock sample_lock(&sample->init_mu);\n      // If flag initialization happens to occur (perhaps in another thread)\n      // while in this block, it will lock `graveyard_` which is usually always\n      // locked before any sample. This will appear as a lock inversion.\n      // However, this code is run exactly once per sample, and this sample\n      // cannot be accessed until after it is returned from this method.  This\n      // means that this lock state can never be recreated, so we can safely\n      // inform the deadlock detector to ignore it.\n      sample->init_mu.ForgetDeadlockInfo();\n      sample->PrepareForSampling(std::forward<Targs>(args)...);\n    }\n    PushNew(sample);\n  }\n\n  return sample;\n}\n\ntemplate <typename T>\nvoid SampleRecorder<T>::Unregister(T* sample) {\n  PushDead(sample);\n  size_estimate_.fetch_sub(1, std::memory_order_relaxed);\n}\n\ntemplate <typename T>\nint64_t SampleRecorder<T>::Iterate(\n    const std::function<void(const T& stack)>& f) {\n  T* s = all_.load(std::memory_order_acquire);\n  while (s != nullptr) {\n    absl::MutexLock l(&s->init_mu);\n    if (s->dead == nullptr) {\n      f(*s);\n    }\n    s = s->next;\n  }\n\n  return dropped_samples_.load(std::memory_order_relaxed);\n}\n\ntemplate <typename T>\nvoid SampleRecorder<T>::SetMaxSamples(size_t max) {\n  max_samples_.store(max, std::memory_order_release);\n}\n\ntemplate <typename T>\nsize_t SampleRecorder<T>::GetMaxSamples() const {\n  return max_samples_.load(std::memory_order_acquire);\n}\n\n}\n}\n\n} // extern \"C\"\n",
  "id": "BLOCK-CPP-05835",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/profiling/internal/sample_recorder.h",
  "source_line": 38,
  "validation_status": "validated"
}