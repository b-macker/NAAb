{
  "code": "{\nABSL_NAMESPACE_BEGIN\nnamespace base_internal {\n\nnamespace {\n\n#if defined(_WIN32)\n\n// Returns number of bits set in `bitMask`\nDWORD Win32CountSetBits(ULONG_PTR bitMask) {\n  for (DWORD bitSetCount = 0; ; ++bitSetCount) {\n    if (bitMask == 0) return bitSetCount;\n    bitMask &= bitMask - 1;\n  }\n}\n\n// Returns the number of logical CPUs using GetLogicalProcessorInformation(), or\n// 0 if the number of processors is not available or can not be computed.\n// https://docs.microsoft.com/en-us/windows/win32/api/sysinfoapi/nf-sysinfoapi-getlogicalprocessorinformation\nint Win32NumCPUs() {\n#pragma comment(lib, \"kernel32.lib\")\n  using Info = SYSTEM_LOGICAL_PROCESSOR_INFORMATION;\n\n  DWORD info_size = sizeof(Info);\n  Info* info(static_cast<Info*>(malloc(info_size)));\n  if (info == nullptr) return 0;\n\n  bool success = GetLogicalProcessorInformation(info, &info_size);\n  if (!success && GetLastError() == ERROR_INSUFFICIENT_BUFFER) {\n    free(info);\n    info = static_cast<Info*>(malloc(info_size));\n    if (info == nullptr) return 0;\n    success = GetLogicalProcessorInformation(info, &info_size);\n  }\n\n  DWORD logicalProcessorCount = 0;\n  if (success) {\n    Info* ptr = info;\n    DWORD byteOffset = 0;\n    while (byteOffset + sizeof(Info) <= info_size) {\n      switch (ptr->Relationship) {\n        case RelationProcessorCore:\n          logicalProcessorCount += Win32CountSetBits(ptr->ProcessorMask);\n          break;\n\n        case RelationNumaNode:\n        case RelationCache:\n        case RelationProcessorPackage:\n          // Ignore other entries\n          break;\n\n        default:\n          // Ignore unknown entries\n          break;\n      }\n      byteOffset += sizeof(Info);\n      ptr++;\n    }\n  }\n  free(info);\n  return static_cast<int>(logicalProcessorCount);\n}\n\n#endif\n\n}  // namespace\n\nstatic int GetNumCPUs() {\n#if defined(__myriad2__)\n  return 1;\n#elif defined(_WIN32)\n  const int hardware_concurrency = Win32NumCPUs();\n  return hardware_concurrency ? hardware_concurrency : 1;\n#elif defined(_AIX)\n  return sysconf(_SC_NPROCESSORS_ONLN);\n#else\n  // Other possibilities:\n  //  - Read /sys/devices/system/cpu/online and use cpumask_parse()\n  //  - sysconf(_SC_NPROCESSORS_ONLN)\n  return static_cast<int>(std::thread::hardware_concurrency());\n#endif\n}\n\n#if defined(_WIN32)\n\nstatic double GetNominalCPUFrequency() {\n#if WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_APP) && \\\n    !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP)\n  // UWP apps don't have access to the registry and currently don't provide an\n  // API informing about CPU nominal frequency.\n  return 1.0;\n#else\n#pragma comment(lib, \"advapi32.lib\")  // For Reg* functions.\n  HKEY key;\n  // Use the Reg* functions rather than the SH functions because shlwapi.dll\n  // pulls in gdi32.dll which makes process destruction much more costly.\n  if (RegOpenKeyExA(HKEY_LOCAL_MACHINE,\n                    \"HARDWARE\\\\DESCRIPTION\\\\System\\\\CentralProcessor\\\\0\", 0,\n                    KEY_READ, &key) == ERROR_SUCCESS) {\n    DWORD type = 0;\n    DWORD data = 0;\n    DWORD data_size = sizeof(data);\n    auto result = RegQueryValueExA(key, \"~MHz\", nullptr, &type,\n                                   reinterpret_cast<LPBYTE>(&data), &data_size);\n    RegCloseKey(key);\n    if (result == ERROR_SUCCESS && type == REG_DWORD &&\n        data_size == sizeof(data)) {\n      return data * 1e6;  // Value is MHz.\n    }\n  }\n  return 1.0;\n#endif  // WINAPI_PARTITION_APP && !WINAPI_PARTITION_DESKTOP\n}\n\n#elif defined(CTL_HW) && defined(HW_CPU_FREQ)\n\nstatic double GetNominalCPUFrequency() {\n  unsigned freq;\n  size_t size = sizeof(freq);\n  int mib[2] = {CTL_HW, HW_CPU_FREQ};\n  if (sysctl(mib, 2, &freq, &size, nullptr, 0) == 0) {\n    return static_cast<double>(freq);\n  }\n  return 1.0;\n}\n\n#else\n\n// Helper function for reading a long from a file. Returns true if successful\n// and the memory location pointed to by value is set to the value read.\nstatic bool ReadLongFromFile(const char *file, long *value) {\n  bool ret = false;\n#if defined(_POSIX_C_SOURCE)\n  const int file_mode = (O_RDONLY | O_CLOEXEC);\n#else\n  const int file_mode = O_RDONLY;\n#endif\n\n  int fd = open(file, file_mode);\n  if (fd != -1) {\n    char line[1024];\n    char *err;\n    memset(line, '\\0', sizeof(line));\n    ssize_t len;\n    do {\n      len = read(fd, line, sizeof(line) - 1);\n    } while (len < 0 && errno == EINTR);\n    if (len <= 0) {\n      ret = false;\n    } else {\n      const long temp_value = strtol(line, &err, 10);\n      if (line[0] != '\\0' && (*err == '\\n' || *err == '\\0')) {\n        *value = temp_value;\n        ret = true;\n      }\n    }\n    close(fd);\n  }\n  return ret;\n}\n\n#if defined(ABSL_INTERNAL_UNSCALED_CYCLECLOCK_FREQUENCY_IS_CPU_FREQUENCY)\n\n// Reads a monotonic time source and returns a value in\n// nanoseconds. The returned value uses an arbitrary epoch, not the\n// Unix epoch.\nstatic int64_t ReadMonotonicClockNanos() {\n  struct timespec t;\n#ifdef CLOCK_MONOTONIC_RAW\n  int rc = clock_gettime(CLOCK_MONOTONIC_RAW, &t);\n#else\n  int rc = clock_gettime(CLOCK_MONOTONIC, &t);\n#endif\n  if (rc != 0) {\n    ABSL_INTERNAL_LOG(\n        FATAL, \"clock_gettime() failed: (\" + std::to_string(errno) + \")\");\n  }\n  return int64_t{t.tv_sec} * 1000000000 + t.tv_nsec;\n}\n\nclass UnscaledCycleClockWrapperForInitializeFrequency {\n public:\n  static int64_t Now() { return base_internal::UnscaledCycleClock::Now(); }\n};\n\nstruct TimeTscPair {\n  int64_t time;  // From ReadMonotonicClockNanos().\n  int64_t tsc;   // From UnscaledCycleClock::Now().\n};\n\n// Returns a pair of values (monotonic kernel time, TSC ticks) that\n// approximately correspond to each other.  This is accomplished by\n// doing several reads and picking the reading with the lowest\n// latency.  This approach is used to minimize the probability that\n// our thread was preempted between clock reads.\nstatic TimeTscPair GetTimeTscPair() {\n  int64_t best_latency = std::numeric_limits<int64_t>::max();\n  TimeTscPair best;\n  for (int i = 0; i < 10; ++i) {\n    int64_t t0 = ReadMonotonicClockNanos();\n    int64_t tsc = UnscaledCycleClockWrapperForInitializeFrequency::Now();\n    int64_t t1 = ReadMonotonicClockNanos();\n    int64_t latency = t1 - t0;\n    if (latency < best_latency) {\n      best_latency = latency;\n      best.time = t0;\n      best.tsc = tsc;\n    }\n  }\n  return best;\n}\n\n// Measures and returns the TSC frequency by taking a pair of\n// measurements approximately `sleep_nanoseconds` apart.\nstatic double MeasureTscFrequencyWithSleep(int sleep_nanoseconds) {\n  auto t0 = GetTimeTscPair();\n  struct timespec ts;\n  ts.tv_sec = 0;\n  ts.tv_nsec = sleep_nanoseconds;\n  while (nanosleep(&ts, &ts) != 0 && errno == EINTR) {}\n  auto t1 = GetTimeTscPair();\n  double elapsed_ticks = t1.tsc - t0.tsc;\n  double elapsed_time = (t1.time - t0.time) * 1e-9;\n  return elapsed_ticks / elapsed_time;\n}\n\n// Measures and returns the TSC frequency by calling\n// MeasureTscFrequencyWithSleep(), doubling the sleep interval until the\n// frequency measurement stabilizes.\nstatic double MeasureTscFrequency() {\n  double last_measurement = -1.0;\n  int sleep_nanoseconds = 1000000;  // 1 millisecond.\n  for (int i = 0; i < 8; ++i) {\n    double measurement = MeasureTscFrequencyWithSleep(sleep_nanoseconds);\n    if (measurement * 0.99 < last_measurement &&\n        last_measurement < measurement * 1.01) {\n      // Use the current measurement if it is within 1% of the\n      // previous measurement.\n      return measurement;\n    }\n    last_measurement = measurement;\n    sleep_nanoseconds *= 2;\n  }\n  return last_measurement;\n}\n\n#endif  // ABSL_INTERNAL_UNSCALED_CYCLECLOCK_FREQUENCY_IS_CPU_FREQUENCY\n\nstatic double GetNominalCPUFrequency() {\n  long freq = 0;\n\n  // Google's production kernel has a patch to export the TSC\n  // frequency through sysfs. If the kernel is exporting the TSC\n  // frequency use that. There are issues where cpuinfo_max_freq\n  // cannot be relied on because the BIOS may be exporting an invalid\n  // p-state (on x86) or p-states may be used to put the processor in\n  // a new mode (turbo mode). Essentially, those frequencies cannot\n  // always be relied upon. The same reasons apply to /proc/cpuinfo as\n  // well.\n  if (ReadLongFromFile(\"/sys/devices/system/cpu/cpu0/tsc_freq_khz\", &freq)) {\n    return freq * 1e3;  // Value is kHz.\n  }\n\n#if defined(ABSL_INTERNAL_UNSCALED_CYCLECLOCK_FREQUENCY_IS_CPU_FREQUENCY)\n  // On these platforms, the TSC frequency is the nominal CPU\n  // frequency.  But without having the kernel export it directly\n  // though /sys/devices/system/cpu/cpu0/tsc_freq_khz, there is no\n  // other way to reliably get the TSC frequency, so we have to\n  // measure it ourselves.  Some CPUs abuse cpuinfo_max_freq by\n  // exporting \"fake\" frequencies for implementing new features. For\n  // example, Intel's turbo mode is enabled by exposing a p-state\n  // value with a higher frequency than that of the real TSC\n  // rate. Because of this, we prefer to measure the TSC rate\n  // ourselves on i386 and x86-64.\n  return MeasureTscFrequency();\n#else\n\n  // If CPU scaling is in effect, we want to use the *maximum*\n  // frequency, not whatever CPU speed some random processor happens\n  // to be using now.\n  if (ReadLongFromFile(\"/sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq\",\n                       &freq)) {\n    return freq * 1e3;  // Value is kHz.\n  }\n\n  return 1.0;\n#endif  // !ABSL_INTERNAL_UNSCALED_CYCLECLOCK_FREQUENCY_IS_CPU_FREQUENCY\n}\n\n#endif\n\nABSL_CONST_INIT static once_flag init_num_cpus_once;\nABSL_CONST_INIT static int num_cpus = 0;\n\n// NumCPUs() may be called before main() and before malloc is properly\n// initialized, therefore this must not allocate memory.\nint NumCPUs() {\n  base_internal::LowLevelCallOnce(\n      &init_num_cpus_once, []() { num_cpus = GetNumCPUs(); });\n  return num_cpus;\n}\n\n// A default frequency of 0.0 might be dangerous if it is used in division.\nABSL_CONST_INIT static once_flag init_nominal_cpu_frequency_once;\nABSL_CONST_INIT static double nominal_cpu_frequency = 1.0;\n\n// NominalCPUFrequency() may be called before main() and before malloc is\n// properly initialized, therefore this must not allocate memory.\ndouble NominalCPUFrequency() {\n  base_internal::LowLevelCallOnce(\n      &init_nominal_cpu_frequency_once,\n      []() { nominal_cpu_frequency = GetNominalCPUFrequency(); });\n  return nominal_cpu_frequency;\n}\n\n#if defined(_WIN32)\n\npid_t GetTID() {\n  return pid_t{GetCurrentThreadId()};\n}\n\n#elif defined(__linux__)\n\n#ifndef SYS_gettid\n#define SYS_gettid __NR_gettid\n#endif\n\npid_t GetTID() {\n  return static_cast<pid_t>(syscall(SYS_gettid));\n}\n\n#elif defined(__akaros__)\n\npid_t GetTID() {\n  // Akaros has a concept of \"vcore context\", which is the state the program\n  // is forced into when we need to make a user-level scheduling decision, or\n  // run a signal handler.  This is analogous to the interrupt context that a\n  // CPU might enter if it encounters some kind of exception.\n  //\n  // There is no current thread context in vcore context, but we need to give\n  // a reasonable answer if asked for a thread ID (e.g., in a signal handler).\n  // Thread 0 always exists, so if we are in vcore context, we return that.\n  //\n  // Otherwise, we know (since we are using pthreads) that the uthread struct\n  // current_uthread is pointing to is the first element of a\n  // struct pthread_tcb, so we extract and return the thread ID from that.\n  //\n  // TODO(dcross): Akaros anticipates moving the thread ID to the uthread\n  // structure at some point. We should modify this code to remove the cast\n  // when that happens.\n  if (in_vcore_context())\n    return 0;\n  return reinterpret_cast<struct pthread_tcb *>(current_uthread)->id;\n}\n\n#elif defined(__myriad2__)\n\npid_t GetTID() {\n  uint32_t tid;\n  rtems_task_ident(RTEMS_SELF, 0, &tid);\n  return tid;\n}\n\n#elif defined(__APPLE__)\n\npid_t GetTID() {\n  uint64_t tid;\n  // `nullptr` here implies this thread.  This only fails if the specified\n  // thread is invalid or the pointer-to-tid is null, so we needn't worry about\n  // it.\n  pthread_threadid_np(nullptr, &tid);\n  return static_cast<pid_t>(tid);\n}\n\n#elif defined(__FreeBSD__)\n\npid_t GetTID() { return static_cast<pid_t>(pthread_getthreadid_np()); }\n\n#elif defined(__OpenBSD__)\n\npid_t GetTID() { return getthrid(); }\n\n#elif defined(__NetBSD__)\n\npid_t GetTID() { return static_cast<pid_t>(_lwp_self()); }\n\n#elif defined(__native_client__)\n\npid_t GetTID() {\n  auto* thread = pthread_self();\n  static_assert(sizeof(pid_t) == sizeof(thread),\n                \"In NaCL int expected to be the same size as a pointer\");\n  return reinterpret_cast<pid_t>(thread);\n}\n\n#else\n\n// Fallback implementation of `GetTID` using `pthread_self`.\npid_t GetTID() {\n  // `pthread_t` need not be arithmetic per POSIX; platforms where it isn't\n  // should be handled above.\n  return static_cast<pid_t>(pthread_self());\n}\n\n#endif\n\n// GetCachedTID() caches the thread ID in thread-local storage (which is a\n// userspace construct) to avoid unnecessary system calls. Without this caching,\n// it can take roughly 98ns, while it takes roughly 1ns with this caching.\npid_t GetCachedTID() {\n#ifdef ABSL_HAVE_THREAD_LOCAL\n  static thread_local pid_t thread_id = GetTID();\n  return thread_id;\n#else\n  return GetTID();\n#endif  // ABSL_HAVE_THREAD_LOCAL\n}\n\n}  // namespace base_internal\nABSL_NAMESPACE_END\n}",
  "id": "BLOCK-CPP-02371",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/base/internal/sysinfo.cc",
  "source_line": 69,
  "validation_status": "validated"
}