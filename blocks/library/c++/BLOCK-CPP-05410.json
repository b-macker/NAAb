{
  "code": "{\n\n#if defined(ABSL_CRC_INTERNAL_HAVE_ARM_SIMD) || \\\n    defined(ABSL_CRC_INTERNAL_HAVE_X86_SIMD)\n\n#if defined(ABSL_CRC_INTERNAL_HAVE_ARM_SIMD)\nusing V128 = uint64x2_t;\n#else\n// Note: Do not use __m128i_u, it is not portable.\n// Use V128_LoadU() perform an unaligned load from __m128i*.\nusing V128 = __m128i;\n#endif\n\n// Starting with the initial value in |crc|, accumulates a CRC32 value for\n// unsigned integers of different sizes.\nuint32_t CRC32_u8(uint32_t crc, uint8_t v);\n\nuint32_t CRC32_u16(uint32_t crc, uint16_t v);\n\nuint32_t CRC32_u32(uint32_t crc, uint32_t v);\n\nuint32_t CRC32_u64(uint32_t crc, uint64_t v);\n\n// Loads 128 bits of integer data. |src| must be 16-byte aligned.\nV128 V128_Load(const V128* src);\n\n// Load 128 bits of integer data. |src| does not need to be aligned.\nV128 V128_LoadU(const V128* src);\n\n// Store 128 bits of integer data. |src| must be 16-byte aligned.\nvoid V128_Store(V128* dst, V128 data);\n\n// Polynomially multiplies the high 64 bits of |l| and |r|.\nV128 V128_PMulHi(const V128 l, const V128 r);\n\n// Polynomially multiplies the low 64 bits of |l| and |r|.\nV128 V128_PMulLow(const V128 l, const V128 r);\n\n// Polynomially multiplies the low 64 bits of |r| and high 64 bits of |l|.\nV128 V128_PMul01(const V128 l, const V128 r);\n\n// Polynomially multiplies the low 64 bits of |l| and high 64 bits of |r|.\nV128 V128_PMul10(const V128 l, const V128 r);\n\n// Produces a XOR operation of |l| and |r|.\nV128 V128_Xor(const V128 l, const V128 r);\n\n// Produces an AND operation of |l| and |r|.\nV128 V128_And(const V128 l, const V128 r);\n\n// Sets two 64 bit integers to one 128 bit vector. The order is reverse.\n// dst[63:0] := |r|\n// dst[127:64] := |l|\nV128 V128_From2x64(const uint64_t l, const uint64_t r);\n\n// Shift |l| right by |imm| bytes while shifting in zeros.\ntemplate <int imm>\nV128 V128_ShiftRight(const V128 l);\n\n// Extracts a 32-bit integer from |l|, selected with |imm|.\ntemplate <int imm>\nint V128_Extract32(const V128 l);\n\n// Extracts a 64-bit integer from |l|, selected with |imm|.\ntemplate <int imm>\nuint64_t V128_Extract64(const V128 l);\n\n// Extracts the low 64 bits from V128.\nint64_t V128_Low64(const V128 l);\n\n// Left-shifts packed 64-bit integers in l by r.\nV128 V128_ShiftLeft64(const V128 l, const V128 r);\n\n#endif\n\n#if defined(ABSL_CRC_INTERNAL_HAVE_X86_SIMD)\n\ninline uint32_t CRC32_u8(uint32_t crc, uint8_t v) {\n  return _mm_crc32_u8(crc, v);\n}\n\ninline uint32_t CRC32_u16(uint32_t crc, uint16_t v) {\n  return _mm_crc32_u16(crc, v);\n}\n\ninline uint32_t CRC32_u32(uint32_t crc, uint32_t v) {\n  return _mm_crc32_u32(crc, v);\n}\n\ninline uint32_t CRC32_u64(uint32_t crc, uint64_t v) {\n  return static_cast<uint32_t>(_mm_crc32_u64(crc, v));\n}\n\ninline V128 V128_Load(const V128* src) { return _mm_load_si128(src); }\n\ninline V128 V128_LoadU(const V128* src) { return _mm_loadu_si128(src); }\n\ninline void V128_Store(V128* dst, V128 data) { _mm_store_si128(dst, data); }\n\ninline V128 V128_PMulHi(const V128 l, const V128 r) {\n  return _mm_clmulepi64_si128(l, r, 0x11);\n}\n\ninline V128 V128_PMulLow(const V128 l, const V128 r) {\n  return _mm_clmulepi64_si128(l, r, 0x00);\n}\n\ninline V128 V128_PMul01(const V128 l, const V128 r) {\n  return _mm_clmulepi64_si128(l, r, 0x01);\n}\n\ninline V128 V128_PMul10(const V128 l, const V128 r) {\n  return _mm_clmulepi64_si128(l, r, 0x10);\n}\n\ninline V128 V128_Xor(const V128 l, const V128 r) { return _mm_xor_si128(l, r); }\n\ninline V128 V128_And(const V128 l, const V128 r) { return _mm_and_si128(l, r); }\n\ninline V128 V128_From2x64(const uint64_t l, const uint64_t r) {\n  return _mm_set_epi64x(static_cast<int64_t>(l), static_cast<int64_t>(r));\n}\n\ntemplate <int imm>\ninline V128 V128_ShiftRight(const V128 l) {\n  return _mm_srli_si128(l, imm);\n}\n\ntemplate <int imm>\ninline int V128_Extract32(const V128 l) {\n  return _mm_extract_epi32(l, imm);\n}\n\ntemplate <int imm>\ninline uint64_t V128_Extract64(const V128 l) {\n  return static_cast<uint64_t>(_mm_extract_epi64(l, imm));\n}\n\ninline int64_t V128_Low64(const V128 l) { return _mm_cvtsi128_si64(l); }\n\ninline V128 V128_ShiftLeft64(const V128 l, const V128 r) {\n  return _mm_sll_epi64(l, r);\n}\n\n#elif defined(ABSL_CRC_INTERNAL_HAVE_ARM_SIMD)\n\ninline uint32_t CRC32_u8(uint32_t crc, uint8_t v) { return __crc32cb(crc, v); }\n\ninline uint32_t CRC32_u16(uint32_t crc, uint16_t v) {\n  return __crc32ch(crc, v);\n}\n\ninline uint32_t CRC32_u32(uint32_t crc, uint32_t v) {\n  return __crc32cw(crc, v);\n}\n\ninline uint32_t CRC32_u64(uint32_t crc, uint64_t v) {\n  return __crc32cd(crc, v);\n}\n\ninline V128 V128_Load(const V128* src) {\n  return vld1q_u64(reinterpret_cast<const uint64_t*>(src));\n}\n\ninline V128 V128_LoadU(const V128* src) {\n  return vld1q_u64(reinterpret_cast<const uint64_t*>(src));\n}\n\ninline void V128_Store(V128* dst, V128 data) {\n  vst1q_u64(reinterpret_cast<uint64_t*>(dst), data);\n}\n\n// Using inline assembly as clang does not generate the pmull2 instruction and\n// performance drops by 15-20%.\n// TODO(b/193678732): Investigate why there is a slight performance hit when\n// using intrinsics instead of inline assembly.\ninline V128 V128_PMulHi(const V128 l, const V128 r) {\n  uint64x2_t res;\n  __asm__ __volatile__(\"pmull2 %0.1q, %1.2d, %2.2d \\n\\t\"\n                       : \"=w\"(res)\n                       : \"w\"(l), \"w\"(r));\n  return res;\n}\n\n// TODO(b/193678732): Investigate why the compiler decides to move the constant\n// loop multiplicands from GPR to Neon registers every loop iteration.\ninline V128 V128_PMulLow(const V128 l, const V128 r) {\n  uint64x2_t res;\n  __asm__ __volatile__(\"pmull %0.1q, %1.1d, %2.1d \\n\\t\"\n                       : \"=w\"(res)\n                       : \"w\"(l), \"w\"(r));\n  return res;\n}\n\ninline V128 V128_PMul01(const V128 l, const V128 r) {\n  return reinterpret_cast<V128>(vmull_p64(\n      reinterpret_cast<poly64_t>(vget_high_p64(vreinterpretq_p64_u64(l))),\n      reinterpret_cast<poly64_t>(vget_low_p64(vreinterpretq_p64_u64(r)))));\n}\n\ninline V128 V128_PMul10(const V128 l, const V128 r) {\n  return reinterpret_cast<V128>(vmull_p64(\n      reinterpret_cast<poly64_t>(vget_low_p64(vreinterpretq_p64_u64(l))),\n      reinterpret_cast<poly64_t>(vget_high_p64(vreinterpretq_p64_u64(r)))));\n}\n\ninline V128 V128_Xor(const V128 l, const V128 r) { return veorq_u64(l, r); }\n\ninline V128 V128_And(const V128 l, const V128 r) { return vandq_u64(l, r); }\n\ninline V128 V128_From2x64(const uint64_t l, const uint64_t r) {\n  return vcombine_u64(vcreate_u64(r), vcreate_u64(l));\n}\n\ntemplate <int imm>\ninline V128 V128_ShiftRight(const V128 l) {\n  return vreinterpretq_u64_s8(\n      vextq_s8(vreinterpretq_s8_u64(l), vdupq_n_s8(0), imm));\n}\n\ntemplate <int imm>\ninline int V128_Extract32(const V128 l) {\n  return vgetq_lane_s32(vreinterpretq_s32_u64(l), imm);\n}\n\ntemplate <int imm>\ninline uint64_t V128_Extract64(const V128 l) {\n  return vgetq_lane_u64(l, imm);\n}\n\ninline int64_t V128_Low64(const V128 l) {\n  return vgetq_lane_s64(vreinterpretq_s64_u64(l), 0);\n}\n\ninline V128 V128_ShiftLeft64(const V128 l, const V128 r) {\n  return vshlq_u64(l, vreinterpretq_s64_u64(r));\n}\n\n#endif\n\n}",
  "id": "BLOCK-CPP-05410",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/crc/internal/crc32_x86_arm_combined_simd.h",
  "source_line": 54,
  "validation_status": "validated"
}