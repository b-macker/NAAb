{
  "code": "{\n  using PolicyTraits = hash_policy_traits<Policy>;\n  using KeyArgImpl =\n      KeyArg<IsTransparent<Eq>::value && IsTransparent<Hash>::value>;\n\n public:\n  using init_type = typename PolicyTraits::init_type;\n  using key_type = typename PolicyTraits::key_type;\n  // TODO(sbenza): Hide slot_type as it is an implementation detail. Needs user\n  // code fixes!\n  using slot_type = typename PolicyTraits::slot_type;\n  using allocator_type = Alloc;\n  using size_type = size_t;\n  using difference_type = ptrdiff_t;\n  using hasher = Hash;\n  using key_equal = Eq;\n  using policy_type = Policy;\n  using value_type = typename PolicyTraits::value_type;\n  using reference = value_type&;\n  using const_reference = const value_type&;\n  using pointer = typename absl::allocator_traits<\n      allocator_type>::template rebind_traits<value_type>::pointer;\n  using const_pointer = typename absl::allocator_traits<\n      allocator_type>::template rebind_traits<value_type>::const_pointer;\n\n  // Alias used for heterogeneous lookup functions.\n  // `key_arg<K>` evaluates to `K` when the functors are transparent and to\n  // `key_type` otherwise. It permits template argument deduction on `K` for the\n  // transparent case.\n  template <class K>\n  using key_arg = typename KeyArgImpl::template type<K, key_type>;\n\n private:\n  // Give an early error when key_type is not hashable/eq.\n  auto KeyTypeCanBeHashed(const Hash& h, const key_type& k) -> decltype(h(k));\n  auto KeyTypeCanBeEq(const Eq& eq, const key_type& k) -> decltype(eq(k, k));\n\n  using AllocTraits = absl::allocator_traits<allocator_type>;\n  using SlotAlloc = typename absl::allocator_traits<\n      allocator_type>::template rebind_alloc<slot_type>;\n  // People are often sloppy with the exact type of their allocator (sometimes\n  // it has an extra const or is missing the pair, but rebinds made it work\n  // anyway).\n  using CharAlloc =\n      typename absl::allocator_traits<Alloc>::template rebind_alloc<char>;\n  using SlotAllocTraits = typename absl::allocator_traits<\n      allocator_type>::template rebind_traits<slot_type>;\n\n  static_assert(std::is_lvalue_reference<reference>::value,\n                \"Policy::element() must return a reference\");\n\n  template <typename T>\n  struct SameAsElementReference\n      : std::is_same<typename std::remove_cv<\n                         typename std::remove_reference<reference>::type>::type,\n                     typename std::remove_cv<\n                         typename std::remove_reference<T>::type>::type> {};\n\n  // An enabler for insert(T&&): T must be convertible to init_type or be the\n  // same as [cv] value_type [ref].\n  // Note: we separate SameAsElementReference into its own type to avoid using\n  // reference unless we need to. MSVC doesn't seem to like it in some\n  // cases.\n  template <class T>\n  using RequiresInsertable = typename std::enable_if<\n      absl::disjunction<std::is_convertible<T, init_type>,\n                        SameAsElementReference<T>>::value,\n      int>::type;\n\n  // RequiresNotInit is a workaround for gcc prior to 7.1.\n  // See https://godbolt.org/g/Y4xsUh.\n  template <class T>\n  using RequiresNotInit =\n      typename std::enable_if<!std::is_same<T, init_type>::value, int>::type;\n\n  template <class... Ts>\n  using IsDecomposable = IsDecomposable<void, PolicyTraits, Hash, Eq, Ts...>;\n\n public:\n  static_assert(std::is_same<pointer, value_type*>::value,\n                \"Allocators with custom pointer types are not supported\");\n  static_assert(std::is_same<const_pointer, const value_type*>::value,\n                \"Allocators with custom pointer types are not supported\");\n\n  class iterator : private HashSetIteratorGenerationInfo {\n    friend class raw_hash_set;\n\n   public:\n    using iterator_category = std::forward_iterator_tag;\n    using value_type = typename raw_hash_set::value_type;\n    using reference =\n        absl::conditional_t<PolicyTraits::constant_iterators::value,\n                            const value_type&, value_type&>;\n    using pointer = absl::remove_reference_t<reference>*;\n    using difference_type = typename raw_hash_set::difference_type;\n\n    iterator() {}\n\n    // PRECONDITION: not an end() iterator.\n    reference operator*() const {\n      AssertIsFull(ctrl_, generation(), generation_ptr(), \"operator*()\");\n      return unchecked_deref();\n    }\n\n    // PRECONDITION: not an end() iterator.\n    pointer operator->() const {\n      AssertIsFull(ctrl_, generation(), generation_ptr(), \"operator->\");\n      return &operator*();\n    }\n\n    // PRECONDITION: not an end() iterator.\n    iterator& operator++() {\n      AssertIsFull(ctrl_, generation(), generation_ptr(), \"operator++\");\n      ++ctrl_;\n      ++slot_;\n      skip_empty_or_deleted();\n      return *this;\n    }\n    // PRECONDITION: not an end() iterator.\n    iterator operator++(int) {\n      auto tmp = *this;\n      ++*this;\n      return tmp;\n    }\n\n    friend bool operator==(const iterator& a, const iterator& b) {\n      AssertIsValidForComparison(a.ctrl_, a.generation(), a.generation_ptr());\n      AssertIsValidForComparison(b.ctrl_, b.generation(), b.generation_ptr());\n      AssertSameContainer(a.ctrl_, b.ctrl_, a.slot_, b.slot_,\n                          a.generation_ptr(), b.generation_ptr());\n      return a.ctrl_ == b.ctrl_;\n    }\n    friend bool operator!=(const iterator& a, const iterator& b) {\n      return !(a == b);\n    }\n\n   private:\n    iterator(ctrl_t* ctrl, slot_type* slot,\n             const GenerationType* generation_ptr)\n        : HashSetIteratorGenerationInfo(generation_ptr),\n          ctrl_(ctrl),\n          slot_(slot) {\n      // This assumption helps the compiler know that any non-end iterator is\n      // not equal to any end iterator.\n      ABSL_ASSUME(ctrl != nullptr);\n    }\n    // For end() iterators.\n    explicit iterator(const GenerationType* generation_ptr)\n        : HashSetIteratorGenerationInfo(generation_ptr), ctrl_(nullptr) {}\n\n    // Fixes up `ctrl_` to point to a full by advancing it and `slot_` until\n    // they reach one.\n    //\n    // If a sentinel is reached, we null `ctrl_` out instead.\n    void skip_empty_or_deleted() {\n      while (IsEmptyOrDeleted(*ctrl_)) {\n        uint32_t shift =\n            GroupEmptyOrDeleted{ctrl_}.CountLeadingEmptyOrDeleted();\n        ctrl_ += shift;\n        slot_ += shift;\n      }\n      if (ABSL_PREDICT_FALSE(*ctrl_ == ctrl_t::kSentinel)) ctrl_ = nullptr;\n    }\n\n    ctrl_t* control() const { return ctrl_; }\n    slot_type* slot() const { return slot_; }\n\n    // We use EmptyGroup() for default-constructed iterators so that they can\n    // be distinguished from end iterators, which have nullptr ctrl_.\n    ctrl_t* ctrl_ = EmptyGroup();\n    // To avoid uninitialized member warnings, put slot_ in an anonymous union.\n    // The member is not initialized on singleton and end iterators.\n    union {\n      slot_type* slot_;\n    };\n\n    // An equality check which skips ABSL Hardening iterator invalidation\n    // checks.\n    // Should be used when the lifetimes of the iterators are well-enough\n    // understood to prove that they cannot be invalid.\n    bool unchecked_equals(const iterator& b) { return ctrl_ == b.control(); }\n\n    // Dereferences the iterator without ABSL Hardening iterator invalidation\n    // checks.\n    reference unchecked_deref() const { return PolicyTraits::element(slot_); }\n  };\n\n  class const_iterator {\n    friend class raw_hash_set;\n    template <class Container, typename Enabler>\n    friend struct absl::container_internal::hashtable_debug_internal::\n        HashtableDebugAccess;\n\n   public:\n    using iterator_category = typename iterator::iterator_category;\n    using value_type = typename raw_hash_set::value_type;\n    using reference = typename raw_hash_set::const_reference;\n    using pointer = typename raw_hash_set::const_pointer;\n    using difference_type = typename raw_hash_set::difference_type;\n\n    const_iterator() = default;\n    // Implicit construction from iterator.\n    const_iterator(iterator i) : inner_(std::move(i)) {}  // NOLINT\n\n    reference operator*() const { return *inner_; }\n    pointer operator->() const { return inner_.operator->(); }\n\n    const_iterator& operator++() {\n      ++inner_;\n      return *this;\n    }\n    const_iterator operator++(int) { return inner_++; }\n\n    friend bool operator==(const const_iterator& a, const const_iterator& b) {\n      return a.inner_ == b.inner_;\n    }\n    friend bool operator!=(const const_iterator& a, const const_iterator& b) {\n      return !(a == b);\n    }\n\n   private:\n    const_iterator(const ctrl_t* ctrl, const slot_type* slot,\n                   const GenerationType* gen)\n        : inner_(const_cast<ctrl_t*>(ctrl), const_cast<slot_type*>(slot), gen) {\n    }\n    ctrl_t* control() const { return inner_.control(); }\n    slot_type* slot() const { return inner_.slot(); }\n\n    iterator inner_;\n\n    bool unchecked_equals(const const_iterator& b) {\n      return inner_.unchecked_equals(b.inner_);\n    }\n  };\n\n  using node_type = node_handle<Policy, hash_policy_traits<Policy>, Alloc>;\n  using insert_return_type = InsertReturnType<iterator, node_type>;\n\n  // Note: can't use `= default` due to non-default noexcept (causes\n  // problems for some compilers). NOLINTNEXTLINE\n  raw_hash_set() noexcept(\n      std::is_nothrow_default_constructible<hasher>::value &&\n      std::is_nothrow_default_constructible<key_equal>::value &&\n      std::is_nothrow_default_constructible<allocator_type>::value) {}\n\n  ABSL_ATTRIBUTE_NOINLINE explicit raw_hash_set(\n      size_t bucket_count, const hasher& hash = hasher(),\n      const key_equal& eq = key_equal(),\n      const allocator_type& alloc = allocator_type())\n      : settings_(CommonFields{}, hash, eq, alloc) {\n    if (bucket_count) {\n      resize(NormalizeCapacity(bucket_count));\n    }\n  }\n\n  raw_hash_set(size_t bucket_count, const hasher& hash,\n               const allocator_type& alloc)\n      : raw_hash_set(bucket_count, hash, key_equal(), alloc) {}\n\n  raw_hash_set(size_t bucket_count, const allocator_type& alloc)\n      : raw_hash_set(bucket_count, hasher(), key_equal(), alloc) {}\n\n  explicit raw_hash_set(const allocator_type& alloc)\n      : raw_hash_set(0, hasher(), key_equal(), alloc) {}\n\n  template <class InputIter>\n  raw_hash_set(InputIter first, InputIter last, size_t bucket_count = 0,\n               const hasher& hash = hasher(), const key_equal& eq = key_equal(),\n               const allocator_type& alloc = allocator_type())\n      : raw_hash_set(SelectBucketCountForIterRange(first, last, bucket_count),\n                     hash, eq, alloc) {\n    insert(first, last);\n  }\n\n  template <class InputIter>\n  raw_hash_set(InputIter first, InputIter last, size_t bucket_count,\n               const hasher& hash, const allocator_type& alloc)\n      : raw_hash_set(first, last, bucket_count, hash, key_equal(), alloc) {}\n\n  template <class InputIter>\n  raw_hash_set(InputIter first, InputIter last, size_t bucket_count,\n               const allocator_type& alloc)\n      : raw_hash_set(first, last, bucket_count, hasher(), key_equal(), alloc) {}\n\n  template <class InputIter>\n  raw_hash_set(InputIter first, InputIter last, const allocator_type& alloc)\n      : raw_hash_set(first, last, 0, hasher(), key_equal(), alloc) {}\n\n  // Instead of accepting std::initializer_list<value_type> as the first\n  // argument like std::unordered_set<value_type> does, we have two overloads\n  // that accept std::initializer_list<T> and std::initializer_list<init_type>.\n  // This is advantageous for performance.\n  //\n  //   // Turns {\"abc\", \"def\"} into std::initializer_list<std::string>, then\n  //   // copies the strings into the set.\n  //   std::unordered_set<std::string> s = {\"abc\", \"def\"};\n  //\n  //   // Turns {\"abc\", \"def\"} into std::initializer_list<const char*>, then\n  //   // copies the strings into the set.\n  //   absl::flat_hash_set<std::string> s = {\"abc\", \"def\"};\n  //\n  // The same trick is used in insert().\n  //\n  // The enabler is necessary to prevent this constructor from triggering where\n  // the copy constructor is meant to be called.\n  //\n  //   absl::flat_hash_set<int> a, b{a};\n  //\n  // RequiresNotInit<T> is a workaround for gcc prior to 7.1.\n  template <class T, RequiresNotInit<T> = 0, RequiresInsertable<T> = 0>\n  raw_hash_set(std::initializer_list<T> init, size_t bucket_count = 0,\n               const hasher& hash = hasher(), const key_equal& eq = key_equal(),\n               const allocator_type& alloc = allocator_type())\n      : raw_hash_set(init.begin(), init.end(), bucket_count, hash, eq, alloc) {}\n\n  raw_hash_set(std::initializer_list<init_type> init, size_t bucket_count = 0,\n               const hasher& hash = hasher(), const key_equal& eq = key_equal(),\n               const allocator_type& alloc = allocator_type())\n      : raw_hash_set(init.begin(), init.end(), bucket_count, hash, eq, alloc) {}\n\n  template <class T, RequiresNotInit<T> = 0, RequiresInsertable<T> = 0>\n  raw_hash_set(std::initializer_list<T> init, size_t bucket_count,\n               const hasher& hash, const allocator_type& alloc)\n      : raw_hash_set(init, bucket_count, hash, key_equal(), alloc) {}\n\n  raw_hash_set(std::initializer_list<init_type> init, size_t bucket_count,\n               const hasher& hash, const allocator_type& alloc)\n      : raw_hash_set(init, bucket_count, hash, key_equal(), alloc) {}\n\n  template <class T, RequiresNotInit<T> = 0, RequiresInsertable<T> = 0>\n  raw_hash_set(std::initializer_list<T> init, size_t bucket_count,\n               const allocator_type& alloc)\n      : raw_hash_set(init, bucket_count, hasher(), key_equal(), alloc) {}\n\n  raw_hash_set(std::initializer_list<init_type> init, size_t bucket_count,\n               const allocator_type& alloc)\n      : raw_hash_set(init, bucket_count, hasher(), key_equal(), alloc) {}\n\n  template <class T, RequiresNotInit<T> = 0, RequiresInsertable<T> = 0>\n  raw_hash_set(std::initializer_list<T> init, const allocator_type& alloc)\n      : raw_hash_set(init, 0, hasher(), key_equal(), alloc) {}\n\n  raw_hash_set(std::initializer_list<init_type> init,\n               const allocator_type& alloc)\n      : raw_hash_set(init, 0, hasher(), key_equal(), alloc) {}\n\n  raw_hash_set(const raw_hash_set& that)\n      : raw_hash_set(that, AllocTraits::select_on_container_copy_construction(\n                               that.alloc_ref())) {}\n\n  raw_hash_set(const raw_hash_set& that, const allocator_type& a)\n      : raw_hash_set(0, that.hash_ref(), that.eq_ref(), a) {\n    const size_t size = that.size();\n    if (size == 0) return;\n    reserve(size);\n    // Because the table is guaranteed to be empty, we can do something faster\n    // than a full `insert`.\n    for (const auto& v : that) {\n      const size_t hash = PolicyTraits::apply(HashElement{hash_ref()}, v);\n      auto target = find_first_non_full_outofline(common(), hash);\n      SetCtrl(common(), target.offset, H2(hash), sizeof(slot_type));\n      emplace_at(target.offset, v);\n      common().maybe_increment_generation_on_insert();\n      infoz().RecordInsert(hash, target.probe_length);\n    }\n    common().set_size(size);\n    set_growth_left(growth_left() - size);\n  }\n\n  ABSL_ATTRIBUTE_NOINLINE raw_hash_set(raw_hash_set&& that) noexcept(\n      std::is_nothrow_copy_constructible<hasher>::value &&\n      std::is_nothrow_copy_constructible<key_equal>::value &&\n      std::is_nothrow_copy_constructible<allocator_type>::value)\n      :  // Hash, equality and allocator are copied instead of moved because\n         // `that` must be left valid. If Hash is std::function<Key>, moving it\n         // would create a nullptr functor that cannot be called.\n         // TODO(b/296061262): move instead of copying hash/eq/alloc.\n         // Note: we avoid using exchange for better generated code.\n        settings_(std::move(that.common()), that.hash_ref(), that.eq_ref(),\n                  that.alloc_ref()) {\n    that.common() = CommonFields{};\n    maybe_increment_generation_or_rehash_on_move();\n  }\n\n  raw_hash_set(raw_hash_set&& that, const allocator_type& a)\n      : settings_(CommonFields{}, that.hash_ref(), that.eq_ref(), a) {\n    if (a == that.alloc_ref()) {\n      std::swap(common(), that.common());\n      maybe_increment_generation_or_rehash_on_move();\n    } else {\n      move_elements_allocs_unequal(std::move(that));\n    }\n  }\n\n  raw_hash_set& operator=(const raw_hash_set& that) {\n    if (ABSL_PREDICT_FALSE(this == &that)) return *this;\n    constexpr bool propagate_alloc =\n        AllocTraits::propagate_on_container_copy_assignment::value;\n    // TODO(ezb): maybe avoid allocating a new backing array if this->capacity()\n    // is an exact match for that.size(). If this->capacity() is too big, then\n    // it would make iteration very slow to reuse the allocation. Maybe we can\n    // do the same heuristic as clear() and reuse if it's small enough.\n    raw_hash_set tmp(that, propagate_alloc ? that.alloc_ref() : alloc_ref());\n    // NOLINTNEXTLINE: not returning *this for performance.\n    return assign_impl<propagate_alloc>(std::move(tmp));\n  }\n\n  raw_hash_set& operator=(raw_hash_set&& that) noexcept(\n      absl::allocator_traits<allocator_type>::is_always_equal::value &&\n      std::is_nothrow_move_assignable<hasher>::value &&\n      std::is_nothrow_move_assignable<key_equal>::value) {\n    // TODO(sbenza): We should only use the operations from the noexcept clause\n    // to make sure we actually adhere to that contract.\n    // NOLINTNEXTLINE: not returning *this for performance.\n    return move_assign(\n        std::move(that),\n        typename AllocTraits::propagate_on_container_move_assignment());\n  }\n\n  ~raw_hash_set() { destructor_impl(); }\n\n  iterator begin() ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    auto it = iterator_at(0);\n    it.skip_empty_or_deleted();\n    return it;\n  }\n  iterator end() ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return iterator(common().generation_ptr());\n  }\n\n  const_iterator begin() const ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return const_cast<raw_hash_set*>(this)->begin();\n  }\n  const_iterator end() const ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return iterator(common().generation_ptr());\n  }\n  const_iterator cbegin() const ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return begin();\n  }\n  const_iterator cend() const ABSL_ATTRIBUTE_LIFETIME_BOUND { return end(); }\n\n  bool empty() const { return !size(); }\n  size_t size() const { return common().size(); }\n  size_t capacity() const { return common().capacity(); }\n  size_t max_size() const { return (std::numeric_limits<size_t>::max)(); }\n\n  ABSL_ATTRIBUTE_REINITIALIZES void clear() {\n    // Iterating over this container is O(bucket_count()). When bucket_count()\n    // is much greater than size(), iteration becomes prohibitively expensive.\n    // For clear() it is more important to reuse the allocated array when the\n    // container is small because allocation takes comparatively long time\n    // compared to destruction of the elements of the container. So we pick the\n    // largest bucket_count() threshold for which iteration is still fast and\n    // past that we simply deallocate the array.\n    const size_t cap = capacity();\n    if (cap == 0) {\n      // Already guaranteed to be empty; so nothing to do.\n    } else {\n      destroy_slots();\n      ClearBackingArray(common(), GetPolicyFunctions(), /*reuse=*/cap < 128);\n    }\n    common().set_reserved_growth(0);\n    common().set_reservation_size(0);\n  }\n\n  // This overload kicks in when the argument is an rvalue of insertable and\n  // decomposable type other than init_type.\n  //\n  //   flat_hash_map<std::string, int> m;\n  //   m.insert(std::make_pair(\"abc\", 42));\n  // TODO(cheshire): A type alias T2 is introduced as a workaround for the nvcc\n  // bug.\n  template <class T, RequiresInsertable<T> = 0, class T2 = T,\n            typename std::enable_if<IsDecomposable<T2>::value, int>::type = 0,\n            T* = nullptr>\n  std::pair<iterator, bool> insert(T&& value) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return emplace(std::forward<T>(value));\n  }\n\n  // This overload kicks in when the argument is a bitfield or an lvalue of\n  // insertable and decomposable type.\n  //\n  //   union { int n : 1; };\n  //   flat_hash_set<int> s;\n  //   s.insert(n);\n  //\n  //   flat_hash_set<std::string> s;\n  //   const char* p = \"hello\";\n  //   s.insert(p);\n  //\n  template <\n      class T, RequiresInsertable<const T&> = 0,\n      typename std::enable_if<IsDecomposable<const T&>::value, int>::type = 0>\n  std::pair<iterator, bool> insert(const T& value)\n      ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return emplace(value);\n  }\n\n  // This overload kicks in when the argument is an rvalue of init_type. Its\n  // purpose is to handle brace-init-list arguments.\n  //\n  //   flat_hash_map<std::string, int> s;\n  //   s.insert({\"abc\", 42});\n  std::pair<iterator, bool> insert(init_type&& value)\n      ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return emplace(std::move(value));\n  }\n\n  // TODO(cheshire): A type alias T2 is introduced as a workaround for the nvcc\n  // bug.\n  template <class T, RequiresInsertable<T> = 0, class T2 = T,\n            typename std::enable_if<IsDecomposable<T2>::value, int>::type = 0,\n            T* = nullptr>\n  iterator insert(const_iterator, T&& value) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return insert(std::forward<T>(value)).first;\n  }\n\n  template <\n      class T, RequiresInsertable<const T&> = 0,\n      typename std::enable_if<IsDecomposable<const T&>::value, int>::type = 0>\n  iterator insert(const_iterator,\n                  const T& value) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return insert(value).first;\n  }\n\n  iterator insert(const_iterator,\n                  init_type&& value) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return insert(std::move(value)).first;\n  }\n\n  template <class InputIt>\n  void insert(InputIt first, InputIt last) {\n    for (; first != last; ++first) emplace(*first);\n  }\n\n  template <class T, RequiresNotInit<T> = 0, RequiresInsertable<const T&> = 0>\n  void insert(std::initializer_list<T> ilist) {\n    insert(ilist.begin(), ilist.end());\n  }\n\n  void insert(std::initializer_list<init_type> ilist) {\n    insert(ilist.begin(), ilist.end());\n  }\n\n  insert_return_type insert(node_type&& node) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    if (!node) return {end(), false, node_type()};\n    const auto& elem = PolicyTraits::element(CommonAccess::GetSlot(node));\n    auto res = PolicyTraits::apply(\n        InsertSlot<false>{*this, std::move(*CommonAccess::GetSlot(node))},\n        elem);\n    if (res.second) {\n      CommonAccess::Reset(&node);\n      return {res.first, true, node_type()};\n    } else {\n      return {res.first, false, std::move(node)};\n    }\n  }\n\n  iterator insert(const_iterator,\n                  node_type&& node) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    auto res = insert(std::move(node));\n    node = std::move(res.node);\n    return res.position;\n  }\n\n  // This overload kicks in if we can deduce the key from args. This enables us\n  // to avoid constructing value_type if an entry with the same key already\n  // exists.\n  //\n  // For example:\n  //\n  //   flat_hash_map<std::string, std::string> m = {{\"abc\", \"def\"}};\n  //   // Creates no std::string copies and makes no heap allocations.\n  //   m.emplace(\"abc\", \"xyz\");\n  template <class... Args, typename std::enable_if<\n                               IsDecomposable<Args...>::value, int>::type = 0>\n  std::pair<iterator, bool> emplace(Args&&... args)\n      ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return PolicyTraits::apply(EmplaceDecomposable{*this},\n                               std::forward<Args>(args)...);\n  }\n\n  // This overload kicks in if we cannot deduce the key from args. It constructs\n  // value_type unconditionally and then either moves it into the table or\n  // destroys.\n  template <class... Args, typename std::enable_if<\n                               !IsDecomposable<Args...>::value, int>::type = 0>\n  std::pair<iterator, bool> emplace(Args&&... args)\n      ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    alignas(slot_type) unsigned char raw[sizeof(slot_type)];\n    slot_type* slot = reinterpret_cast<slot_type*>(&raw);\n\n    construct(slot, std::forward<Args>(args)...);\n    const auto& elem = PolicyTraits::element(slot);\n    return PolicyTraits::apply(InsertSlot<true>{*this, std::move(*slot)}, elem);\n  }\n\n  template <class... Args>\n  iterator emplace_hint(const_iterator,\n                        Args&&... args) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return emplace(std::forward<Args>(args)...).first;\n  }\n\n  // Extension API: support for lazy emplace.\n  //\n  // Looks up key in the table. If found, returns the iterator to the element.\n  // Otherwise calls `f` with one argument of type `raw_hash_set::constructor`,\n  // and returns an iterator to the new element.\n  //\n  // `f` must abide by several restrictions:\n  //  - it MUST call `raw_hash_set::constructor` with arguments as if a\n  //    `raw_hash_set::value_type` is constructed,\n  //  - it MUST NOT access the container before the call to\n  //    `raw_hash_set::constructor`, and\n  //  - it MUST NOT erase the lazily emplaced element.\n  // Doing any of these is undefined behavior.\n  //\n  // For example:\n  //\n  //   std::unordered_set<ArenaString> s;\n  //   // Makes ArenaStr even if \"abc\" is in the map.\n  //   s.insert(ArenaString(&arena, \"abc\"));\n  //\n  //   flat_hash_set<ArenaStr> s;\n  //   // Makes ArenaStr only if \"abc\" is not in the map.\n  //   s.lazy_emplace(\"abc\", [&](const constructor& ctor) {\n  //     ctor(&arena, \"abc\");\n  //   });\n  //\n  // WARNING: This API is currently experimental. If there is a way to implement\n  // the same thing with the rest of the API, prefer that.\n  class constructor {\n    friend class raw_hash_set;\n\n   public:\n    template <class... Args>\n    void operator()(Args&&... args) const {\n      assert(*slot_);\n      PolicyTraits::construct(alloc_, *slot_, std::forward<Args>(args)...);\n      *slot_ = nullptr;\n    }\n\n   private:\n    constructor(allocator_type* a, slot_type** slot) : alloc_(a), slot_(slot) {}\n\n    allocator_type* alloc_;\n    slot_type** slot_;\n  };\n\n  template <class K = key_type, class F>\n  iterator lazy_emplace(const key_arg<K>& key,\n                        F&& f) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    auto res = find_or_prepare_insert(key);\n    if (res.second) {\n      slot_type* slot = slot_array() + res.first;\n      std::forward<F>(f)(constructor(&alloc_ref(), &slot));\n      assert(!slot);\n    }\n    return iterator_at(res.first);\n  }\n\n  // Extension API: support for heterogeneous keys.\n  //\n  //   std::unordered_set<std::string> s;\n  //   // Turns \"abc\" into std::string.\n  //   s.erase(\"abc\");\n  //\n  //   flat_hash_set<std::string> s;\n  //   // Uses \"abc\" directly without copying it into std::string.\n  //   s.erase(\"abc\");\n  template <class K = key_type>\n  size_type erase(const key_arg<K>& key) {\n    auto it = find(key);\n    if (it == end()) return 0;\n    erase(it);\n    return 1;\n  }\n\n  // Erases the element pointed to by `it`.  Unlike `std::unordered_set::erase`,\n  // this method returns void to reduce algorithmic complexity to O(1).  The\n  // iterator is invalidated, so any increment should be done before calling\n  // erase.  In order to erase while iterating across a map, use the following\n  // idiom (which also works for standard containers):\n  //\n  // for (auto it = m.begin(), end = m.end(); it != end;) {\n  //   // `erase()` will invalidate `it`, so advance `it` first.\n  //   auto copy_it = it++;\n  //   if (<pred>) {\n  //     m.erase(copy_it);\n  //   }\n  // }\n  void erase(const_iterator cit) { erase(cit.inner_); }\n\n  // This overload is necessary because otherwise erase<K>(const K&) would be\n  // a better match if non-const iterator is passed as an argument.\n  void erase(iterator it) {\n    AssertIsFull(it.control(), it.generation(), it.generation_ptr(), \"erase()\");\n    destroy(it.slot());\n    erase_meta_only(it);\n  }\n\n  iterator erase(const_iterator first,\n                 const_iterator last) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    // We check for empty first because ClearBackingArray requires that\n    // capacity() > 0 as a precondition.\n    if (empty()) return end();\n    if (first == begin() && last == end()) {\n      // TODO(ezb): we access control bytes in destroy_slots so it could make\n      // sense to combine destroy_slots and ClearBackingArray to avoid cache\n      // misses when the table is large. Note that we also do this in clear().\n      destroy_slots();\n      ClearBackingArray(common(), GetPolicyFunctions(), /*reuse=*/true);\n      common().set_reserved_growth(common().reservation_size());\n      return end();\n    }\n    while (first != last) {\n      erase(first++);\n    }\n    return last.inner_;\n  }\n\n  // Moves elements from `src` into `this`.\n  // If the element already exists in `this`, it is left unmodified in `src`.\n  template <typename H, typename E>\n  void merge(raw_hash_set<Policy, H, E, Alloc>& src) {  // NOLINT\n    assert(this != &src);\n    for (auto it = src.begin(), e = src.end(); it != e;) {\n      auto next = std::next(it);\n      if (PolicyTraits::apply(InsertSlot<false>{*this, std::move(*it.slot())},\n                              PolicyTraits::element(it.slot()))\n              .second) {\n        src.erase_meta_only(it);\n      }\n      it = next;\n    }\n  }\n\n  template <typename H, typename E>\n  void merge(raw_hash_set<Policy, H, E, Alloc>&& src) {\n    merge(src);\n  }\n\n  node_type extract(const_iterator position) {\n    AssertIsFull(position.control(), position.inner_.generation(),\n                 position.inner_.generation_ptr(), \"extract()\");\n    auto node = CommonAccess::Transfer<node_type>(alloc_ref(), position.slot());\n    erase_meta_only(position);\n    return node;\n  }\n\n  template <\n      class K = key_type,\n      typename std::enable_if<!std::is_same<K, iterator>::value, int>::type = 0>\n  node_type extract(const key_arg<K>& key) {\n    auto it = find(key);\n    return it == end() ? node_type() : extract(const_iterator{it});\n  }\n\n  void swap(raw_hash_set& that) noexcept(\n      IsNoThrowSwappable<hasher>() && IsNoThrowSwappable<key_equal>() &&\n      IsNoThrowSwappable<allocator_type>(\n          typename AllocTraits::propagate_on_container_swap{})) {\n    using std::swap;\n    swap(common(), that.common());\n    swap(hash_ref(), that.hash_ref());\n    swap(eq_ref(), that.eq_ref());\n    SwapAlloc(alloc_ref(), that.alloc_ref(),\n              typename AllocTraits::propagate_on_container_swap{});\n  }\n\n  void rehash(size_t n) {\n    if (n == 0 && capacity() == 0) return;\n    if (n == 0 && size() == 0) {\n      ClearBackingArray(common(), GetPolicyFunctions(), /*reuse=*/false);\n      return;\n    }\n\n    // bitor is a faster way of doing `max` here. We will round up to the next\n    // power-of-2-minus-1, so bitor is good enough.\n    auto m = NormalizeCapacity(n | GrowthToLowerboundCapacity(size()));\n    // n == 0 unconditionally rehashes as per the standard.\n    if (n == 0 || m > capacity()) {\n      resize(m);\n\n      // This is after resize, to ensure that we have completed the allocation\n      // and have potentially sampled the hashtable.\n      infoz().RecordReservation(n);\n    }\n  }\n\n  void reserve(size_t n) {\n    if (n > size() + growth_left()) {\n      size_t m = GrowthToLowerboundCapacity(n);\n      resize(NormalizeCapacity(m));\n\n      // This is after resize, to ensure that we have completed the allocation\n      // and have potentially sampled the hashtable.\n      infoz().RecordReservation(n);\n    }\n    common().reset_reserved_growth(n);\n    common().set_reservation_size(n);\n  }\n\n  // Extension API: support for heterogeneous keys.\n  //\n  //   std::unordered_set<std::string> s;\n  //   // Turns \"abc\" into std::string.\n  //   s.count(\"abc\");\n  //\n  //   ch_set<std::string> s;\n  //   // Uses \"abc\" directly without copying it into std::string.\n  //   s.count(\"abc\");\n  template <class K = key_type>\n  size_t count(const key_arg<K>& key) const {\n    return find(key) == end() ? 0 : 1;\n  }\n\n  // Issues CPU prefetch instructions for the memory needed to find or insert\n  // a key.  Like all lookup functions, this support heterogeneous keys.\n  //\n  // NOTE: This is a very low level operation and should not be used without\n  // specific benchmarks indicating its importance.\n  template <class K = key_type>\n  void prefetch(const key_arg<K>& key) const {\n    (void)key;\n    // Avoid probing if we won't be able to prefetch the addresses received.\n#ifdef ABSL_HAVE_PREFETCH\n    prefetch_heap_block();\n    auto seq = probe(common(), hash_ref()(key));\n    PrefetchToLocalCache(control() + seq.offset());\n    PrefetchToLocalCache(slot_array() + seq.offset());\n#endif  // ABSL_HAVE_PREFETCH\n  }\n\n  // The API of find() has two extensions.\n  //\n  // 1. The hash can be passed by the user. It must be equal to the hash of the\n  // key.\n  //\n  // 2. The type of the key argument doesn't have to be key_type. This is so\n  // called heterogeneous key support.\n  template <class K = key_type>\n  iterator find(const key_arg<K>& key,\n                size_t hash) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    auto seq = probe(common(), hash);\n    slot_type* slot_ptr = slot_array();\n    const ctrl_t* ctrl = control();\n    while (true) {\n      Group g{ctrl + seq.offset()};\n      for (uint32_t i : g.Match(H2(hash))) {\n        if (ABSL_PREDICT_TRUE(PolicyTraits::apply(\n                EqualElement<K>{key, eq_ref()},\n                PolicyTraits::element(slot_ptr + seq.offset(i)))))\n          return iterator_at(seq.offset(i));\n      }\n      if (ABSL_PREDICT_TRUE(g.MaskEmpty())) return end();\n      seq.next();\n      assert(seq.index() <= capacity() && \"full table!\");\n    }\n  }\n  template <class K = key_type>\n  iterator find(const key_arg<K>& key) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    prefetch_heap_block();\n    return find(key, hash_ref()(key));\n  }\n\n  template <class K = key_type>\n  const_iterator find(const key_arg<K>& key,\n                      size_t hash) const ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return const_cast<raw_hash_set*>(this)->find(key, hash);\n  }\n  template <class K = key_type>\n  const_iterator find(const key_arg<K>& key) const\n      ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    prefetch_heap_block();\n    return find(key, hash_ref()(key));\n  }\n\n  template <class K = key_type>\n  bool contains(const key_arg<K>& key) const {\n    // Here neither the iterator returned by `find()` nor `end()` can be invalid\n    // outside of potential thread-safety issues.\n    // `find()`'s return value is constructed, used, and then destructed\n    // all in this context.\n    return !find(key).unchecked_equals(end());\n  }\n\n  template <class K = key_type>\n  std::pair<iterator, iterator> equal_range(const key_arg<K>& key)\n      ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    auto it = find(key);\n    if (it != end()) return {it, std::next(it)};\n    return {it, it};\n  }\n  template <class K = key_type>\n  std::pair<const_iterator, const_iterator> equal_range(\n      const key_arg<K>& key) const ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    auto it = find(key);\n    if (it != end()) return {it, std::next(it)};\n    return {it, it};\n  }\n\n  size_t bucket_count() const { return capacity(); }\n  float load_factor() const {\n    return capacity() ? static_cast<double>(size()) / capacity() : 0.0;\n  }\n  float max_load_factor() const { return 1.0f; }\n  void max_load_factor(float) {\n    // Does nothing.\n  }\n\n  hasher hash_function() const { return hash_ref(); }\n  key_equal key_eq() const { return eq_ref(); }\n  allocator_type get_allocator() const { return alloc_ref(); }\n\n  friend bool operator==(const raw_hash_set& a, const raw_hash_set& b) {\n    if (a.size() != b.size()) return false;\n    const raw_hash_set* outer = &a;\n    const raw_hash_set* inner = &b;\n    if (outer->capacity() > inner->capacity()) std::swap(outer, inner);\n    for (const value_type& elem : *outer) {\n      auto it = PolicyTraits::apply(FindElement{*inner}, elem);\n      if (it == inner->end() || !(*it == elem)) return false;\n    }\n    return true;\n  }\n\n  friend bool operator!=(const raw_hash_set& a, const raw_hash_set& b) {\n    return !(a == b);\n  }\n\n  template <typename H>\n  friend typename std::enable_if<H::template is_hashable<value_type>::value,\n                                 H>::type\n  AbslHashValue(H h, const raw_hash_set& s) {\n    return H::combine(H::combine_unordered(std::move(h), s.begin(), s.end()),\n                      s.size());\n  }\n\n  friend void swap(raw_hash_set& a,\n                   raw_hash_set& b) noexcept(noexcept(a.swap(b))) {\n    a.swap(b);\n  }\n\n private:\n  template <class Container, typename Enabler>\n  friend struct absl::container_internal::hashtable_debug_internal::\n      HashtableDebugAccess;\n\n  struct FindElement {\n    template <class K, class... Args>\n    const_iterator operator()(const K& key, Args&&...) const {\n      return s.find(key);\n    }\n    const raw_hash_set& s;\n  };\n\n  struct HashElement {\n    template <class K, class... Args>\n    size_t operator()(const K& key, Args&&...) const {\n      return h(key);\n    }\n    const hasher& h;\n  };\n\n  template <class K1>\n  struct EqualElement {\n    template <class K2, class... Args>\n    bool operator()(const K2& lhs, Args&&...) const {\n      return eq(lhs, rhs);\n    }\n    const K1& rhs;\n    const key_equal& eq;\n  };\n\n  struct EmplaceDecomposable {\n    template <class K, class... Args>\n    std::pair<iterator, bool> operator()(const K& key, Args&&... args) const {\n      auto res = s.find_or_prepare_insert(key);\n      if (res.second) {\n        s.emplace_at(res.first, std::forward<Args>(args)...);\n      }\n      return {s.iterator_at(res.first), res.second};\n    }\n    raw_hash_set& s;\n  };\n\n  template <bool do_destroy>\n  struct InsertSlot {\n    template <class K, class... Args>\n    std::pair<iterator, bool> operator()(const K& key, Args&&...) && {\n      auto res = s.find_or_prepare_insert(key);\n      if (res.second) {\n        s.transfer(s.slot_array() + res.first, &slot);\n      } else if (do_destroy) {\n        s.destroy(&slot);\n      }\n      return {s.iterator_at(res.first), res.second};\n    }\n    raw_hash_set& s;\n    // Constructed slot. Either moved into place or destroyed.\n    slot_type&& slot;\n  };\n\n  // TODO(b/303305702): re-enable reentrant validation.\n  template <typename... Args>\n  inline void construct(slot_type* slot, Args&&... args) {\n    PolicyTraits::construct(&alloc_ref(), slot, std::forward<Args>(args)...);\n  }\n  inline void destroy(slot_type* slot) {\n    PolicyTraits::destroy(&alloc_ref(), slot);\n  }\n  inline void transfer(slot_type* to, slot_type* from) {\n    PolicyTraits::transfer(&alloc_ref(), to, from);\n  }\n\n  inline void destroy_slots() {\n    const size_t cap = capacity();\n    const ctrl_t* ctrl = control();\n    slot_type* slot = slot_array();\n    for (size_t i = 0; i != cap; ++i) {\n      if (IsFull(ctrl[i])) {\n        destroy(slot + i);\n      }\n    }\n  }\n\n  inline void dealloc() {\n    assert(capacity() != 0);\n    // Unpoison before returning the memory to the allocator.\n    SanitizerUnpoisonMemoryRegion(slot_array(), sizeof(slot_type) * capacity());\n    infoz().Unregister();\n    Deallocate<BackingArrayAlignment(alignof(slot_type))>(\n        &alloc_ref(), common().backing_array_start(),\n        common().alloc_size(sizeof(slot_type), alignof(slot_type)));\n  }\n\n  inline void destructor_impl() {\n    if (capacity() == 0) return;\n    destroy_slots();\n    dealloc();\n  }\n\n  // Erases, but does not destroy, the value pointed to by `it`.\n  //\n  // This merely updates the pertinent control byte. This can be used in\n  // conjunction with Policy::transfer to move the object to another place.\n  void erase_meta_only(const_iterator it) {\n    EraseMetaOnly(common(), static_cast<size_t>(it.control() - control()),\n                  sizeof(slot_type));\n  }\n\n  // Resizes table to the new capacity and move all elements to the new\n  // positions accordingly.\n  //\n  // Note that for better performance instead of\n  // find_first_non_full(common(), hash),\n  // HashSetResizeHelper::FindFirstNonFullAfterResize(\n  //    common(), old_capacity, hash)\n  // can be called right after `resize`.\n  ABSL_ATTRIBUTE_NOINLINE void resize(size_t new_capacity) {\n    assert(IsValidCapacity(new_capacity));\n    HashSetResizeHelper resize_helper(common());\n    auto* old_slots = slot_array();\n    common().set_capacity(new_capacity);\n    // Note that `InitializeSlots` does different number initialization steps\n    // depending on the values of `transfer_uses_memcpy` and capacities.\n    // Refer to the comment in `InitializeSlots` for more details.\n    const bool grow_single_group =\n        resize_helper.InitializeSlots<CharAlloc, sizeof(slot_type),\n                                      PolicyTraits::transfer_uses_memcpy(),\n                                      alignof(slot_type)>(\n            common(), const_cast<std::remove_const_t<slot_type>*>(old_slots),\n            CharAlloc(alloc_ref()));\n\n    if (resize_helper.old_capacity() == 0) {\n      // InitializeSlots did all the work including infoz().RecordRehash().\n      return;\n    }\n\n    if (grow_single_group) {\n      if (PolicyTraits::transfer_uses_memcpy()) {\n        // InitializeSlots did all the work.\n        return;\n      }\n      // We want GrowSizeIntoSingleGroup to be called here in order to make\n      // InitializeSlots not depend on PolicyTraits.\n      resize_helper.GrowSizeIntoSingleGroup<PolicyTraits>(common(), alloc_ref(),\n                                                          old_slots);\n    } else {\n      // InitializeSlots prepares control bytes to correspond to empty table.\n      auto* new_slots = slot_array();\n      size_t total_probe_length = 0;\n      for (size_t i = 0; i != resize_helper.old_capacity(); ++i) {\n        if (IsFull(resize_helper.old_ctrl()[i])) {\n          size_t hash = PolicyTraits::apply(\n              HashElement{hash_ref()}, PolicyTraits::element(old_slots + i));\n          auto target = find_first_non_full(common(), hash);\n          size_t new_i = target.offset;\n          total_probe_length += target.probe_length;\n          SetCtrl(common(), new_i, H2(hash), sizeof(slot_type));\n          transfer(new_slots + new_i, old_slots + i);\n        }\n      }\n      infoz().RecordRehash(total_probe_length);\n    }\n    resize_helper.DeallocateOld<alignof(slot_type)>(\n        CharAlloc(alloc_ref()), sizeof(slot_type),\n        const_cast<std::remove_const_t<slot_type>*>(old_slots));\n  }\n\n  // Prunes control bytes to remove as many tombstones as possible.\n  //\n  // See the comment on `rehash_and_grow_if_necessary()`.\n  inline void drop_deletes_without_resize() {\n    // Stack-allocate space for swapping elements.\n    alignas(slot_type) unsigned char tmp[sizeof(slot_type)];\n    DropDeletesWithoutResize(common(), GetPolicyFunctions(), tmp);\n  }\n\n  // Called whenever the table *might* need to conditionally grow.\n  //\n  // This function is an optimization opportunity to perform a rehash even when\n  // growth is unnecessary, because vacating tombstones is beneficial for\n  // performance in the long-run.\n  void rehash_and_grow_if_necessary() {\n    const size_t cap = capacity();\n    if (cap > Group::kWidth &&\n        // Do these calculations in 64-bit to avoid overflow.\n        size() * uint64_t{32} <= cap * uint64_t{25}) {\n      // Squash DELETED without growing if there is enough capacity.\n      //\n      // Rehash in place if the current size is <= 25/32 of capacity.\n      // Rationale for such a high factor: 1) drop_deletes_without_resize() is\n      // faster than resize, and 2) it takes quite a bit of work to add\n      // tombstones.  In the worst case, seems to take approximately 4\n      // insert/erase pairs to create a single tombstone and so if we are\n      // rehashing because of tombstones, we can afford to rehash-in-place as\n      // long as we are reclaiming at least 1/8 the capacity without doing more\n      // than 2X the work.  (Where \"work\" is defined to be size() for rehashing\n      // or rehashing in place, and 1 for an insert or erase.)  But rehashing in\n      // place is faster per operation than inserting or even doubling the size\n      // of the table, so we actually afford to reclaim even less space from a\n      // resize-in-place.  The decision is to rehash in place if we can reclaim\n      // at about 1/8th of the usable capacity (specifically 3/28 of the\n      // capacity) which means that the total cost of rehashing will be a small\n      // fraction of the total work.\n      //\n      // Here is output of an experiment using the BM_CacheInSteadyState\n      // benchmark running the old case (where we rehash-in-place only if we can\n      // reclaim at least 7/16*capacity) vs. this code (which rehashes in place\n      // if we can recover 3/32*capacity).\n      //\n      // Note that although in the worst-case number of rehashes jumped up from\n      // 15 to 190, but the number of operations per second is almost the same.\n      //\n      // Abridged output of running BM_CacheInSteadyState benchmark from\n      // raw_hash_set_benchmark.   N is the number of insert/erase operations.\n      //\n      //      | OLD (recover >= 7/16        | NEW (recover >= 3/32)\n      // size |    N/s LoadFactor NRehashes |    N/s LoadFactor NRehashes\n      //  448 | 145284       0.44        18 | 140118       0.44        19\n      //  493 | 152546       0.24        11 | 151417       0.48        28\n      //  538 | 151439       0.26        11 | 151152       0.53        38\n      //  583 | 151765       0.28        11 | 150572       0.57        50\n      //  628 | 150241       0.31        11 | 150853       0.61        66\n      //  672 | 149602       0.33        12 | 150110       0.66        90\n      //  717 | 149998       0.35        12 | 149531       0.70       129\n      //  762 | 149836       0.37        13 | 148559       0.74       190\n      //  807 | 149736       0.39        14 | 151107       0.39        14\n      //  852 | 150204       0.42        15 | 151019       0.42        15\n      drop_deletes_without_resize();\n    } else {\n      // Otherwise grow the container.\n      resize(NextCapacity(cap));\n    }\n  }\n\n  void maybe_increment_generation_or_rehash_on_move() {\n    common().maybe_increment_generation_on_move();\n    if (!empty() && common().should_rehash_for_bug_detection_on_move()) {\n      resize(capacity());\n    }\n  }\n\n  template<bool propagate_alloc>\n  raw_hash_set& assign_impl(raw_hash_set&& that) {\n    // We don't bother checking for this/that aliasing. We just need to avoid\n    // breaking the invariants in that case.\n    destructor_impl();\n    common() = std::move(that.common());\n    // TODO(b/296061262): move instead of copying hash/eq/alloc.\n    hash_ref() = that.hash_ref();\n    eq_ref() = that.eq_ref();\n    CopyAlloc(alloc_ref(), that.alloc_ref(),\n              std::integral_constant<bool, propagate_alloc>());\n    that.common() = CommonFields{};\n    maybe_increment_generation_or_rehash_on_move();\n    return *this;\n  }\n\n  raw_hash_set& move_elements_allocs_unequal(raw_hash_set&& that) {\n    const size_t size = that.size();\n    if (size == 0) return *this;\n    reserve(size);\n    for (iterator it = that.begin(); it != that.end(); ++it) {\n      insert(std::move(PolicyTraits::element(it.slot())));\n      that.destroy(it.slot());\n    }\n    that.dealloc();\n    that.common() = CommonFields{};\n    maybe_increment_generation_or_rehash_on_move();\n    return *this;\n  }\n\n  raw_hash_set& move_assign(raw_hash_set&& that,\n                            std::true_type /*propagate_alloc*/) {\n    return assign_impl<true>(std::move(that));\n  }\n  raw_hash_set& move_assign(raw_hash_set&& that,\n                            std::false_type /*propagate_alloc*/) {\n    if (alloc_ref() == that.alloc_ref()) {\n      return assign_impl<false>(std::move(that));\n    }\n    // Aliasing can't happen here because allocs would compare equal above.\n    assert(this != &that);\n    destructor_impl();\n    // We can't take over that's memory so we need to move each element.\n    // While moving elements, this should have that's hash/eq so copy hash/eq\n    // before moving elements.\n    // TODO(b/296061262): move instead of copying hash/eq.\n    hash_ref() = that.hash_ref();\n    eq_ref() = that.eq_ref();\n    return move_elements_allocs_unequal(std::move(that));\n  }\n\n protected:\n  // Attempts to find `key` in the table; if it isn't found, returns a slot that\n  // the value can be inserted into, with the control byte already set to\n  // `key`'s H2.\n  template <class K>\n  std::pair<size_t, bool> find_or_prepare_insert(const K& key) {\n    prefetch_heap_block();\n    auto hash = hash_ref()(key);\n    auto seq = probe(common(), hash);\n    const ctrl_t* ctrl = control();\n    while (true) {\n      Group g{ctrl + seq.offset()};\n      for (uint32_t i : g.Match(H2(hash))) {\n        if (ABSL_PREDICT_TRUE(PolicyTraits::apply(\n                EqualElement<K>{key, eq_ref()},\n                PolicyTraits::element(slot_array() + seq.offset(i)))))\n          return {seq.offset(i), false};\n      }\n      if (ABSL_PREDICT_TRUE(g.MaskEmpty())) break;\n      seq.next();\n      assert(seq.index() <= capacity() && \"full table!\");\n    }\n    return {prepare_insert(hash), true};\n  }\n\n  // Given the hash of a value not currently in the table, finds the next\n  // viable slot index to insert it at.\n  //\n  // REQUIRES: At least one non-full slot available.\n  size_t prepare_insert(size_t hash) ABSL_ATTRIBUTE_NOINLINE {\n    const bool rehash_for_bug_detection =\n        common().should_rehash_for_bug_detection_on_insert();\n    if (rehash_for_bug_detection) {\n      // Move to a different heap allocation in order to detect bugs.\n      const size_t cap = capacity();\n      resize(growth_left() > 0 ? cap : NextCapacity(cap));\n    }\n    auto target = find_first_non_full(common(), hash);\n    if (!rehash_for_bug_detection &&\n        ABSL_PREDICT_FALSE(growth_left() == 0 &&\n                           !IsDeleted(control()[target.offset]))) {\n      size_t old_capacity = capacity();\n      rehash_and_grow_if_necessary();\n      // NOTE: It is safe to use `FindFirstNonFullAfterResize`.\n      // `FindFirstNonFullAfterResize` must be called right after resize.\n      // `rehash_and_grow_if_necessary` may *not* call `resize`\n      // and perform `drop_deletes_without_resize` instead. But this\n      // could happen only on big tables.\n      // For big tables `FindFirstNonFullAfterResize` will always\n      // fallback to normal `find_first_non_full`, so it is safe to use it.\n      target = HashSetResizeHelper::FindFirstNonFullAfterResize(\n          common(), old_capacity, hash);\n    }\n    common().increment_size();\n    set_growth_left(growth_left() - IsEmpty(control()[target.offset]));\n    SetCtrl(common(), target.offset, H2(hash), sizeof(slot_type));\n    common().maybe_increment_generation_on_insert();\n    infoz().RecordInsert(hash, target.probe_length);\n    return target.offset;\n  }\n\n  // Constructs the value in the space pointed by the iterator. This only works\n  // after an unsuccessful find_or_prepare_insert() and before any other\n  // modifications happen in the raw_hash_set.\n  //\n  // PRECONDITION: i is an index returned from find_or_prepare_insert(k), where\n  // k is the key decomposed from `forward<Args>(args)...`, and the bool\n  // returned by find_or_prepare_insert(k) was true.\n  // POSTCONDITION: *m.iterator_at(i) == value_type(forward<Args>(args)...).\n  template <class... Args>\n  void emplace_at(size_t i, Args&&... args) {\n    construct(slot_array() + i, std::forward<Args>(args)...);\n\n    assert(PolicyTraits::apply(FindElement{*this}, *iterator_at(i)) ==\n               iterator_at(i) &&\n           \"constructed value does not match the lookup key\");\n  }\n\n  iterator iterator_at(size_t i) ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return {control() + i, slot_array() + i, common().generation_ptr()};\n  }\n  const_iterator iterator_at(size_t i) const ABSL_ATTRIBUTE_LIFETIME_BOUND {\n    return {control() + i, slot_array() + i, common().generation_ptr()};\n  }\n\n  reference unchecked_deref(iterator it) { return it.unchecked_deref(); }\n\n private:\n  friend struct RawHashSetTestOnlyAccess;\n\n  // The number of slots we can still fill without needing to rehash.\n  //\n  // This is stored separately due to tombstones: we do not include tombstones\n  // in the growth capacity, because we'd like to rehash when the table is\n  // otherwise filled with tombstones: otherwise, probe sequences might get\n  // unacceptably long without triggering a rehash. Callers can also force a\n  // rehash via the standard `rehash(0)`, which will recompute this value as a\n  // side-effect.\n  //\n  // See `CapacityToGrowth()`.\n  size_t growth_left() const { return common().growth_left(); }\n  void set_growth_left(size_t gl) { return common().set_growth_left(gl); }\n\n  // Prefetch the heap-allocated memory region to resolve potential TLB and\n  // cache misses. This is intended to overlap with execution of calculating the\n  // hash for a key.\n  void prefetch_heap_block() const {\n#if ABSL_HAVE_BUILTIN(__builtin_prefetch) || defined(__GNUC__)\n    __builtin_prefetch(control(), 0, 1);\n#endif\n  }\n\n  CommonFields& common() { return settings_.template get<0>(); }\n  const CommonFields& common() const { return settings_.template get<0>(); }\n\n  ctrl_t* control() const { return common().control(); }\n  slot_type* slot_array() const {\n    return static_cast<slot_type*>(common().slot_array());\n  }\n  HashtablezInfoHandle infoz() { return common().infoz(); }\n\n  hasher& hash_ref() { return settings_.template get<1>(); }\n  const hasher& hash_ref() const { return settings_.template get<1>(); }\n  key_equal& eq_ref() { return settings_.template get<2>(); }\n  const key_equal& eq_ref() const { return settings_.template get<2>(); }\n  allocator_type& alloc_ref() { return settings_.template get<3>(); }\n  const allocator_type& alloc_ref() const {\n    return settings_.template get<3>();\n  }\n\n  // Make type-specific functions for this type's PolicyFunctions struct.\n  static size_t hash_slot_fn(void* set, void* slot) {\n    auto* h = static_cast<raw_hash_set*>(set);\n    return PolicyTraits::apply(\n        HashElement{h->hash_ref()},\n        PolicyTraits::element(static_cast<slot_type*>(slot)));\n  }\n  static void transfer_slot_fn(void* set, void* dst, void* src) {\n    auto* h = static_cast<raw_hash_set*>(set);\n    h->transfer(static_cast<slot_type*>(dst), static_cast<slot_type*>(src));\n  }\n  // Note: dealloc_fn will only be used if we have a non-standard allocator.\n  static void dealloc_fn(CommonFields& common, const PolicyFunctions&) {\n    auto* set = reinterpret_cast<raw_hash_set*>(&common);\n\n    // Unpoison before returning the memory to the allocator.\n    SanitizerUnpoisonMemoryRegion(common.slot_array(),\n                                  sizeof(slot_type) * common.capacity());\n\n    common.infoz().Unregister();\n    Deallocate<BackingArrayAlignment(alignof(slot_type))>(\n        &set->alloc_ref(), common.backing_array_start(),\n        common.alloc_size(sizeof(slot_type), alignof(slot_type)));\n  }\n\n  static const PolicyFunctions& GetPolicyFunctions() {\n    static constexpr PolicyFunctions value = {\n        sizeof(slot_type),\n        &raw_hash_set::hash_slot_fn,\n        PolicyTraits::transfer_uses_memcpy()\n            ? TransferRelocatable<sizeof(slot_type)>\n            : &raw_hash_set::transfer_slot_fn,\n        (std::is_same<SlotAlloc, std::allocator<slot_type>>::value\n             ? &DeallocateStandard<alignof(slot_type)>\n             : &raw_hash_set::dealloc_fn),\n    };\n    return value;\n  }\n\n  // Bundle together CommonFields plus other objects which might be empty.\n  // CompressedTuple will ensure that sizeof is not affected by any of the empty\n  // fields that occur after CommonFields.\n  absl::container_internal::CompressedTuple<CommonFields, hasher, key_equal,\n                                            allocator_type>\n      settings_{CommonFields{}, hasher{}, key_equal{}, allocator_type{}};\n}",
  "id": "BLOCK-CPP-05342",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/container/internal/raw_hash_set.h",
  "source_line": 1844,
  "validation_status": "validated"
}