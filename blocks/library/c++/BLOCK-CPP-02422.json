{
  "code": "{\n\n#ifdef ABSL_INTERNAL_NEED_REDUNDANT_CONSTEXPR_DECL\nconstexpr int HashtablezInfo::kMaxStackDepth;\n#endif\n\nnamespace {\nABSL_CONST_INIT std::atomic<bool> g_hashtablez_enabled{\n    false\n};\nABSL_CONST_INIT std::atomic<int32_t> g_hashtablez_sample_parameter{1 << 10};\nstd::atomic<HashtablezConfigListener> g_hashtablez_config_listener{nullptr};\n\n#if defined(ABSL_INTERNAL_HASHTABLEZ_SAMPLE)\nABSL_PER_THREAD_TLS_KEYWORD absl::profiling_internal::ExponentialBiased\n    g_exponential_biased_generator;\n#endif\n\nvoid TriggerHashtablezConfigListener() {\n  auto* listener = g_hashtablez_config_listener.load(std::memory_order_acquire);\n  if (listener != nullptr) listener();\n}\n\n}  // namespace\n\n#if defined(ABSL_INTERNAL_HASHTABLEZ_SAMPLE)\nABSL_PER_THREAD_TLS_KEYWORD SamplingState global_next_sample = {0, 0};\n#endif  // defined(ABSL_INTERNAL_HASHTABLEZ_SAMPLE)\n\nHashtablezSampler& GlobalHashtablezSampler() {\n  static auto* sampler = new HashtablezSampler();\n  return *sampler;\n}\n\nHashtablezInfo::HashtablezInfo() = default;\nHashtablezInfo::~HashtablezInfo() = default;\n\nvoid HashtablezInfo::PrepareForSampling(int64_t stride,\n                                        size_t inline_element_size_value) {\n  capacity.store(0, std::memory_order_relaxed);\n  size.store(0, std::memory_order_relaxed);\n  num_erases.store(0, std::memory_order_relaxed);\n  num_rehashes.store(0, std::memory_order_relaxed);\n  max_probe_length.store(0, std::memory_order_relaxed);\n  total_probe_length.store(0, std::memory_order_relaxed);\n  hashes_bitwise_or.store(0, std::memory_order_relaxed);\n  hashes_bitwise_and.store(~size_t{}, std::memory_order_relaxed);\n  hashes_bitwise_xor.store(0, std::memory_order_relaxed);\n  max_reserve.store(0, std::memory_order_relaxed);\n\n  create_time = absl::Now();\n  weight = stride;\n  // The inliner makes hardcoded skip_count difficult (especially when combined\n  // with LTO).  We use the ability to exclude stacks by regex when encoding\n  // instead.\n  depth = absl::GetStackTrace(stack, HashtablezInfo::kMaxStackDepth,\n                              /* skip_count= */ 0);\n  inline_element_size = inline_element_size_value;\n}\n\nstatic bool ShouldForceSampling() {\n  enum ForceState {\n    kDontForce,\n    kForce,\n    kUninitialized\n  };\n  ABSL_CONST_INIT static std::atomic<ForceState> global_state{\n      kUninitialized};\n  ForceState state = global_state.load(std::memory_order_relaxed);\n  if (ABSL_PREDICT_TRUE(state == kDontForce)) return false;\n\n  if (state == kUninitialized) {\n    state = ABSL_INTERNAL_C_SYMBOL(AbslContainerInternalSampleEverything)()\n                ? kForce\n                : kDontForce;\n    global_state.store(state, std::memory_order_relaxed);\n  }\n  return state == kForce;\n}\n\nHashtablezInfo* SampleSlow(SamplingState& next_sample,\n                           size_t inline_element_size) {\n  if (ABSL_PREDICT_FALSE(ShouldForceSampling())) {\n    next_sample.next_sample = 1;\n    const int64_t old_stride = exchange(next_sample.sample_stride, 1);\n    HashtablezInfo* result =\n        GlobalHashtablezSampler().Register(old_stride, inline_element_size);\n    return result;\n  }\n\n#if !defined(ABSL_INTERNAL_HASHTABLEZ_SAMPLE)\n  next_sample = {\n      std::numeric_limits<int64_t>::max(),\n      std::numeric_limits<int64_t>::max(),\n  };\n  return nullptr;\n#else\n  bool first = next_sample.next_sample < 0;\n\n  const int64_t next_stride = g_exponential_biased_generator.GetStride(\n      g_hashtablez_sample_parameter.load(std::memory_order_relaxed));\n\n  next_sample.next_sample = next_stride;\n  const int64_t old_stride = exchange(next_sample.sample_stride, next_stride);\n  // Small values of interval are equivalent to just sampling next time.\n  ABSL_ASSERT(next_stride >= 1);\n\n  // g_hashtablez_enabled can be dynamically flipped, we need to set a threshold\n  // low enough that we will start sampling in a reasonable time, so we just use\n  // the default sampling rate.\n  if (!g_hashtablez_enabled.load(std::memory_order_relaxed)) return nullptr;\n\n  // We will only be negative on our first count, so we should just retry in\n  // that case.\n  if (first) {\n    if (ABSL_PREDICT_TRUE(--next_sample.next_sample > 0)) return nullptr;\n    return SampleSlow(next_sample, inline_element_size);\n  }\n\n  return GlobalHashtablezSampler().Register(old_stride, inline_element_size);\n#endif\n}\n\nvoid UnsampleSlow(HashtablezInfo* info) {\n  GlobalHashtablezSampler().Unregister(info);\n}\n\nvoid RecordRehashSlow(HashtablezInfo* info, size_t total_probe_length) {\n#ifdef ABSL_INTERNAL_HAVE_SSE2\n  total_probe_length /= 16;\n#else\n  total_probe_length /= 8;\n#endif\n  info->total_probe_length.store(total_probe_length, std::memory_order_relaxed);\n  info->num_erases.store(0, std::memory_order_relaxed);\n  // There is only one concurrent writer, so `load` then `store` is sufficient\n  // instead of using `fetch_add`.\n  info->num_rehashes.store(\n      1 + info->num_rehashes.load(std::memory_order_relaxed),\n      std::memory_order_relaxed);\n}\n\nvoid RecordReservationSlow(HashtablezInfo* info, size_t target_capacity) {\n  info->max_reserve.store(\n      (std::max)(info->max_reserve.load(std::memory_order_relaxed),\n                 target_capacity),\n      std::memory_order_relaxed);\n}\n\nvoid RecordClearedReservationSlow(HashtablezInfo* info) {\n  info->max_reserve.store(0, std::memory_order_relaxed);\n}\n\nvoid RecordStorageChangedSlow(HashtablezInfo* info, size_t size,\n                              size_t capacity) {\n  info->size.store(size, std::memory_order_relaxed);\n  info->capacity.store(capacity, std::memory_order_relaxed);\n  if (size == 0) {\n    // This is a clear, reset the total/num_erases too.\n    info->total_probe_length.store(0, std::memory_order_relaxed);\n    info->num_erases.store(0, std::memory_order_relaxed);\n  }\n}\n\nvoid RecordInsertSlow(HashtablezInfo* info, size_t hash,\n                      size_t distance_from_desired) {\n  // SwissTables probe in groups of 16, so scale this to count items probes and\n  // not offset from desired.\n  size_t probe_length = distance_from_desired;\n#ifdef ABSL_INTERNAL_HAVE_SSE2\n  probe_length /= 16;\n#else\n  probe_length /= 8;\n#endif\n\n  info->hashes_bitwise_and.fetch_and(hash, std::memory_order_relaxed);\n  info->hashes_bitwise_or.fetch_or(hash, std::memory_order_relaxed);\n  info->hashes_bitwise_xor.fetch_xor(hash, std::memory_order_relaxed);\n  info->max_probe_length.store(\n      std::max(info->max_probe_length.load(std::memory_order_relaxed),\n               probe_length),\n      std::memory_order_relaxed);\n  info->total_probe_length.fetch_add(probe_length, std::memory_order_relaxed);\n  info->size.fetch_add(1, std::memory_order_relaxed);\n}\n\nvoid RecordEraseSlow(HashtablezInfo* info) {\n  info->size.fetch_sub(1, std::memory_order_relaxed);\n  // There is only one concurrent writer, so `load` then `store` is sufficient\n  // instead of using `fetch_add`.\n  info->num_erases.store(1 + info->num_erases.load(std::memory_order_relaxed),\n                         std::memory_order_relaxed);\n}\n\nvoid SetHashtablezConfigListener(HashtablezConfigListener l) {\n  g_hashtablez_config_listener.store(l, std::memory_order_release);\n}\n\nbool IsHashtablezEnabled() {\n  return g_hashtablez_enabled.load(std::memory_order_acquire);\n}\n\nvoid SetHashtablezEnabled(bool enabled) {\n  SetHashtablezEnabledInternal(enabled);\n  TriggerHashtablezConfigListener();\n}\n\nvoid SetHashtablezEnabledInternal(bool enabled) {\n  g_hashtablez_enabled.store(enabled, std::memory_order_release);\n}\n\nint32_t GetHashtablezSampleParameter() {\n  return g_hashtablez_sample_parameter.load(std::memory_order_acquire);\n}\n\nvoid SetHashtablezSampleParameter(int32_t rate) {\n  SetHashtablezSampleParameterInternal(rate);\n  TriggerHashtablezConfigListener();\n}\n\nvoid SetHashtablezSampleParameterInternal(int32_t rate) {\n  if (rate > 0) {\n    g_hashtablez_sample_parameter.store(rate, std::memory_order_release);\n  } else {\n    ABSL_RAW_LOG(ERROR, \"Invalid hashtablez sample rate: %lld\",\n                 static_cast<long long>(rate));  // NOLINT(runtime/int)\n  }\n}\n\nsize_t GetHashtablezMaxSamples() {\n  return GlobalHashtablezSampler().GetMaxSamples();\n}\n\nvoid SetHashtablezMaxSamples(size_t max) {\n  SetHashtablezMaxSamplesInternal(max);\n  TriggerHashtablezConfigListener();\n}\n\nvoid SetHashtablezMaxSamplesInternal(size_t max) {\n  if (max > 0) {\n    GlobalHashtablezSampler().SetMaxSamples(max);\n  } else {\n    ABSL_RAW_LOG(ERROR, \"Invalid hashtablez max samples: 0\");\n  }\n}\n\n}",
  "id": "BLOCK-CPP-02422",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/container/internal/hashtablez_sampler.cc",
  "source_line": 37,
  "validation_status": "validated"
}