{
  "code": "{\nABSL_NAMESPACE_BEGIN\n\ninline uint64_t gbswap_64(uint64_t host_int) {\n#if ABSL_HAVE_BUILTIN(__builtin_bswap64) || defined(__GNUC__)\n  return __builtin_bswap64(host_int);\n#elif defined(_MSC_VER)\n  return _byteswap_uint64(host_int);\n#else\n  return (((host_int & uint64_t{0xFF}) << 56) |\n          ((host_int & uint64_t{0xFF00}) << 40) |\n          ((host_int & uint64_t{0xFF0000}) << 24) |\n          ((host_int & uint64_t{0xFF000000}) << 8) |\n          ((host_int & uint64_t{0xFF00000000}) >> 8) |\n          ((host_int & uint64_t{0xFF0000000000}) >> 24) |\n          ((host_int & uint64_t{0xFF000000000000}) >> 40) |\n          ((host_int & uint64_t{0xFF00000000000000}) >> 56));\n#endif\n}\n\ninline uint32_t gbswap_32(uint32_t host_int) {\n#if ABSL_HAVE_BUILTIN(__builtin_bswap32) || defined(__GNUC__)\n  return __builtin_bswap32(host_int);\n#elif defined(_MSC_VER)\n  return _byteswap_ulong(host_int);\n#else\n  return (((host_int & uint32_t{0xFF}) << 24) |\n          ((host_int & uint32_t{0xFF00}) << 8) |\n          ((host_int & uint32_t{0xFF0000}) >> 8) |\n          ((host_int & uint32_t{0xFF000000}) >> 24));\n#endif\n}\n\ninline uint16_t gbswap_16(uint16_t host_int) {\n#if ABSL_HAVE_BUILTIN(__builtin_bswap16) || defined(__GNUC__)\n  return __builtin_bswap16(host_int);\n#elif defined(_MSC_VER)\n  return _byteswap_ushort(host_int);\n#else\n  return (((host_int & uint16_t{0xFF}) << 8) |\n          ((host_int & uint16_t{0xFF00}) >> 8));\n#endif\n}\n\n#ifdef ABSL_IS_LITTLE_ENDIAN\n\n// Portable definitions for htonl (host-to-network) and friends on little-endian\n// architectures.\ninline uint16_t ghtons(uint16_t x) { return gbswap_16(x); }\ninline uint32_t ghtonl(uint32_t x) { return gbswap_32(x); }\ninline uint64_t ghtonll(uint64_t x) { return gbswap_64(x); }\n\n#elif defined ABSL_IS_BIG_ENDIAN\n\n// Portable definitions for htonl (host-to-network) etc on big-endian\n// architectures. These definitions are simpler since the host byte order is the\n// same as network byte order.\ninline uint16_t ghtons(uint16_t x) { return x; }\ninline uint32_t ghtonl(uint32_t x) { return x; }\ninline uint64_t ghtonll(uint64_t x) { return x; }\n\n#else\n#error \\\n    \"Unsupported byte order: Either ABSL_IS_BIG_ENDIAN or \" \\\n       \"ABSL_IS_LITTLE_ENDIAN must be defined\"\n#endif  // byte order\n\ninline uint16_t gntohs(uint16_t x) { return ghtons(x); }\ninline uint32_t gntohl(uint32_t x) { return ghtonl(x); }\ninline uint64_t gntohll(uint64_t x) { return ghtonll(x); }\n\n// Utilities to convert numbers between the current hosts's native byte\n// order and little-endian byte order\n//\n// Load/Store methods are alignment safe\nnamespace little_endian {\n// Conversion functions.\n#ifdef ABSL_IS_LITTLE_ENDIAN\n\ninline uint16_t FromHost16(uint16_t x) { return x; }\ninline uint16_t ToHost16(uint16_t x) { return x; }\n\ninline uint32_t FromHost32(uint32_t x) { return x; }\ninline uint32_t ToHost32(uint32_t x) { return x; }\n\ninline uint64_t FromHost64(uint64_t x) { return x; }\ninline uint64_t ToHost64(uint64_t x) { return x; }\n\ninline constexpr bool IsLittleEndian() { return true; }\n\n#elif defined ABSL_IS_BIG_ENDIAN\n\ninline uint16_t FromHost16(uint16_t x) { return gbswap_16(x); }\ninline uint16_t ToHost16(uint16_t x) { return gbswap_16(x); }\n\ninline uint32_t FromHost32(uint32_t x) { return gbswap_32(x); }\ninline uint32_t ToHost32(uint32_t x) { return gbswap_32(x); }\n\ninline uint64_t FromHost64(uint64_t x) { return gbswap_64(x); }\ninline uint64_t ToHost64(uint64_t x) { return gbswap_64(x); }\n\ninline constexpr bool IsLittleEndian() { return false; }\n\n#endif /* ENDIAN */\n\ninline uint8_t FromHost(uint8_t x) { return x; }\ninline uint16_t FromHost(uint16_t x) { return FromHost16(x); }\ninline uint32_t FromHost(uint32_t x) { return FromHost32(x); }\ninline uint64_t FromHost(uint64_t x) { return FromHost64(x); }\ninline uint8_t ToHost(uint8_t x) { return x; }\ninline uint16_t ToHost(uint16_t x) { return ToHost16(x); }\ninline uint32_t ToHost(uint32_t x) { return ToHost32(x); }\ninline uint64_t ToHost(uint64_t x) { return ToHost64(x); }\n\ninline int8_t FromHost(int8_t x) { return x; }\ninline int16_t FromHost(int16_t x) {\n  return bit_cast<int16_t>(FromHost16(bit_cast<uint16_t>(x)));\n}\ninline int32_t FromHost(int32_t x) {\n  return bit_cast<int32_t>(FromHost32(bit_cast<uint32_t>(x)));\n}\ninline int64_t FromHost(int64_t x) {\n  return bit_cast<int64_t>(FromHost64(bit_cast<uint64_t>(x)));\n}\ninline int8_t ToHost(int8_t x) { return x; }\ninline int16_t ToHost(int16_t x) {\n  return bit_cast<int16_t>(ToHost16(bit_cast<uint16_t>(x)));\n}\ninline int32_t ToHost(int32_t x) {\n  return bit_cast<int32_t>(ToHost32(bit_cast<uint32_t>(x)));\n}\ninline int64_t ToHost(int64_t x) {\n  return bit_cast<int64_t>(ToHost64(bit_cast<uint64_t>(x)));\n}\n\n// Functions to do unaligned loads and stores in little-endian order.\ninline uint16_t Load16(absl::Nonnull<const void *> p) {\n  return ToHost16(ABSL_INTERNAL_UNALIGNED_LOAD16(p));\n}\n\ninline void Store16(absl::Nonnull<void *> p, uint16_t v) {\n  ABSL_INTERNAL_UNALIGNED_STORE16(p, FromHost16(v));\n}\n\ninline uint32_t Load32(absl::Nonnull<const void *> p) {\n  return ToHost32(ABSL_INTERNAL_UNALIGNED_LOAD32(p));\n}\n\ninline void Store32(absl::Nonnull<void *> p, uint32_t v) {\n  ABSL_INTERNAL_UNALIGNED_STORE32(p, FromHost32(v));\n}\n\ninline uint64_t Load64(absl::Nonnull<const void *> p) {\n  return ToHost64(ABSL_INTERNAL_UNALIGNED_LOAD64(p));\n}\n\ninline void Store64(absl::Nonnull<void *> p, uint64_t v) {\n  ABSL_INTERNAL_UNALIGNED_STORE64(p, FromHost64(v));\n}\n\n}  // namespace little_endian\n\n// Utilities to convert numbers between the current hosts's native byte\n// order and big-endian byte order (same as network byte order)\n//\n// Load/Store methods are alignment safe\nnamespace big_endian {\n#ifdef ABSL_IS_LITTLE_ENDIAN\n\ninline uint16_t FromHost16(uint16_t x) { return gbswap_16(x); }\ninline uint16_t ToHost16(uint16_t x) { return gbswap_16(x); }\n\ninline uint32_t FromHost32(uint32_t x) { return gbswap_32(x); }\ninline uint32_t ToHost32(uint32_t x) { return gbswap_32(x); }\n\ninline uint64_t FromHost64(uint64_t x) { return gbswap_64(x); }\ninline uint64_t ToHost64(uint64_t x) { return gbswap_64(x); }\n\ninline constexpr bool IsLittleEndian() { return true; }\n\n#elif defined ABSL_IS_BIG_ENDIAN\n\ninline uint16_t FromHost16(uint16_t x) { return x; }\ninline uint16_t ToHost16(uint16_t x) { return x; }\n\ninline uint32_t FromHost32(uint32_t x) { return x; }\ninline uint32_t ToHost32(uint32_t x) { return x; }\n\ninline uint64_t FromHost64(uint64_t x) { return x; }\ninline uint64_t ToHost64(uint64_t x) { return x; }\n\ninline constexpr bool IsLittleEndian() { return false; }\n\n#endif /* ENDIAN */\n\ninline uint8_t FromHost(uint8_t x) { return x; }\ninline uint16_t FromHost(uint16_t x) { return FromHost16(x); }\ninline uint32_t FromHost(uint32_t x) { return FromHost32(x); }\ninline uint64_t FromHost(uint64_t x) { return FromHost64(x); }\ninline uint8_t ToHost(uint8_t x) { return x; }\ninline uint16_t ToHost(uint16_t x) { return ToHost16(x); }\ninline uint32_t ToHost(uint32_t x) { return ToHost32(x); }\ninline uint64_t ToHost(uint64_t x) { return ToHost64(x); }\n\ninline int8_t FromHost(int8_t x) { return x; }\ninline int16_t FromHost(int16_t x) {\n  return bit_cast<int16_t>(FromHost16(bit_cast<uint16_t>(x)));\n}\ninline int32_t FromHost(int32_t x) {\n  return bit_cast<int32_t>(FromHost32(bit_cast<uint32_t>(x)));\n}\ninline int64_t FromHost(int64_t x) {\n  return bit_cast<int64_t>(FromHost64(bit_cast<uint64_t>(x)));\n}\ninline int8_t ToHost(int8_t x) { return x; }\ninline int16_t ToHost(int16_t x) {\n  return bit_cast<int16_t>(ToHost16(bit_cast<uint16_t>(x)));\n}\ninline int32_t ToHost(int32_t x) {\n  return bit_cast<int32_t>(ToHost32(bit_cast<uint32_t>(x)));\n}\ninline int64_t ToHost(int64_t x) {\n  return bit_cast<int64_t>(ToHost64(bit_cast<uint64_t>(x)));\n}\n\n// Functions to do unaligned loads and stores in big-endian order.\ninline uint16_t Load16(absl::Nonnull<const void *> p) {\n  return ToHost16(ABSL_INTERNAL_UNALIGNED_LOAD16(p));\n}\n\ninline void Store16(absl::Nonnull<void *> p, uint16_t v) {\n  ABSL_INTERNAL_UNALIGNED_STORE16(p, FromHost16(v));\n}\n\ninline uint32_t Load32(absl::Nonnull<const void *> p) {\n  return ToHost32(ABSL_INTERNAL_UNALIGNED_LOAD32(p));\n}\n\ninline void Store32(absl::Nonnull<void *>p, uint32_t v) {\n  ABSL_INTERNAL_UNALIGNED_STORE32(p, FromHost32(v));\n}\n\ninline uint64_t Load64(absl::Nonnull<const void *> p) {\n  return ToHost64(ABSL_INTERNAL_UNALIGNED_LOAD64(p));\n}\n\ninline void Store64(absl::Nonnull<void *> p, uint64_t v) {\n  ABSL_INTERNAL_UNALIGNED_STORE64(p, FromHost64(v));\n}\n\n}  // namespace big_endian\n\nABSL_NAMESPACE_END\n}",
  "id": "BLOCK-CPP-04662",
  "language": "c++",
  "source_file": "/storage/emulated/0/Download/cpp_codebases/abseil/absl/base/internal/endian.h",
  "source_line": 28,
  "validation_status": "validated"
}