// insight_generator.naab - Module for generating insights from processed data

use io as io
use json as json
use string as str
use array as arr

// Imports the ProcessedItem struct from data_transformer.naab
use data_transformer
// Imports AnalysisConfig from config.naab
use app_config

// Structure to hold analysis results
struct AnalysisResults {
    total_items: int,
    avg_description_length: float,
    max_description_length: int,
    min_description_length: int,
    sentiment_summary: dict<string, float>, // e.g., {"positive": 0.3, "neutral": 0.6, "negative": 0.1}
    anomalies_detected: int,
    anomalous_items: list<string> // Names of anomalous items
}

// Function to perform various analyses on the processed data
fn perform_analytics(data: list<data_transformer.ProcessedItem>, analysis_config: app_config.AnalysisConfig) -> AnalysisResults? {
    io.write("üìä Performing data analytics...\n")

    let data_json = json.stringify(data)
    let analysis_config_json = json.stringify(analysis_config)
    let result_json = <<python[data_json, analysis_config_json]
import pandas as pd
import numpy as np
import json
import sys
import io as pyio
from textblob import TextBlob

old_stdout = sys.stdout
old_stderr = sys.stderr
redirected_output = pyio.StringIO()
sys.stdout = redirected_output
sys.stderr = old_stderr

data_str = data_json
analysis_config_str = analysis_config_json

analysis_results_dict = {}
error_message = None

try:
    processed_data = json.loads(data_str)
    analysis_config = json.loads(analysis_config_str)

    df = pd.DataFrame(processed_data)

    # --- Basic Statistics ---
    analysis_results_dict['total_items'] = int(len(df))

    if not df.empty and 'description_length' in df.columns:
        analysis_results_dict['avg_description_length'] = float(df['description_length'].mean())
        analysis_results_dict['max_description_length'] = int(df['description_length'].max())
        analysis_results_dict['min_description_length'] = int(df['description_length'].min())
    else:
        analysis_results_dict['avg_description_length'] = 0.0
        analysis_results_dict['max_description_length'] = 0
        analysis_results_dict['min_description_length'] = 0


    # --- Sentiment Analysis (if description column exists and sentiment_model is set) ---
    sentiment_summary = {"positive": 0.0, "neutral": 0.0, "negative": 0.0}
    if not df.empty and 'description' in df.columns and analysis_config.get('sentiment_model') == 'textblob':
        positive_count = 0
        neutral_count = 0
        negative_count = 0
        
        for desc in df['description']:
            if pd.isna(desc): # Handle NaN descriptions
                continue
            analysis = TextBlob(str(desc))
            if analysis.sentiment.polarity > 0:
                positive_count += 1
            elif analysis.sentiment.polarity == 0:
                neutral_count += 1
            else:
                negative_count += 1
        
        total_sentiment_items = positive_count + neutral_count + negative_count
        if total_sentiment_items > 0:
            sentiment_summary['positive'] = float(positive_count / total_sentiment_items)
            sentiment_summary['neutral'] = float(neutral_count / total_sentiment_items)
            sentiment_summary['negative'] = float(negative_count / total_sentiment_items)

    analysis_results_dict['sentiment_summary'] = sentiment_summary

    # --- Anomaly Detection (IQR-based method using pure pandas) ---
    anomalies_detected = 0
    anomalous_items = []
    if not df.empty and 'description_length' in df.columns and len(df) > 1:
        # Use Interquartile Range (IQR) method for outlier detection
        # This is a standard statistical method that doesn't require ML libraries
        Q1 = df['description_length'].quantile(0.25)
        Q3 = df['description_length'].quantile(0.75)
        IQR = Q3 - Q1

        # Define outlier bounds (1.5 * IQR is the standard multiplier)
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Flag items outside the bounds as anomalies
        df['anomaly'] = ((df['description_length'] < lower_bound) |
                         (df['description_length'] > upper_bound))

        anomalies_detected = int(df['anomaly'].sum())

        # Get names of anomalous items if 'name' column exists
        if 'name' in df.columns:
            anomalous_items = df[df['anomaly'] == True]['name'].tolist()
        else:
            anomalous_items = []

    analysis_results_dict['anomalies_detected'] = anomalies_detected
    analysis_results_dict['anomalous_items'] = anomalous_items

except Exception as e:
    error_message = f"Python Analysis Error: {e}"

finally:
    sys.stdout = old_stdout
    sys.stderr = old_stderr

if error_message:
    _ = json.dumps({"error": error_message})
else:
    _ = json.dumps(analysis_results_dict)
    >>

    let python_output_str = result_json

    if str.starts_with(python_output_str, "{") && str.contains(python_output_str, "\"error\":") {
        let error_obj = json.parse(python_output_str)
        if error_obj["error"] != null {
            io.write_error("‚ùå Data analysis failed: ", error_obj["error"], "\n")
            return null
        }
    }

    let parsed_results = json.parse(python_output_str)

    let sentiment_summary_dict: dict<string, float> = {}
    for key in parsed_results.sentiment_summary {
        sentiment_summary_dict[key] = parsed_results.sentiment_summary[key]
    }

    let anomalous_items_list: list<string> = []
    for item in parsed_results.anomalous_items {
        array.push(anomalous_items_list, item)
    }

    return new AnalysisResults {
        total_items: parsed_results["total_items"],
        avg_description_length: parsed_results["avg_description_length"],
        max_description_length: parsed_results["max_description_length"],
        min_description_length: parsed_results["min_description_length"],
        sentiment_summary: sentiment_summary_dict,
        anomalies_detected: parsed_results["anomalies_detected"],
        anomalous_items: anomalous_items_list
    }
}
