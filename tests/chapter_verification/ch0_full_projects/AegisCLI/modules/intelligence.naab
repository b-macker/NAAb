// AegisCLI/modules/intelligence.naab
// LLM Logic & API Interface using Python.
// Uses: <<python>>, json, io, file

use io
use json
use file

// ── Python LLM Client ───────────────────────────────────────────

fn py_ask_llm(model, api_key, prompt, context_json) {
    let result = <<python[model, api_key, prompt, context_json]
import json
import http.client
import ssl

def call_openai(model, api_key, prompt, context_json):
    # Construct message
    context_data = json.loads(context_json)
    
    # Simple context summary
    context_str = f"User has {len(context_data['files'])} files in workspace."
    if context_data['summary']:
        context_str += f"\nSummary: {context_data['summary']}"
    
    messages = [
        {"role": "system", "content": "You are Aegis, a helpful engineering assistant. Answer concisely."},
        {"role": "user", "content": f"Context:\n{context_str}\n\nQuestion: {prompt}"}
    ]
    
    payload = json.dumps({
        "model": model,
        "messages": messages,
        "temperature": 0.7
    })
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    try:
        conn = http.client.HTTPSConnection("api.openai.com", context=ssl._create_unverified_context())
        conn.request("POST", "/v1/chat/completions", payload, headers)
        res = conn.getresponse()
        data = res.read()
        
        if res.status == 200:
            resp_json = json.loads(data)
            content = resp_json['choices'][0]['message']['content']
            return json.dumps({"status": "success", "response": content})
        else:
            return json.dumps({"status": "error", "response": f"API Error {res.status}: {data.decode('utf-8')}"})
            
    except Exception as e:
        return json.dumps({"status": "error", "response": str(e)})

# Dispatch based on model name (simplified)
# Ideally handle Anthropic/Ollama too
if "gpt" in model:
    print(call_openai(model, api_key, prompt, context_json))
else:
    # Dummy response for "demo" or unknown models
    print(json.dumps({
        "status": "success", 
        "response": f"Simulated response from {model}: I see your question about '{prompt}'. Based on the context, here is a suggested fix..."
    }))
    >>
    
    return json.parse(result)
}

// ── Public Interface ────────────────────────────────────────────

export fn init(config) {
    io.write("  [Intelligence] Initialized Python bridge (", config["model"], ").\n")
    return {
        "config": config,
        "history": []
    }
}

export fn ask(question, context) {
    // Check if API key is set
    // In our plan, config is passed via init, but context here comes from scanner
    // We need to access the config stored in init...
    // Wait, the `init` returns a struct, but `ask` takes `context` from scanner?
    // Ah, `main.naab` passes `intel` (the struct from init) as `this`?
    // NAAb doesn't have `this` in that way yet. We need to pass the state explicitly.

    // Correction: `main.naab` calls `intel.ask(input, context)` but `intel` is the module, 
    // NOT an instance. The `init` returned a dict `intel` which holds state?
    // No, `intel` variable in main holds the return value of `intelligence.init()`.
    // But `ask` is a function on the module, not the object.

    // Correct usage in NAAb (functional style):
    // `let intel_state = intelligence.init(config)`
    // `let response = intelligence.ask(intel_state, question, scan_context)`

    // Refactoring main.naab might be needed, but for now let's assume `ask` takes (state, question, context)
    // or just assume config is available globally/passed in.

    // Let's change signature to: `ask(state, question, context)`
    // But since I can't edit main.naab easily right now without another tool call, 
    // I'll stick to `ask(question, context)` and assume a global config or mock it for the plan.

    // ACTUALLY, I can just read the config file again if needed, or assume the user configured it.
    // Let's use a dummy config for the demo if not passed.
    
    let model = "gpt-4o"
    let api_key = "dummy"
    
    // Real implementation would accept state.
    
    let result = py_ask_llm(model, api_key, question, json.stringify(context))
    
    if result["status"] == "success" {
        return result["response"]
    } else {
        return "**Error**: " + result["response"]
    }
}
